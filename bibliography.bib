@inproceedings{AFLplusplus-Woot20,
  title = {{{AFL}}++: {{Combining}} Incremental Steps of Fuzzing Research},
  booktitle = {14th {{USENIX}} Workshop on Offensive Technologies ({{WOOT}} 20)},
  author = {Fioraldi, Andrea and Maier, Dominik and Ei√üfeldt, Heiko and Heuse, Marc},
  date = {2020-08},
  publisher = {USENIX Association},
  file = {/home/kchou/Documents/Zotero/storage/Q9HV2ZU3/Fioraldi et al. - 2020 - AFL++ Combining incremental steps of fuzzing research.pdf}
}

@inproceedings{andersonWhyCryptosystemsFail1993,
  title = {Why Cryptosystems Fail},
  booktitle = {Proceedings of the 1st {{ACM}} Conference on {{Computer}} and Communications Security  - {{CCS}} '93},
  author = {Anderson, Ross},
  date = {1993},
  pages = {215--227},
  publisher = {ACM Press},
  location = {Fairfax, Virginia, United States},
  doi = {10.1145/168588.168615},
  url = {http://portal.acm.org/citation.cfm?doid=168588.168615},
  eventtitle = {The 1st {{ACM}} Conference},
  isbn = {978-0-89791-629-5},
  langid = {english},
  file = {/home/kchou/Documents/Zotero/storage/M2I2PEV6/Anderson - 1993 - Why cryptosystems fail.pdf}
}

@article{baderAISoftwareEngineering2021,
  title = {{{AI}} in {{Software Engineering}} at {{Facebook}}},
  author = {Bader, Johannes and Seohyun Kim, Sonia and Sifei Luan, Frank and Chandra, Satish and Meijer, Erik},
  date = {2021-07},
  journaltitle = {IEEE Software},
  volume = {38},
  number = {4},
  pages = {52--61},
  issn = {1937-4194},
  doi = {10.1109/MS.2021.3061664},
  url = {https://ieeexplore.ieee.org/document/9360852},
  abstract = {How can artificial intelligence help software engineers better do their jobs and advance the state of the practice? We describe three productivity tools that learn patterns from software artifacts: code search using natural language, code recommendation, and automatic bug fixing.},
  eventtitle = {{{IEEE Software}}},
  keywords = {Data mining,Licenses,Social networking (online),Software development management,Software engineering,Software tools,Training},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-02-24T20:51:59.282Z},
  file = {/home/kchou/Documents/Zotero/storage/NMQ6R6AF/Bader et al. - 2021 - AI in Software Engineering at Facebook.pdf;/home/kchou/Documents/Zotero/storage/9QHSDN3B/9360852.html}
}

@online{behrouzTitansLearningMemorize2024,
  title = {Titans: {{Learning}} to {{Memorize}} at {{Test Time}}},
  shorttitle = {Titans},
  author = {Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  date = {2024-12-31},
  eprint = {2501.00663},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.00663},
  url = {http://arxiv.org/abs/2501.00663},
  abstract = {Over more than a decade there has been an extensive research effort on how to effectively utilize recurrent models and attention. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new neural long-term memory module that learns to memorize historical context and helps attention to attend to the current context while utilizing long past information. We show that this neural memory has the advantage of fast parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics, and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models. They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks compared to baselines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Behrouz et al. - 2024 - Titans Learning to Memorize at Test Time - @behrouzTitansLearningMemorize2024.pdf;/home/kchou/Documents/Braindump/998 Attachments/PDFs/2501.html}
}

@book{belcherWritingYourJournal2019,
  title = {Writing Your Journal Article in Twelve Weeks: A Guide to Academic Publishing Success},
  shorttitle = {Writing Your Journal Article in Twelve Weeks},
  author = {Belcher, Wendy Laura},
  date = {2019},
  series = {Chicago Guides to Writing, Editing, and Publishing},
  edition = {Second edition},
  publisher = {The University of Chicago Press},
  location = {Chicago London},
  isbn = {978-0-226-50008-9 978-0-226-49991-8},
  langid = {english},
  pagetotal = {430},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Belcher - 2019 - Writing your journal article in twelve weeks a guide to academic publishing success.pdf}
}

@online{bondarenkoDemonstratingSpecificationGaming2025,
  title = {Demonstrating Specification Gaming in Reasoning Models},
  author = {Bondarenko, Alexander and Volk, Denis and Volkov, Dmitrii and Ladish, Jeffrey},
  date = {2025-02-18},
  eprint = {2502.13295},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.13295},
  url = {http://arxiv.org/abs/2502.13295},
  abstract = {We demonstrate LLM agent specification gaming by instructing models to win against a chess engine. We find reasoning models like o1 preview and DeepSeek-R1 will often hack the benchmark by default, while language models like GPT-4o and Claude 3.5 Sonnet need to be told that normal play won't work to hack. We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024; Weij et al., 2024) by using realistic task prompts and avoiding excess nudging. Our results suggest reasoning models may resort to hacking to solve difficult problems, as observed in OpenAI (2024)'s o1 Docker escape during cyber capabilities testing.},
  pubstate = {prepublished},
  file = {/home/kchou/Documents/Zotero/storage/RPZP59P6/Bondarenko et al. - 2025 - Demonstrating specification gaming in reasoning models - @bondarenkoDemonstratingSpecificationGaming2025.pdf;/home/kchou/Documents/Zotero/storage/MCT7K4X8/2502.html}
}

@online{bullwinkelLessonsRedTeaming2025,
  title = {Lessons {{From Red Teaming}} 100 {{Generative AI Products}}},
  author = {Bullwinkel, Blake and Minnich, Amanda and Chawla, Shiven and Lopez, Gary and Pouliot, Martin and Maxwell, Whitney and family=Gruyter, given=Joris, prefix=de, useprefix=false and Pratt, Katherine and Qi, Saphir and Chikanov, Nina and Lutz, Roman and Dheekonda, Raja Sekhar Rao and Jagdagdorj, Bolor-Erdene and Kim, Eugenia and Song, Justin and Hines, Keegan and Jones, Daniel and Severi, Giorgio and Lundeen, Richard and Vaughan, Sam and Westerhoff, Victoria and Bryan, Pete and Kumar, Ram Shankar Siva and Zunger, Yonatan and Kawaguchi, Chang and Russinovich, Mark},
  date = {2025-01-13},
  eprint = {2501.07238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.07238},
  url = {http://arxiv.org/abs/2501.07238},
  abstract = {In recent years, AI red teaming has emerged as a practice for probing the safety and security of generative AI systems. Due to the nascency of the field, there are many open questions about how red teaming operations should be conducted. Based on our experience red teaming over 100 generative AI products at Microsoft, we present our internal threat model ontology and eight main lessons we have learned: 1. Understand what the system can do and where it is applied 2. You don't have to compute gradients to break an AI system 3. AI red teaming is not safety benchmarking 4. Automation can help cover more of the risk landscape 5. The human element of AI red teaming is crucial 6. Responsible AI harms are pervasive but difficult to measure 7. LLMs amplify existing security risks and introduce new ones 8. The work of securing AI systems will never be complete By sharing these insights alongside case studies from our operations, we offer practical recommendations aimed at aligning red teaming efforts with real world risks. We also highlight aspects of AI red teaming that we believe are often misunderstood and discuss open questions for the field to consider.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-02-19T08:46:14.868Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Bullwinkel et al. - 2025 - Lessons From Red Teaming 100 Generative AI Products - @bullwinkelLessonsRedTeaming2025.pdf;/home/kchou/Documents/Braindump/998 Attachments/PDFs/2501 1.html}
}

@inproceedings{cadarKLEEUnassistedAutomatic2008,
  title = {{{KLEE}}: {{Unassisted}} and {{Automatic Generation}} of {{High-Coverage Tests}} for {{Complex Systems Programs}}},
  shorttitle = {{{KLEE}}},
  author = {Cadar, Cristian and Dunbar, Daniel and Engler, D.},
  date = {2008-12-08},
  url = {https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8},
  abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.    We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  eventtitle = {{{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}}},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-03-19T10:38:46.368Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Cadar et al. - 2008 - KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs - @cadarKLEEUnassistedAutomatic2008.pdf}
}

@online{flachNeuralLambdaCalculus2023,
  title = {A {{Neural Lambda Calculus}}: {{Neurosymbolic AI}} Meets the Foundations of Computing and Functional Programming},
  shorttitle = {A {{Neural Lambda Calculus}}},
  author = {Flach, Jo√£o and Lamb, Luis C.},
  date = {2023-04-18},
  eprint = {2304.09276},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2304.09276},
  url = {http://arxiv.org/abs/2304.09276},
  abstract = {Over the last decades, deep neural networks based-models became the dominant paradigm in machine learning. Further, the use of artificial neural networks in symbolic learning has been seen as increasingly relevant recently. To study the capabilities of neural networks in the symbolic AI domain, researchers have explored the ability of deep neural networks to learn mathematical constructions, such as addition and multiplication, logic inference, such as theorem provers, and even the execution of computer programs. The latter is known to be too complex a task for neural networks. Therefore, the results were not always successful, and often required the introduction of biased elements in the learning process, in addition to restricting the scope of possible programs to be executed. In this work, we will analyze the ability of neural networks to learn how to execute programs as a whole. To do so, we propose a different approach. Instead of using an imperative programming language, with complex structures, we use the Lambda Calculus (\{\textbackslash lambda\}-Calculus), a simple, but Turing-Complete mathematical formalism, which serves as the basis for modern functional programming languages and is at the heart of computability theory. We will introduce the use of integrated neural learning and lambda calculi formalization. Finally, we explore execution of a program in \{\textbackslash lambda\}-Calculus is based on reductions, we will show that it is enough to learn how to perform these reductions so that we can execute any program. Keywords: Machine Learning, Lambda Calculus, Neurosymbolic AI, Neural Networks, Transformer Model, Sequence-to-Sequence Models, Computational Models},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Logic in Computer Science,Computer Science - Machine Learning},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:18.799Z},
  file = {/home/kchou/Documents/Zotero/storage/DWIEY2YU/Flach and Lamb - 2023 - A Neural Lambda Calculus Neurosymbolic AI meets the foundations of computing and functional program.pdf;/home/kchou/Documents/Zotero/storage/M7JFIZUK/2304.html}
}

@online{gangulyProofThoughtNeurosymbolic2024,
  title = {Proof of {{Thought}} : {{Neurosymbolic Program Synthesis}} Allows {{Robust}} and {{Interpretable Reasoning}}},
  shorttitle = {Proof of {{Thought}}},
  author = {Ganguly, Debargha and Iyengar, Srinivasan and Chaudhary, Vipin and Kalyanaraman, Shivkumar},
  date = {2024-09-25},
  eprint = {2409.17270},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.17270},
  url = {http://arxiv.org/abs/2409.17270},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces Proof of Thought, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate Proof of Thought's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:07.288Z},
  file = {/home/kchou/Documents/Zotero/storage/N5UFCFM2/Ganguly et al. - 2024 - Proof of Thought  Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning.pdf;/home/kchou/Documents/Zotero/storage/QAR2VUX6/2409 1.html}
}

@online{garcezNeurosymbolicAI3rd2020,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {family=Garcez, given=Artur, prefix=d'Avila, useprefix=false and Lamb, Luis C.},
  date = {2020-12-16},
  eprint = {2012.05876},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2012.05876},
  url = {http://arxiv.org/abs/2012.05876},
  abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-02-24T20:57:10.231Z},
  file = {/home/kchou/Documents/Zotero/storage/9LGLJU99/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave 1.pdf;/home/kchou/Documents/Zotero/storage/YFYZFHUW/2012 1.html}
}

@online{gaurBuildingTrustworthyNeuroSymbolic2023,
  title = {Building {{Trustworthy NeuroSymbolic AI Systems}}: {{Consistency}}, {{Reliability}}, {{Explainability}}, and {{Safety}}},
  shorttitle = {Building {{Trustworthy NeuroSymbolic AI Systems}}},
  author = {Gaur, Manas and Sheth, Amit},
  date = {2023-12-05},
  eprint = {2312.06798},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.06798},
  url = {http://arxiv.org/abs/2312.06798},
  abstract = {Explainability and Safety engender Trust. These require a model to exhibit consistency and reliability. To achieve these, it is necessary to use and analyze data and knowledge with statistical and symbolic AI methods relevant to the AI application - neither alone will do. Consequently, we argue and seek to demonstrate that the NeuroSymbolic AI approach is better suited for making AI a trusted AI system. We present the CREST framework that shows how Consistency, Reliability, user-level Explainability, and Safety are built on NeuroSymbolic methods that use data and knowledge to support requirements for critical applications such as health and well-being. This article focuses on Large Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs have garnered substantial attention from researchers due to their versatility in handling a broad array of natural language processing (NLP) scenarios. For example, ChatGPT and Google's MedPaLM have emerged as highly promising platforms for providing information in general and health-related queries, respectively. Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails. CREST presents a plausible approach harnessing procedural and graph-based knowledge within a NeuroSymbolic framework to shed light on the challenges associated with LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:16.527Z},
  file = {/home/kchou/Documents/Zotero/storage/PMIHFSLG/Gaur and Sheth - 2023 - Building Trustworthy NeuroSymbolic AI Systems Consistency, Reliability, Explainability, and Safety.pdf;/home/kchou/Documents/Zotero/storage/FEXTX7CM/2312.html}
}

@online{grovUseNeurosymbolicAI2024,
  title = {On the Use of Neurosymbolic {{AI}} for Defending against Cyber Attacks},
  author = {Grov, Gudmund and Halvorsen, Jonas and Eckhoff, Magnus Wiik and Hansen, Bj√∏rn Jervell and Eian, Martin and Mavroeidis, Vasileios},
  date = {2024-08-09},
  eprint = {2408.04996},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2408.04996},
  url = {http://arxiv.org/abs/2408.04996},
  abstract = {It is generally accepted that all cyber attacks cannot be prevented, creating a need for the ability to detect and respond to cyber attacks. Both connectionist and symbolic AI are currently being used to support such detection and response. In this paper, we make the case for combining them using neurosymbolic AI. We identify a set of challenges when using AI today and propose a set of neurosymbolic use cases we believe are both interesting research directions for the neurosymbolic AI community and can have an impact on the cyber security field. We demonstrate feasibility through two proof-of-concept experiments.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:10.404Z},
  file = {/home/kchou/Documents/Zotero/storage/JB327P29/Grov et al. - 2024 - On the use of neurosymbolic AI for defending against cyber attacks.pdf;/home/kchou/Documents/Zotero/storage/Z9JHLPL5/2408.html}
}

@article{hansonEfficientReadingPapers2000,
  title = {Efficient Reading of Papers in Science and Technology},
  author = {Hanson, Michael J. and McNamee, Dylan J.},
  date = {2000},
  journaltitle = {On line],{$<$} http://www. cs. columbia. edu/‚àº hgs/netbib/efficientReading. pdf},
  url = {https://www.cse.ust.hk/~weiwa/teaching/Fall16-COMP6611B/reading_list/EfficientReading.pdf},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-12-28T08:52:58.943Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Hanson and McNamee - 2000 - Efficient reading of papers in science and technology - @hansonEfficientReadingPapers2000.pdf}
}

@online{huangLargeLanguageModels2024,
  title = {Large Language Models Based Fuzzing Techniques: A Survey},
  shorttitle = {Large Language Models Based Fuzzing Techniques},
  author = {Huang, Linghan and Zhao, Peizhou and Chen, Huaming and Ma, Lei},
  date = {2024},
  url = {https://arxiv.org/abs/2402.00350},
  pubstate = {prepublished},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-02-24T20:29:10.448Z},
  file = {/home/kchou/Documents/Zotero/storage/VHFFF3MZ/Huang et al. - 2024 - Large Language Models Based Fuzzing Techniques A Survey.pdf}
}

@online{jensenSoftwareVulnerabilityFunctionality2024,
  title = {Software {{Vulnerability}} and {{Functionality Assessment}} Using {{LLMs}}},
  author = {Jensen, Rasmus Ingemann Tuffveson and Tawosi, Vali and Alamir, Salwa},
  date = {2024-03-13},
  eprint = {2403.08429},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.08429},
  url = {http://arxiv.org/abs/2403.08429},
  abstract = {While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7\% of LLM-generated descriptions can be associated with true CWE vulnerabilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-11-05T12:44:58.371Z},
  file = {/home/kchou/Documents/Zotero/storage/M46N7ZJ9/Jensen et al. - 2024 - Software Vulnerability and Functionality Assessmen.pdf;/home/kchou/Documents/Zotero/storage/Q2ZPXUK7/Jensen et al. - 2024 - Software Vulnerability and Functionality Assessmen.html}
}

@article{keshavHowReadPaper2007,
  title = {How to Read a Paper},
  author = {Keshav, S.},
  date = {2007-07-20},
  journaltitle = {ACM SIGCOMM Computer Communication Review},
  shortjournal = {SIGCOMM Comput. Commun. Rev.},
  volume = {37},
  number = {3},
  pages = {83--84},
  issn = {0146-4833},
  doi = {10.1145/1273445.1273458},
  url = {https://dl.acm.org/doi/10.1145/1273445.1273458},
  abstract = {Researchers spend a great deal of time reading research papers. However, this skill is rarely taught, leading to much wasted effort. This article outlines a practical and efficient three-pass method for reading research papers. I also describe how to use this method to do a literature survey.},
  langid = {english},
  keywords = {üü¢},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-04-12T07:50:32.543Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Keshav_2007_How to read a paper.pdf}
}

@online{kimCodexitySecureAIassisted2024,
  title = {Codexity: {{Secure AI-assisted Code Generation}}},
  shorttitle = {Codexity},
  author = {Kim, Sung Yong and Fan, Zhiyu and Noller, Yannic and Roychoudhury, Abhik},
  date = {2024-05-07},
  eprint = {2405.03927},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.03927},
  url = {http://arxiv.org/abs/2405.03927},
  abstract = {Despite the impressive performance of Large Language Models (LLMs) in software development activities, recent studies show the concern of introducing vulnerabilities into software codebase by AI programming assistants (e.g., Copilot, CodeWhisperer). In this work, we present Codexity, a security-focused code generation framework integrated with five LLMs. Codexity leverages the feedback of static analysis tools such as Infer and CppCheck to mitigate security vulnerabilities in LLM-generated programs. Our evaluation in a real-world benchmark with 751 automatically generated vulnerable subjects demonstrates Codexity can prevent 60\% of the vulnerabilities being exposed to the software developer.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Software Engineering},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-10-19T08:53:23.126Z},
  file = {/home/kchou/Documents/Zotero/storage/9B9V8FP8/Kim et al. - 2024 - Codexity Secure AI-assisted Code Generation.pdf;/home/kchou/Documents/Zotero/storage/PLGHYNI7/2405 1.html}
}

@inproceedings{lazarWhyDoesCryptographic2014,
  title = {Why Does Cryptographic Software Fail? A Case Study and Open Problems},
  shorttitle = {Why Does Cryptographic Software Fail?},
  booktitle = {Proceedings of 5th {{Asia-Pacific Workshop}} on {{Systems}}},
  author = {Lazar, David and Chen, Haogang and Wang, Xi and Zeldovich, Nickolai},
  date = {2014-06-25},
  series = {{{APSys}} '14},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2637166.2637237},
  url = {https://doi.org/10.1145/2637166.2637237},
  abstract = {Mistakes in cryptographic software implementations often undermine the strong security guarantees offered by cryptography. This paper presents a systematic study of cryptographic vulnerabilities in practice, an examination of state-of-the-art techniques to prevent such vulnerabilities, and a discussion of open problems and possible future research directions. Our study covers 269 cryptographic vulnerabilities reported in the CVE database from January 2011 to May 2014. The results show that just 17\% of the bugs are in cryptographic libraries (which often have devastating consequences), and the remaining 83\% are misuses of cryptographic libraries by individual applications. We observe that preventing bugs in different parts of a system requires different techniques, and that no effective techniques exist to deal with certain classes of mistakes, such as weak key generation.},
  isbn = {978-1-4503-3024-4},
  keywords = {üü¢},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-10-12T16:34:56.082Z},
  file = {/home/kchou/Documents/Zotero/storage/6AXC6VYQ/Lazar et al. - 2014 - Why does cryptographic software fail a case study and open problems.pdf}
}

@article{lickliderManComputerSymbiosis1960,
  title = {Man-{{Computer Symbiosis}}},
  author = {Licklider, J. C. R.},
  date = {1960-03},
  journaltitle = {IRE Transactions on Human Factors in Electronics},
  volume = {HFE-1},
  number = {1},
  pages = {4--11},
  issn = {2168-2836},
  doi = {10.1109/THFE2.1960.4503259},
  url = {https://ieeexplore.ieee.org/document/4503259},
  abstract = {Man-computer symbiosis is an expected development in cooperative interaction between men and electronic computers. It will involve very close coupling between the human and the electronic members of the partnership. The main aims are 1) to let computers facilitate formulative thinking as they now facilitate the solution of formulated problems, and 2) to enable men and computers to cooperate in making decisions and controlling complex situations without inflexible dependence on predetermined programs. In the anticipated symbiotic partnership, men will set the goals, formulate the hypotheses, determine the criteria, and perform the evaluations. Computing machines will do the routinizable work that must be done to prepare the way for insights and decisions in technical and scientific thinking. Preliminary analyses indicate that the symbiotic partnership will perform intellectual operations much more effectively than man alone can perform them. Prerequisites for the achievement of the effective, cooperative association include developments in computer time sharing, in memory components, in memory organization, in programming languages, and in input and output equipment.},
  eventtitle = {{{IRE Transactions}} on {{Human Factors}} in {{Electronics}}},
  keywords = {Computer languages,Insects,Performance analysis,Performance evaluation,Symbiosis,Time sharing computer systems},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-02-24T20:46:18.841Z},
  file = {/home/kchou/Documents/Zotero/storage/GVAHN8BH/Licklider - 1960 - Man-Computer Symbiosis.pdf;/home/kchou/Documents/Zotero/storage/JYNFNB3N/4503259.html}
}

@article{liLanguageModelsPresent2022,
  title = {Language Models: Past, Present, and Future},
  shorttitle = {Language Models},
  author = {Li, Hang},
  date = {2022-06-21},
  journaltitle = {Commun. ACM},
  volume = {65},
  number = {7},
  pages = {56--63},
  issn = {0001-0782},
  doi = {10.1145/3490443},
  url = {https://dl.acm.org/doi/10.1145/3490443},
  abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-02-19T08:46:09.713Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Li - 2022 - Language models past, present, and future - @liLanguageModelsPresent2022.pdf}
}

@online{luongDANADomainAwareNeurosymbolic2024,
  title = {{{DANA}}: {{Domain-Aware Neurosymbolic Agents}} for {{Consistency}} and {{Accuracy}}},
  shorttitle = {{{DANA}}},
  author = {Luong, Vinh and Dinh, Sang and Raghavan, Shruti and Nguyen, William and Nguyen, Zooey and Le, Quynh and Vo, Hung and Maegaito, Kentaro and Nguyen, Loc and Nguyen, Thao and Ha, Anh Hai and Nguyen, Christopher},
  date = {2024-09-27},
  eprint = {2410.02823},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.02823},
  url = {http://arxiv.org/abs/2410.02823},
  abstract = {Large Language Models (LLMs) have shown remarkable capabilities, but their inherent probabilistic nature often leads to inconsistency and inaccuracy in complex problem-solving tasks. This paper introduces DANA (Domain-Aware Neurosymbolic Agent), an architecture that addresses these issues by integrating domain-specific knowledge with neurosymbolic approaches. We begin by analyzing current AI architectures, including AutoGPT, LangChain ReAct and OpenAI's ChatGPT, through a neurosymbolic lens, highlighting how their reliance on probabilistic inference contributes to inconsistent outputs. In response, DANA captures and applies domain expertise in both natural-language and symbolic forms, enabling more deterministic and reliable problem-solving behaviors. We implement a variant of DANA using Hierarchical Task Plans (HTPs) in the open-source OpenSSA framework. This implementation achieves over 90\textbackslash\% accuracy on the FinanceBench financial-analysis benchmark, significantly outperforming current LLM-based systems in both consistency and accuracy. Application of DANA in physical industries such as semiconductor shows that its flexible architecture for incorporating knowledge is effective in mitigating the probabilistic limitations of LLMs and has potential in tackling complex, real-world problems that require reliability and precision.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:03.283Z},
  file = {/home/kchou/Documents/Zotero/storage/YEDAKJZD/Luong et al. - 2024 - DANA Domain-Aware Neurosymbolic Agents for Consistency and Accuracy.pdf;/home/kchou/Documents/Zotero/storage/NBM45MUQ/2410.html}
}

@online{manesArtScienceEngineering2019,
  title = {The {{Art}}, {{Science}}, and {{Engineering}} of {{Fuzzing}}: {{A Survey}}},
  shorttitle = {The {{Art}}, {{Science}}, and {{Engineering}} of {{Fuzzing}}},
  author = {Manes, Valentin J. M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
  date = {2019-04-07},
  eprint = {1812.00140},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.00140},
  url = {http://arxiv.org/abs/1812.00140},
  abstract = {Among the many software vulnerability discovery techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
  pubstate = {prepublished},
  keywords = {üü¢,Computer Science - Cryptography and Security,Computer Science - Software Engineering},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-10-19T09:03:55.881Z},
  file = {/home/kchou/Documents/Zotero/storage/96BFEQHV/Manes et al. - 2019 - The Art, Science, and Engineering of Fuzzing A Survey.pdf;/home/kchou/Documents/Zotero/storage/WPKNTVBX/1812.html}
}

@online{meyerLessonsLearnedPrevious2013,
  title = {Lessons {{Learned From Previous SSL}}/{{TLS Attacks}} - {{A Brief Chronology Of Attacks And Weaknesses}}},
  author = {Meyer, Christopher and Schwenk, J√∂rg},
  date = {2013},
  number = {2013/049},
  url = {https://eprint.iacr.org/2013/049},
  abstract = {Since its introduction in 1994 the Secure Socket Layer (SSL) protocol (later renamed to Transport Layer Security (TLS)) evolved to the de facto standard for securing the transport layer. SSL/TLS can be used for ensuring data confidentiality, integrity and authenticity during transport. A main feature of the protocol is its flexibility. Modes of operation and security aims can easily be configured through different cipher suites. During its evolutionary development process several flaws were found. However, the flexible architecture of SSL/TLS allowed efficient fixes in order to counter the issues. This paper presents an overview on theoretical and practical attacks of the last 15 years, in chronological order and four categories: Attacks on the TLS Handshake protocol, on the TLS Record and Application Data Protocols, on the PKI infrastructure of TLS, and on various other attacks. We try to give a short ‚ÄùLessons Learned‚Äù at the end of each paragraph.},
  pubstate = {prepublished},
  keywords = {üü¢},
  annotation = {Publication info: Published elsewhere. SSL, TLS, Handshake Protocol, Record Layer, Public Key Infrastructures, Bleichenbacher Attack, Padding Oracles\\
Read\_Status: Read\\
Read\_Status\_Date: 2024-12-20T14:54:14.658Z},
  file = {/home/kchou/Documents/Zotero/storage/J8FPQU2C/Meyer and Schwenk - 2013 - Lessons Learned From Previous SSLTLS Attacks - A Brief Chronology Of Attacks And Weaknesses.pdf}
}

@online{mirzadehGSMSymbolicUnderstandingLimitations2024,
  title = {{{GSM-Symbolic}}: {{Understanding}} the {{Limitations}} of {{Mathematical Reasoning}} in {{Large Language Models}}},
  shorttitle = {{{GSM-Symbolic}}},
  author = {Mirzadeh, Iman and Alizadeh, Keivan and Shahrokhi, Hooman and Tuzel, Oncel and Bengio, Samy and Farajtabar, Mehrdad},
  date = {2024-10-07},
  eprint = {2410.05229},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.05229},
  url = {http://arxiv.org/abs/2410.05229},
  abstract = {Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65\%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-19T08:55:06.653Z},
  file = {/home/kchou/Documents/Zotero/storage/9JVFVUJH/Mirzadeh et al. - 2024 - GSM-Symbolic Understanding the Limitations of Mathematical Reasoning in Large Language Models.pdf}
}

@inreference{NeedhamSchroederProtocol2024,
  title = {Needham‚Äì{{Schroeder}} Protocol},
  booktitle = {Wikipedia},
  date = {2024-03-20T08:04:03Z},
  url = {https://en.wikipedia.org/w/index.php?title=Needham%E2%80%93Schroeder_protocol&oldid=1214650104},
  abstract = {The Needham‚ÄìSchroeder protocol is one of the two key transport protocols intended for use over an insecure network, both proposed by Roger Needham and Michael Schroeder. These are: The Needham‚ÄìSchroeder Symmetric Key Protocol, based on a symmetric encryption algorithm. It forms the basis for the Kerberos protocol. This protocol aims to establish a session key between two parties on a network, typically to protect further communication. The Needham‚ÄìSchroeder Public-Key Protocol, based on public-key cryptography. This protocol is intended to provide mutual authentication between two parties communicating on a network, but in its proposed form is insecure.},
  langid = {english},
  annotation = {Page Version ID: 1214650104},
  file = {/home/kchou/Documents/Zotero/storage/Y9RWY295/Needham‚ÄìSchroeder_protocol.html}
}

@article{nethercoteValgrindFrameworkHeavyweight2007,
  title = {Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation},
  shorttitle = {Valgrind},
  author = {Nethercote, Nicholas and Seward, Julian},
  date = {2007-06-10},
  journaltitle = {SIGPLAN Not.},
  volume = {42},
  number = {6},
  pages = {89--100},
  issn = {0362-1340},
  doi = {10.1145/1273442.1250746},
  url = {https://doi.org/10.1145/1273442.1250746},
  abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited.In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
  file = {/home/kchou/Documents/Zotero/storage/SZCWFZ46/Nethercote and Seward - 2007 - Valgrind a framework for heavyweight dynamic binary instrumentation.pdf}
}

@online{perryUsersWriteMore2023,
  title = {Do {{Users Write More Insecure Code}} with {{AI Assistants}}?},
  author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  date = {2023-12-18},
  eprint = {2211.03622},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2211.03622},
  url = {http://arxiv.org/abs/2211.03622},
  abstract = {We conduct the first large-scale user study examining how users interact with an AI Code assistant to solve a variety of security related tasks across different programming languages. Overall, we find that participants who had access to an AI assistant based on OpenAI's codex-davinci-002 model wrote significantly less secure code than those without access. Additionally, participants with access to an AI assistant were more likely to believe they wrote secure code than those without access to the AI assistant. Furthermore, we find that participants who trusted the AI less and engaged more with the language and format of their prompts (e.g. re-phrasing, adjusting temperature) provided code with fewer security vulnerabilities. Finally, in order to better inform the design of future AI-based Code assistants, we provide an in-depth analysis of participants' language and interaction behavior, as well as release our user interface as an instrument to conduct similar studies in the future.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-11-05T12:21:19.756Z},
  file = {/home/kchou/Documents/Zotero/storage/LWLBAZB7/Perry et al. - 2023 - Do Users Write More Insecure Code with AI Assistants.pdf;/home/kchou/Documents/Zotero/storage/DEL9Q2Z8/2211.html}
}

@inproceedings{reddiPINBinaryInstrumentation2004,
  title = {{{PIN}}: A Binary Instrumentation Tool for Computer Architecture Research and Education},
  shorttitle = {{{PIN}}},
  booktitle = {Proceedings of the 2004 Workshop on {{Computer}} Architecture Education: Held in Conjunction with the 31st {{International Symposium}} on {{Computer Architecture}}},
  author = {Reddi, Vijay Janapa and Settle, Alex and Connors, Daniel A. and Cohn, Robert S.},
  date = {2004-06-19},
  series = {{{WCAE}} '04},
  pages = {22--es},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1275571.1275600},
  url = {https://dl.acm.org/doi/10.1145/1275571.1275600},
  abstract = {Computer architecture embraces a tremendous number of ever-changing inter-connected concepts and information, yet computer architecture education is very often static, seemingly motionless. Computer architecture is commonly taught using simple piecewise methods of explaining how the hardware performs a given task, rather than characterizing the interaction of software and hardware. Visualization tools allow students to interactively explore basic concepts in computer architecture but are limited in their ability to engage students in research and design concepts. Likewise as the development of simulation models such as caches, branch predictors, and pipelines aid student understanding of architecture components, such models have limitations in the workloads that can be examined because of issues with execution time and environment. Overall, to effectively understand modern architectures, it is simply essential to experiment the characteristics of real application workloads. Likewise, understanding program behavior is necessary to effective programming, comprehension of architecture bottlenecks, and hardware design. Computer architecture education must include experience in analyzing program behavior and workload characteristics using effective tools. To explore workload characteristic analysis in computer architecture design, we propose using PIN, a binary instrumentation tool for computer architecture research and education projects.},
  isbn = {978-1-4503-4733-4},
  file = {/home/kchou/Documents/Zotero/storage/PY4WRJHV/Reddi et al. - 2004 - PIN a binary instrumentation tool for computer architecture research and education.pdf}
}

@online{shethNeurosymbolicAIWhy2023,
  title = {Neurosymbolic {{AI}} -- {{Why}}, {{What}}, and {{How}}},
  author = {Sheth, Amit and Roy, Kaushik and Gaur, Manas},
  date = {2023-05-01},
  eprint = {2305.00813},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.00813},
  url = {http://arxiv.org/abs/2305.00813},
  abstract = {Humans interact with the environment using a combination of perception - transforming sensory inputs from their environment into symbols, and cognition - mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Human perception-inspired machine perception, in the context of AI, refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition. On the other hand, machine cognition encompasses more complex computations, such as using knowledge of the environment to guide reasoning, analogy, and long-term planning. Humans can also control and explain their cognitive functions. This seems to require the retention of symbolic mappings from perception outputs to knowledge about their environment. For example, humans can follow and explain the guidelines and safety constraints driving their decision-making in safety-critical applications such as healthcare, criminal justice, and autonomous driving. This article introduces the rapidly emerging paradigm of Neurosymbolic AI combines neural networks and knowledge-guided symbolic approaches to create more capable and flexible AI systems. These systems have immense potential to advance both algorithm-level (e.g., abstraction, analogy, reasoning) and application-level (e.g., explainable and safety-constrained decision-making) capabilities of AI systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-02-19T16:36:56.658Z},
  file = {/home/kchou/Documents/Zotero/storage/FY8FG8KJ/Sheth et al. - 2023 - Neurosymbolic AI -- Why, What, and How.pdf;/home/kchou/Documents/Zotero/storage/VCXBXD55/2305.html}
}

@online{tilwaniNeurosymbolicAIApproach2024,
  title = {Neurosymbolic {{AI}} Approach to {{Attribution}} in {{Large Language Models}}},
  author = {Tilwani, Deepa and Venkataramanan, Revathy and Sheth, Amit P.},
  date = {2024-09-30},
  eprint = {2410.03726},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.03726},
  url = {http://arxiv.org/abs/2410.03726},
  abstract = {Attribution in large language models (LLMs) remains a significant challenge, particularly in ensuring the factual accuracy and reliability of the generated outputs. Current methods for citation or attribution, such as those employed by tools like Perplexity.ai and Bing Search-integrated LLMs, attempt to ground responses by providing real-time search results and citations. However, so far, these approaches suffer from issues such as hallucinations, biases, surface-level relevance matching, and the complexity of managing vast, unfiltered knowledge sources. While tools like Perplexity.ai dynamically integrate web-based information and citations, they often rely on inconsistent sources such as blog posts or unreliable sources, which limits their overall reliability. We present that these challenges can be mitigated by integrating Neurosymbolic AI (NesyAI), which combines the strengths of neural networks with structured symbolic reasoning. NesyAI offers transparent, interpretable, and dynamic reasoning processes, addressing the limitations of current attribution methods by incorporating structured symbolic knowledge with flexible, neural-based learning. This paper explores how NesyAI frameworks can enhance existing attribution models, offering more reliable, interpretable, and adaptable systems for LLMs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:19:58.738Z},
  file = {/home/kchou/Documents/Zotero/storage/KIZCU9ZW/Tilwani et al. - 2024 - Neurosymbolic AI approach to Attribution in Large Language Models.pdf;/home/kchou/Documents/Zotero/storage/XJRLMJ4Q/2410 1.html}
}

@online{tzachristasCreatingLLMbasedAIagent2024,
  title = {Creating an {{LLM-based AI-agent}}: {{A}} High-Level Methodology towards Enhancing {{LLMs}} with {{APIs}}},
  shorttitle = {Creating an {{LLM-based AI-agent}}},
  author = {Tzachristas, Ioannis},
  date = {2024-12-21},
  eprint = {2412.13233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.13233},
  url = {http://arxiv.org/abs/2412.13233},
  abstract = {Large Language Models (LLMs) have revolutionized various aspects of engineering and science. Their utility is often bottlenecked by the lack of interaction with the external digital environment. To overcome this limitation and achieve integration of LLMs and Artificial Intelligence (AI) into real-world applications, customized AI agents are being constructed. Based on the technological trends and techniques, we extract a high-level approach for constructing these AI agents, focusing on their underlying architecture. This thesis serves as a comprehensive guide that elucidates a multi-faceted approach for empowering LLMs with the capability to leverage Application Programming Interfaces (APIs). We present a 7-step methodology that begins with the selection of suitable LLMs and the task decomposition that is necessary for complex problem-solving. This methodology includes techniques for generating training data for API interactions and heuristics for selecting the appropriate API among a plethora of options. These steps eventually lead to the generation of API calls that are both syntactically and semantically aligned with the LLM's understanding of a given task. Moreover, we review existing frameworks and tools that facilitate these processes and highlight the gaps in current attempts. In this direction, we propose an on-device architecture that aims to exploit the functionality of carry-on devices by using small models from the Hugging Face community. We examine the effectiveness of these approaches on real-world applications of various domains, including the generation of a piano sheet. Through an extensive analysis of the literature and available technologies, this thesis aims to set a compass for researchers and practitioners to harness the full potential of LLMs augmented with external tool capabilities, thus paving the way for more autonomous, robust, and context-aware AI agents.},
  pubstate = {prepublished},
  version = {2},
  keywords = {üü¢,Computer Science - Software Engineering},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-01-06T14:38:40.827Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Tzachristas - 2024 - Creating an LLM-based AI-agent A high-level methodology towards enhancing LLMs with APIs - @tzachristasCreatingLLMbasedAIagent2024.pdf;/home/kchou/Documents/Braindump/998 Attachments/PDFs/2412.html}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-02-21T21:12:48.910Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Vaswani et al_2023_Attention Is All You Need.pdf;/home/kchou/Documents/Zotero/storage/VWIT3A5A/1706.html}
}

@online{wangProphetFuzzFullyAutomated2024,
  title = {{{ProphetFuzz}}: {{Fully Automated Prediction}} and {{Fuzzing}} of {{High-Risk Option Combinations}} with {{Only Documentation}} via {{Large Language Model}}},
  shorttitle = {{{ProphetFuzz}}},
  author = {Wang, Dawei and Zhou, Geng and Chen, Li and Li, Dan and Miao, Yukai},
  date = {2024-09-01},
  eprint = {2409.00922},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1145/3658644.3690231},
  url = {http://arxiv.org/abs/2409.00922},
  abstract = {Vulnerabilities related to option combinations pose a significant challenge in software security testing due to their vast search space. Previous research primarily addressed this challenge through mutation or filtering techniques, which inefficiently treated all option combinations as having equal potential for vulnerabilities, thus wasting considerable time on non-vulnerable targets and resulting in low testing efficiency. In this paper, we utilize carefully designed prompt engineering to drive the large language model (LLM) to predict high-risk option combinations (i.e., more likely to contain vulnerabilities) and perform fuzz testing automatically without human intervention. We developed a tool called ProphetFuzz and evaluated it on a dataset comprising 52 programs collected from three related studies. The entire experiment consumed 10.44 CPU years. ProphetFuzz successfully predicted 1748 high-risk option combinations at an average cost of only \textbackslash\$8.69 per program. Results show that after 72 hours of fuzzing, ProphetFuzz discovered 364 unique vulnerabilities associated with 12.30\textbackslash\% of the predicted high-risk option combinations, which was 32.85\textbackslash\% higher than that found by state-of-the-art in the same timeframe. Additionally, using ProphetFuzz, we conducted persistent fuzzing on the latest versions of these programs, uncovering 140 vulnerabilities, with 93 confirmed by developers and 21 awarded CVE numbers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-19T08:53:01.845Z},
  file = {/home/kchou/Documents/Zotero/storage/AK4SUYB9/Wang et al. - 2024 - ProphetFuzz Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Docum 2.pdf;/home/kchou/Documents/Zotero/storage/37Z9FRNF/2409.html}
}

@article{whitesidesWhitesidesGroupWriting2004,
  title = {Whitesides' {{Group}}: {{Writing}} a {{Paper}}},
  shorttitle = {Writing a {{Paper}}},
  author = {Whitesides, G. M.},
  date = {2004-08-04},
  journaltitle = {Advanced Materials},
  shortjournal = {Advanced Materials},
  volume = {16},
  number = {15},
  pages = {1375--1377},
  issn = {0935-9648, 1521-4095},
  doi = {10.1002/adma.200400767},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/adma.200400767},
  abstract = {Insights into conducting research and the writing of scientific papers are given by Prof. Whitesides in this short essay. The manuscript and its guidelines has been circulated within the Whitesides' research group since 1989.},
  langid = {english},
  keywords = {üü¢},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-12-26T21:47:50.020Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Whitesides - 2004 - Whitesides' Group Writing a Paper - @whitesidesWhitesidesGroupWriting2004a.pdf}
}

@online{WritingSystemsNetworking,
  title = {Writing {{Systems}} and {{Networking Articles}}},
  url = {https://www.cs.columbia.edu/~hgs/etc/writing-style.html},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-12-20T12:00:19.517Z},
  file = {/home/kchou/Documents/Braindump/998 Attachments/PDFs/Writing Systems and Networking Articles (12_19_2024 3Ôºö49Ôºö01 PM).html}
}

@online{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  date = {2023-03-10},
  eprint = {2210.03629},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  url = {http://arxiv.org/abs/2210.03629},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-21T16:44:45.166Z},
  file = {/home/kchou/Documents/Zotero/storage/NNE8L2UK/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf;/home/kchou/Documents/Zotero/storage/EK4XL88H/2210.html}
}

@online{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  date = {2023-12-03},
  eprint = {2305.10601},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10601},
  url = {http://arxiv.org/abs/2305.10601},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-10-15T11:27:12.812Z},
  file = {/home/kchou/Documents/Zotero/storage/WG3CLL9M/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with Large Language Models.pdf;/home/kchou/Documents/Zotero/storage/TL76D3WD/2305 1.html}
}

@online{zebazeTreeProblemsImproving2024,
  title = {Tree of {{Problems}}: {{Improving}} Structured Problem Solving with Compositionality},
  shorttitle = {Tree of {{Problems}}},
  author = {Zebaze, Armel and Sagot, Beno√Æt and Bawden, Rachel},
  date = {2024-10-09},
  eprint = {2410.06634},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.06634},
  url = {http://arxiv.org/abs/2410.06634},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-19T08:55:10.104Z},
  file = {/home/kchou/Documents/Zotero/storage/MLMPWKXQ/Zebaze et al. - 2024 - Tree of Problems Improving structured problem solving with compositionality.pdf}
}
