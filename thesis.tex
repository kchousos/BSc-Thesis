% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \ifLuaTeX
    \usepackage{luaotfload}
    \directlua{luaotfload.add_fallback("mainfontfallback",{
      "Liberation Serif:","NotoColorEmoji:mode=harf"
    })}
  \fi
  \setmainfont[,RawFeature={fallback=mainfontfallback}]{Libertinus
Serif}
  \setsansfont[]{Libertinus Sans}
  \setmonofont[]{Iosevka}
  \setmathfont[]{Libertinus Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{254,254,254}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[style=ieee,]{biblatex}
\addbibresource{resources/bibliography.bib}


\pagenumbering{roman}
\usepackage{lineno}
\linenumbers
\renewcommand*{\bibfont}{\small}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{algorithm}{}{\usepackage{algorithm}}
\makeatother
\makeatletter
\@ifpackageloaded{algpseudocode}{}{\usepackage{algpseudocode}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={OverHAuL},
  pdfauthor={Konstantinos Chousos},
  pdfkeywords={LLMs, Fuzzing, Automation, Security, Neurosymbolic AI},
  colorlinks=true,
  linkcolor={blue},
  filecolor={cyan},
  citecolor={red},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}


\title{OverHAuL}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Harnessing Automation for C Libraries via LLMs}
\author{Konstantinos Chousos}
\date{July, 2025}
\begin{document}
\maketitle
\begin{abstract}
Lorem ipsum odor amet, consectetuer adipiscing elit. Habitasse congue
tempus erat rhoncus sapien interdum dolor nec. Posuere habitant metus
tellus erat eu. Risus ultricies eu rhoncus, conubia euismod convallis
commodo per. Nam tellus quisque maximus duis eleifend; arcu aptent. Nisi
rutrum primis luctus tortor tempor maecenas. Donec curae cras dolor;
malesuada ultricies scelerisque. Molestie class tincidunt quis gravida
ut proin. Consequat lacinia arcu justo leo maecenas nunc neque ex.
Platea eros ullamcorper nullam rutrum facilisis.
\end{abstract}

\floatname{algorithm}{Algorithm}

\numberwithin{algorithm}{chapter}


\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}

\markboth{Preface}{Preface}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat
leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque
habitant morbi tristique senectus et netus et malesuada fames ac turpis
egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere
iaculis. Suspendisse et maximus elit. In fringilla gravida ornare.
Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel
neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet
vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor
lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque
ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse
potenti.

\newpage{}

\bookmarksetup{startatroot}

\chapter*{Acknowledgments}\label{acknowledgments}

\markboth{Acknowledgments}{Acknowledgments}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

\newpage{}

\tableofcontents

\newpage{}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

\pagenumbering{arabic}

\section{Motivation}\label{motivation}

\begin{itemize}
\tightlist
\item
  Memory unsafety is and will be prevalent
\item
  Software is safe until it's not
\item
  Humans make mistakes
\item
  Humans now use Large Language Models (LLMs) to write software
\item
  LLMs make mistakes \autocite{perry2023}
\end{itemize}

Result: Bugs exist

\section{Goal}\label{goal}

A system that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Takes a bare C project as input
\item
  Generates a new fuzzing harness from scratch using LLMs
\item
  Compiles it
\item
  Executes it and evaluates it
\end{enumerate}

\section{Preview of following sections
(rename)}\label{preview-of-following-sections-rename}

\bookmarksetup{startatroot}

\chapter{Background}\label{background}

\section{Fuzz Testing}\label{fuzz-testing}

\emph{Fuzzing} is an automated software-testing technique in which a
\emph{Program Under Test} (PUT) is executed with (pseudo-)random inputs
in the hope of exposing undefined behavior. When such behavior manifests
as a crash, hang, or memory-safety violation, the corresponding input
constitutes a \emph{test-case} that reveals a bug and often a
vulnerability \autocite{manes2019}. In essence, fuzzing is a form of
adversarial, penetration-style testing carried out by the defender
before the adversary has an opportunity to do so. Interest in the
technique surged after the publication of three practitioner-oriented
books in 2007--2008 \autocite{takanen2018,sutton2007,rathaus2007}.

Historically, the term was coined by Miller et al.~in 1990, who used
``fuzz'' to describe a program that ``generates a stream of random
characters to be consumed by a target program'' \autocite{miller1990}.
This informal usage captured the essence of what fuzzing aims to do:
stress test software by bombarding it with unexpected inputs to reveal
bugs. To formalize this concept, we adopt Manes et al.'s rigorous
definitions \autocite{manes2019}:

\begin{definition}[Fuzzing]\protect\hypertarget{def-fuzzing}{}\label{def-fuzzing}

Fuzzing is the execution of a Program Under Test (PUT) using input(s)
sampled from an input space (the \emph{fuzz input space}) that protrudes
the expected input space of the PUT.

\end{definition}

This means fuzzing involves running the target program on inputs that go
beyond those it is typically designed to handle, aiming to uncover
hidden issues. An individual instance of such execution---or a bounded
sequence thereof---is called a \emph{fuzzing run}. When these runs are
conducted systematically and at scale with the specific goal of
detecting violations of a security policy, the activity is known as
\emph{fuzz testing} (or simply \emph{fuzzing}):

\begin{definition}[Fuzz
Testing]\protect\hypertarget{def-fuzz-testing}{}\label{def-fuzz-testing}

Fuzz testing is the use of fuzzing to test whether a PUT violates a
security policy.

\end{definition}

This distinction highlights that fuzz testing is fuzzing with an
explicit focus on security properties and policy enforcement. Central to
managing this process is the \emph{fuzzer engine}, which orchestrates
the execution of one or more fuzzing runs as part of a \emph{fuzz
campaign}. A fuzz campaign represents a concrete instance of fuzz
testing tailored to a particular program and security policy:

\begin{definition}[Fuzzer, Fuzzer
Engine]\protect\hypertarget{def-fuzzer}{}\label{def-fuzzer}

A fuzzer is a program that performs fuzz testing on a PUT.

\end{definition}

\begin{definition}[Fuzz
Campaign]\protect\hypertarget{def-campaign}{}\label{def-campaign}

A fuzz campaign is a specific execution of a fuzzer on a PUT with a
specific security policy.

\end{definition}

Throughout each execution within a campaign, a \emph{bug oracle} plays a
critical role in evaluating the program's behavior to determine whether
it violates the defined security policy:

\begin{definition}[Bug
Oracle]\protect\hypertarget{def-oracle}{}\label{def-oracle}

A bug oracle is a component (often inside the fuzzer) that determines
whether a given execution of the PUT violates a specific security
policy.

\end{definition}

In practice, bug oracles often rely on runtime instrumentation
techniques, such as monitoring for fatal POSIX signals (e.g.,
\texttt{SIGSEGV}) or using sanitizers like AddressSanitizer (ASan)
\autocite{serebryany2012}. Tools like LibFuzzer \autocite{libfuzzer}
commonly incorporate such instrumentation to reliably identify crashes
or memory errors during fuzzing.

Most fuzz campaigns begin with a set of \emph{seeds}---inputs that are
well-formed and belong to the PUT's expected input space---called a
\emph{seed corpus}. These seeds serve as starting points from which the
fuzzer generates new test cases by applying transformations or
mutations, thereby exploring a broader input space:

\begin{definition}[Seed]\protect\hypertarget{def-seed}{}\label{def-seed}

An input given to the PUT that is mutated by the fuzzer to produce new
test cases. During a fuzz campaign (Definition~\ref{def-campaign}) all
seeds are stored in a seed \emph{pool} or \emph{corpus}.

\end{definition}

The process of selecting an effective initial corpus is crucial because
it directly impacts how quickly and thoroughly the fuzzer can cover the
target program's code. This challenge---studied as the
\emph{seed-selection problem}---involves identifying seeds that enable
rapid discovery of diverse execution paths and is non-trivial
\autocite{rebert2014}. A well-chosen seed set often accelerates bug
discovery and improves overall fuzzing efficiency.

\subsection{Motivation}\label{motivation-1}

\begin{quote}
The purpose of fuzzing relies on the assumption that there are bugs
within every program, which are waiting to be discovered. Therefore, a
systematic approach should find them sooner or later.

--- OWASP Foundation \autocite{owaspfoundation}
\end{quote}

Fuzz testing offers several tangible benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Early vulnerability discovery}: Detecting defects during
  development is cheaper and safer than addressing exploits in
  production.
\item
  \textbf{Adversary-parity}: Performing the same randomised stress that
  attackers employ allows defenders to pre-empt zero-days.
\item
  \textbf{Robustness and correctness}: Beyond security, fuzzing exposes
  logic errors and stability issues in complex, high-throughput APIs
  (e.g., decompressors) even when inputs are \emph{expected} to be
  well-formed.
\item
  \textbf{Regression prevention}: Re-running a corpus of crashing inputs
  as part of continuous integration ensures that fixed bugs remain
  fixed.
\end{enumerate}

\subsubsection{Success Stories}\label{success-stories}

\emph{Heartbleed} (CVE-2014-0160) \autocite{heartbleed,heartbleed-cve}
arose from a buffer over-read\footnote{\url{https://xkcd.com/1354/}} in
OpenSSL \autocite{theopensslproject2025} introduced on 1 February 2012
and unnoticed until 1 April 2014. Post-mortem analyses showed that a
simple fuzz campaign exercising the TLS heartbeat extension would have
revealed the defect almost immediately \autocite{wheeler2014}.

Likewise, the \emph{Shellshock} (or \emph{Bashdoor}) family of bugs in
GNU Bash \autocite{bash} enabled arbitrary command execution on many
UNIX systems. While the initial flaw was fixed promptly, subsequent bug
variants were discovered by Google's Michał Zalewski using his own
fuzzer \autocite{afl} in late 2014 \autocite{saarinen2014}.

On the defensive tooling side, the security tool named
\emph{Mayhem}---developed by the company of the same name---has since
been adopted by the US Air Force, the Pentagon, Cloudflare, and numerous
open-source communities. It has found and facilitated the remediation of
thousands of previously unknown vulnerabilities
\autocite{simonite2020mayhem}.

These cases underscore the central thesis of fuzz testing: exhaustive
manual review is infeasible, but scalable stochastic exploration
reliably surfaces the critical few defects that matter most.

\subsection{Methodology}\label{methodology}

As previously discussed, fuzz testing of a program under test (PUT) is
typically conducted using a dedicated fuzzing engine (see
Definition~\ref{def-fuzzer}). Among the most widely adopted fuzzers for
C and C++ projects and libraries are AFL \autocite{afl}---which has
since evolved into AFL++ \autocite{aflpp}---and LibFuzzer
\autocite{libfuzzer}. Within the OverHAuL framework, LibFuzzer is
preferred owing to its superior suitability for library fuzzing, whereas
AFL++ predominantly targets executables and binary fuzzing.

\subsubsection{LibFuzzer}\label{libfuzzer}

LibFuzzer \autocite{libfuzzer} is an in-process, coverage-guided
evolutionary fuzzing engine primarily designed for testing libraries. It
forms part of the LLVM ecosystem \autocite{llvm} and operates by linking
directly with the library under evaluation. The fuzzer delivers mutated
input data to the library through a designated fuzzing entry point,
commonly referred to as the \emph{fuzz target}.

\begin{definition}[Fuzz
target]\protect\hypertarget{def-target}{}\label{def-target}

A function that accepts a byte array as input and exercises the
application programming interface (API) under test using these inputs
\autocite{libfuzzer}. This construct is also known as a \emph{fuzz
driver}, \emph{fuzzer entry point}, or \emph{fuzzing harness}.

\end{definition}

For the remainder of this thesis, the terms presented in
Definition~\ref{def-target} will be used interchangeably.

To effectively validate an implementation or library, developers are
required to author a fuzzing harness that invokes the target library's
API functions utilizing the fuzz-generated inputs. This harness serves
as the principal interface for the fuzzer and is executed iteratively,
each time with mutated input designed to maximize code coverage and
uncover defects. To comply with LibFuzzer's interface requirements, a
harness must conform to the following function signature:

\begin{codelisting}

\caption{\label{lst-basic-example}This function receives the fuzzing
input via a pointer to an array of bytes (\texttt{Data}) and its
associated size (\texttt{Size}). Efficiency in fuzzing is achieved by
invoking the API of interest within the body of this function, thereby
allowing the fuzzer to explore a broad spectrum of behavior through
systematic input mutation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\DataTypeTok{int}\NormalTok{ LLVMFuzzerTestOneInput}\OperatorTok{(}\DataTypeTok{const} \DataTypeTok{uint8\_t} \OperatorTok{*}\NormalTok{Data}\OperatorTok{,} \DataTypeTok{size\_t}\NormalTok{ Size}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{  DoSomethingInterestingWithData}\OperatorTok{(}\NormalTok{Data}\OperatorTok{,}\NormalTok{ Size}\OperatorTok{);}
  \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

A more illustrative example of such a harness is provided in
Listing~\ref{lst-fuzzing-example}.

\begin{codelisting}

\caption{\label{lst-fuzzing-example}This example demonstrates a minimal
harness that triggers a controlled crash upon receiving \texttt{HI!} as
input.}

\centering{

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\CommentTok{// test\_fuzzer.cpp}
\PreprocessorTok{\#include }\ImportTok{\textless{}stdint.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}stddef.h\textgreater{}}

\AttributeTok{extern} \StringTok{"C"} \DataTypeTok{int}\NormalTok{ LLVMFuzzerTestOneInput}\OperatorTok{(}\AttributeTok{const} \DataTypeTok{uint8\_t} \OperatorTok{*}\NormalTok{data}\OperatorTok{,} \DataTypeTok{size\_t}\NormalTok{ size}\OperatorTok{)} \OperatorTok{\{}
  \ControlFlowTok{if} \OperatorTok{(}\NormalTok{size }\OperatorTok{\textgreater{}} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ data}\OperatorTok{[}\DecValTok{0}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}H\textquotesingle{}}\OperatorTok{)}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{size }\OperatorTok{\textgreater{}} \DecValTok{1} \OperatorTok{\&\&}\NormalTok{ data}\OperatorTok{[}\DecValTok{1}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}I\textquotesingle{}}\OperatorTok{)}
      \ControlFlowTok{if} \OperatorTok{(}\NormalTok{size }\OperatorTok{\textgreater{}} \DecValTok{2} \OperatorTok{\&\&}\NormalTok{ data}\OperatorTok{[}\DecValTok{2}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}!\textquotesingle{}}\OperatorTok{)}
        \FunctionTok{\_\_builtin\_trap}\OperatorTok{();}
  \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

To compile and link such a harness with LibFuzzer, the Clang
compiler---also part of the LLVM project \autocite{llvm}---must be used
alongside appropriate compiler flags. For instance, compiling the
harness in Listing~\ref{lst-fuzzing-example} can be achieved as follows:

\begin{codelisting}

\caption{\label{lst-harness-compilation}This example illustrates the
compilation and execution workflow necessary for deploying a
LibFuzzer-based fuzzing harness.}

\centering{

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\CommentTok{\# Compile test\_fuzzer.cc with AddressSanitizer and link against LibFuzzer.}
\FunctionTok{clang}\NormalTok{++ }\AttributeTok{{-}fsanitize}\OperatorTok{=}\NormalTok{address,fuzzer test\_fuzzer.cc}
\CommentTok{\# Execute the fuzzer without any pre{-}existing seed corpus.}
\ExtensionTok{./a.out}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{AFL and AFL++}\label{afl-and-afl}

\emph{American Fuzzy Lop} (AFL) \autocite{afl}, developed by Michał
Zalewski, is a seminal fuzzer targeting C and C++ applications. Its core
methodology relies on instrumented binaries to provide edge coverage
feedback, thereby guiding input mutation towards unexplored program
paths. AFL supports several emulation backends including QEMU
\autocite{bellard2025}---an open-source CPU emulator facilitating
fuzzing on diverse architectures---and Unicorn
\autocite{unicornengine2025}, a lightweight multi-platform CPU emulator.
While AFL established itself as a foundational tool within the fuzzing
community, its successor AFL++ \autocite{aflpp} incorporates numerous
enhancements and additional features to improve fuzzing efficacy.

AFL operates by ingesting seed inputs from a specified directory
(\texttt{seeds\_dir}), applying mutations, and then executing the target
binary to discover novel execution paths. Execution can be initiated
using the following command-line syntax:

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\ExtensionTok{./afl{-}fuzz} \AttributeTok{{-}i}\NormalTok{ seeds\_dir }\AttributeTok{{-}o}\NormalTok{ output\_dir }\AttributeTok{{-}{-}}\NormalTok{ /path/to/tested/program}
\end{Highlighting}
\end{Shaded}

AFL is capable of fuzzing both black-box and instrumented binaries,
employing a fork-server mechanism to optimize performance. It
additionally supports persistent mode execution as well as modes
leveraging QEMU and Unicorn emulators, thereby providing extensive
flexibility for different testing environments.

Although AFL is traditionally utilized for fuzzing standalone programs
or binaries, it is also capable of fuzzing libraries and other software
components. In such scenarios, rather than implementing the
\texttt{LLVMFuzzerTestOneInput} style harness, AFL can use the standard
\texttt{main()} function as the fuzzing entry point. Nonetheless, AFL
also accommodates integration with \texttt{LLVMFuzzerTestOneInput}-based
harnesses, underscoring its adaptability across varied fuzzing use
cases.

\subsection{Challenges in Adoption}\label{challenges-in-adoption}

Despite its potential for uncovering software vulnerabilities, fuzzing
remains a relatively underutilized testing technique compared to more
established methodologies such as Test-Driven Development (TDD). This
limited adoption can be attributed, in part, to the substantial initial
investment required to design and implement appropriate test harnesses
that enable effective fuzzing processes. Furthermore, the interpretation
of fuzzing outcomes---particularly the identification, diagnostic
analysis, and prioritization of program crashes---demands considerable
resources and specialized expertise. These factors collectively pose
significant barriers to the widespread integration of fuzzing within
standard software development and testing practices.

\section{Large Language Models}\label{large-language-models}

Natural Language Processing (NLP), a subfield of Artificial Intelligence
(AI), has a rich and ongoing history that has evolved significantly
since its beginning in the 1990s \autocite{li2022,wang2025}. Among the
most notable---and recent---advancements in this domain are Large
Language Models (LLMs), which have transformed the landscape of NLP and
AI in general.

At the core of many LLMs is the attention mechanism, which was
introduced by Bahdanau et al.~in 2014 \autocite{bahdanau2016}. This
pivotal innovation enabled models to focus on relevant parts of the
input sequence when making predictions, significantly improving language
understanding and generation tasks. Building on this foundation, the
Transformer architecture was proposed by Vaswani et al.~in 2017
\autocite{vaswani2023}. This architecture has become the backbone of
most contemporary LLMs, as it efficiently processes sequences of data,
capturing long-range dependencies without being hindered by sequential
processing limitations.

One of the first major breakthroughs utilizing the Transformer
architecture was BERT (Bidirectional Encoder Representations from
Transformers), developed by Devlin et al.~in 2019 \autocite{devlin2019}.
BERT's bi-directional understanding allowed it to capture the context of
words from both directions, which improved the accuracy of various NLP
tasks. Following this, the Generative Pre-trained Transformer (GPT)
series, initiated by OpenAI with the original GPT model in 2018
\autocite{radford2018}, further pushed the boundaries. Subsequent
iterations, including GPT-2 \autocite{radford2019}, GPT-3
\autocite{brown2020}, and the most current GPT-4 \autocite{openai2024},
have continued to enhance performance by scaling model size, data, and
training techniques.

In addition to OpenAI's contributions, other significant models have
emerged, such as Claude, DeepSeek-R1 and the Llama series (1 through 3)
\autocite{claude,deepseek-ai2025,grattafiori2024}. The proliferation of
LLMs has sparked an active discourse about their capabilities,
applications, and implications in various fields.

\subsection{Biggest GPTs}\label{biggest-gpts}

User-facing LLMs are generally categorized between closed-source and
open-source models. Closed-source LLMs like ChatGPT, Claude, and Gemini
\autocite{chatgpt,claude,gemini} represent commercially developed
systems often optimized for specific tasks without public access to
their underlying weights. In contrast, open-source models\footnote{The
  term ``open-source'' models is somewhat misleading, since these are
  better termed as \emph{open-weights} models. While their weights are
  publicly available, their training data and underlying code are often
  proprietary. This terminology reflects community usage but fails to
  capture the limitations of transparency and accessibility inherent in
  these models.}, including the Llama series \autocite{grattafiori2024}
and Deepseek \autocite{deepseek-ai2025}, provide researchers and
practitioners with access to model weights, allowing for greater
transparency and adaptability.

\subsection{Prompting}\label{sec-prompting}

Interaction with LLMs typically occurs through chat-like interfaces, a
process commonly referred to as \emph{prompting}. A critical aspect of
effective engagement with LLMs is the usage of different prompting
strategies, which can significantly influence the quality and relevance
of the generated outputs. Various approaches to prompting have been
developed and studied, including zero-shot and few-shot prompting. In
zero-shot prompting, the model is expected to perform a specific task
without any examples, while in few-shot prompting, the user provides a
limited number of examples to guide the model's responses
\autocite{brown2020}.

To enhance performance on more complex tasks, several advanced prompting
techniques have emerged. One notable strategy is the \emph{Chain of
Thought} approach \autocite{chainofthought}, which entails presenting
the model with sample thought processes for solving a given task. This
method encourages the model to generate more coherent and logical
reasoning by mimicking human-like cognitive pathways. A refined variant
of this approach is the \emph{Tree of Thoughts} technique
\autocite{yao2023}, which enables the LLM to explore multiple lines of
reasoning concurrently, thereby facilitating the selection of the most
promising train of thought for further exploration.

In addition to these cognitive strategies, Retrieval-Augmented
Generation (RAG) \autocite{lewis2021} is another innovative technique
that enhances the model's capacity to provide accurate information by
incorporating external knowledge not present in its training dataset.
RAG operates by integrating the LLM with an external storage
system---often a vector store containing relevant documents---that the
model can query in real-time. This allows the LLM to pull up pertinent
and/or proprietary information in response to user queries, resulting in
more comprehensive and accurate answers.

Moreover, the ReAct framework \autocite{reAct}, which stands for
Reasoning and Acting, empowers LLMs by granting access to external
tools. This capability allows LLM instances to function as intelligent
agents that can interact meaningfully with their environment through
user-defined tools. For instance, a ReAct tool could be a function that
returns a weather forecast based on the user's current location. In this
scenario, the LLM can provide accurate and truthful predictions, thereby
mitigating risks associated with hallucinated responses.

\subsection{LLMs for Coding}\label{sec-llm-coding}

The impact of LLMs in software development in recent years is apparent,
with hundreds of LLM-assistance extensions and Integrated Development
Environments (IDEs) being published. Notable instances include tools
like GitHub Copilot and IDEs such as Cursor, which leverage LLM
capabilities to provide developers with coding suggestions,
auto-completions, and even real-time debugging assistance
\autocite{cursor,ghcopilot}. Such innovations have introduced a layer of
interaction that enhances productivity and fosters a more intuitive
coding experience. Simultaneously, certain LLMs are trained themselves
with the code-generation task in mind
\autocite{nijkamp2023a,nijkamp2023,openai2025a}.

One exemplary product of this innovation is \emph{vibecoding} and the
no-code movement, which describe the development of software by only
prompting and tasking an LLM, i.e.~without any actual programming
required by the user. This constitutes a showcase of how LLMs can be
harnessed to elevate the coding experience by supporting developers as
they navigate complex programming tasks \autocite{sarkar2025}. By
analyzing the context of the code being written, these sophisticated
models can provide contextualized insights and relevant snippets,
effectively streamlining the development process. Developers can benefit
from reduced cognitive load, as they receive suggestions that not only
cater to immediate coding needs but also promote adherence to best
practices and coding standards.

Despite these advancements, it is crucial to recognize the inherent
limitations of LLMs when applied to software development. While they can
help in many aspects of coding, they are not immune to generating
erroneous outputs---a phenomenon often referred to as ``hallucination''.
Hallucinations occur when LLMs produce information that is unfounded or
inaccurate, which can stem from several factors, including the
limitations of their training data and the constrained context window
within which they operate. As LLMs generate code suggestions based on
the patterns learned from vast datasets, they may inadvertently propose
solutions that do not align with the specific requirements of a task or
that utilize outdated programming paradigms.

Moreover, the challenge of limited context windows can lead to
suboptimal suggestions. LLMs generally process a fixed amount of text
when generating responses, which can impact their ability to fully grasp
the nuances of complex coding scenarios. This may result in outputs that
lack the necessary depth and specificity required for successful
implementation. As a consequence, developers must exercise caution and
critically evaluate the suggestions offered by these models, as reliance
on them without due diligence could lead to the introduction of bugs or
other issues in the code.

\subsection{LLMs for Fuzzing}\label{llms-for-fuzzing}

While large language models (LLMs) demonstrate significant potential in
enhancing the software development process, the challenges highlighted
in Section~\ref{sec-llm-coding} become even more pronounced and
troublesome when these models are employed to generate fuzzing
harnesses. The task of writing a fuzzing harness inherently demands an
in-depth comprehension of both the library being tested and the
intricate interactions expected among its various components. This level
of understanding is often beyond the capabilities of LLMs, primarily due
to their context window limitations, which restrict the amount of
information they can effectively process and retain during code
generation.

In addition to this issue, the risk of error-prone code produced by LLMs
further complicates the fuzzing workflow. When a crash occurs during the
fuzzing process, it becomes imperative for developers to ascertain that
the root cause of the failure is not attributable to deficiencies or
bugs within the harness itself. This additional layer of verification
adds to the cognitive load placed upon developers, potentially
detracting from their ability to focus on testing and improving the
underlying software.

To enhance the reliability of LLM-generated harnesses in fuzzing
contexts, it is essential that these generated artifacts undergo
thorough evaluation and validation through programmatic means. This
involves the implementation of systematic techniques that assess the
accuracy and robustness of the generated code, ensuring that it aligns
with the expected behavior of the components it is intended to interact
with. This strategy can be conceptualized within the framework of
Neurosymbolic AI (Section~\ref{sec-nesy}), which seeks to integrate the
strengths of neural networks with symbolic reasoning capabilities. By
marrying these two paradigms, it may be possible to improve the
reliability and efficacy of LLMs in the creation of fuzzing harnesses,
ultimately leading to a more seamless integration of automated testing
methodologies into the software development lifecycle.

\section{Neurosymbolic AI}\label{sec-nesy}

Neurosymbolic AI (NeSy AI) represents a groundbreaking fusion of neural
network methodologies with symbolic execution techniques and tools,
providing a multi-faceted approach to overcoming the inherent
limitations of traditional AI paradigms \autocite{sheth2023,garcez2020}.
This innovative synthesis seeks to combine the strengths of both neural
networks, which excel in pattern recognition and data-driven learning,
and symbolic systems, which offer structured reasoning and
interpretability. By integrating these two approaches, NeSy AI aims to
create cognitive models that are not only more accurate but also more
robust in problem-solving contexts.

At its core, NeSy AI facilitates the development of AI systems that are
capable of understanding and interpreting feedback in real-world
scenarios \autocite{ganguly2024}. This characteristic is particularly
significant in the current landscape of artificial intelligence, where
LLMs are predominant. In this context, NeSy AI is increasingly viewed as
a critical solution to pressing issues related to explainability,
attribution, and reliability in AI systems
\autocite{gaur2023,tilwani2024}. These challenges are essential for
ensuring that AI systems can be trusted and effectively utilized in
various applications, from business to healthcare.

The burgeoning field of neurosymbolic AI is still in its nascent stages,
with ongoing research and development actively exploring its potential
to enhance attribution methodologies within large language models. By
addressing these critical challenges, NeSy AI can significantly
contribute to the broader landscape of trustworthy AI systems, allowing
for more transparent and accountable decision-making processes
\autocite{sheth2023,gaur2023,tilwani2024}.

Moreover, the application of neurosymbolic AI within the domain of
fuzzing is gaining traction, paving the way for innovative explorations.
This integration of LLMs with symbolic systems opens up new avenues for
research. Currently, there are only a limited number of tools that
support such hybrid approaches (Chapter~\ref{sec-related}). Among these,
OverHAuL constitutes a Neuro{[}Symbolic{]} tool, as classified by Henry
Kautz's taxonomy \autocite{sarker2022,kautz2020}. This means that the
neural model---specifically the LLM---can leverage symbolic reasoning
tools---in this case a source code explorer
(Chapter~\ref{sec-implementation})---to augment its reasoning
capabilities. This symbiotic relationship enhances the overall efficacy
and versatility of LLMs for fuzzing harnesses generation, demonstrating
the profound potential held by the fusion of neural and symbolic
methodologies.

\bookmarksetup{startatroot}

\chapter{Related work}\label{sec-related}

Automated testing, automated fuzzing and automated harness creation have
a long research history. Still, a lot of ground remains to be covered
until true automation of these tasks is achieved. Until the introduction
of transformers \autocite{vaswani2023} and the 2020's boom of commercial
GPTs \autocite{chatgpt}, automation regarding testing and fuzzing was
mainly attempted through static and dynamic program analysis methods.
These approaches are still utilized, but the fuzzing community has
shifted almost entirely to researching the incorporation and employment
of LLMs in the last half decade, in the name of automation
\autocite{iris,sun2024,prophetfuzz,oss-fuzz-gen,green2022,utopia,fuzzgpt,titanfuzz,fuzzgen,fudge}.

\section{Previous Projects}\label{previous-projects}

\subsection{KLEE}\label{klee}

KLEE \autocite{klee} is a seminal and widely cited symbolic execution
engine introduced in 2008 by Cadar et al.~It was designed to
automatically generate high-coverage test cases for programs written in
C, using symbolic execution to systematically explore the control flow
of a program. KLEE operates on the LLVM \autocite{llvm} bytecode
representation of programs, allowing it to be applied to a wide range of
C programs compiled to the LLVM intermediate representation.

Instead of executing a program on concrete inputs, KLEE performs
symbolic execution---that is, it runs the program on symbolic inputs,
which represent all possible values simultaneously. At each conditional
branch, KLEE explores both paths by forking the execution and
accumulating path constraints (i.e., logical conditions on input
variables) along each path. This enables it to traverse many feasible
execution paths in the program, including corner cases that may be
difficult to reach through random testing or manual test creation.

When an execution path reaches a terminal state (e.g., a program exit,
an assertion failure, or a segmentation fault), KLEE uses a constraint
solver to compute concrete input values that satisfy the accumulated
constraints for that path. These values form a test case that will
deterministically drive the program down that specific path when
executed concretely.

\subsection{IRIS}\label{iris}

IRIS \autocite{iris} is a 2025 open-source neurosymbolic system for
static vulnerability analysis. Given a codebase and a list of
user-specified Common Weakness Enumerations (CWEs), it analyzes source
code to identify paths that may correspond to known vulnerability
classes. IRIS combines symbolic analysis---such as control- and
data-flow reasoning---with neural models trained to generalize over code
patterns. It outputs candidate vulnerable paths along with explanations
and CWE references. The system operates on full repositories and
supports extensible CWE definitions.

\subsection{FUDGE}\label{fudge}

FUDGE \autocite{fudge} is a closed-source tool, made by Google, for
automatic harness generation of C and C++ projects based on existing
client code. It was used in conjunction with and in the improvement of
Google's OSS-Fuzz \autocite{oss-fuzz}. Being deployed inside Google's
infrastructure, FUDGE continuously examines Google's internal code
repository, searching for code that uses external libraries in a
meaningful and ``fuzzable'' way (i.e.~predominantly for parsing). If
found, such code is \textbf{sliced} \autocite{sasirekha2011Slicing}, per
FUDGE, based on its Abstract Syntax Tree (AST) using LLVM's Clang tool
\autocite{llvm}. The above process results in a set of abstracted
mostly-self-contained code snippets that make use of a library's calls
and/or API. These snippets are later \textbf{synthesized} into the body
of a fuzz driver, with variables being replaced and the fuzz input being
utilized. Each is then injected in an \texttt{LLVMFuzzerTestOneInput}
function and finalized as a fuzzing harness. A building and evaluation
phase follows for each harness, where they are executed and examined.
Every passing harness along with its evaluation results is stored in
FUDGE's database, reachable to the user through a custom web-based UI.

\subsection{UTopia}\label{utopia}

UTopia \autocite{utopia} (stylized \textsc{UTopia}) is another
open-source automatic harness generation framework. Aside from the
library code, It operates solely on user-provided unit tests since,
according to \textcite{utopia}, they are a resource of complete and
correct API usage examples containing working library set-ups and
tear-downs. Additionally, each of them are already close to a fuzz
target, in the sense that they already examine a single and
self-contained API usage pattern. Each generated harness follows the
same data flow of the originating unit test. Static analysis is employed
to figure out what fuzz input placement would yield the most results. It
is also utilized in abstracting the tests away from the syntactical
differences between testing frameworks, along with slicing and AST
traversing using Clang.

\subsection{FuzzGen}\label{fuzzgen}

Another project of Google is FuzzGen \autocite{fuzzgen}, this time
open-source. Like FUDGE, it leverages existing client code of the target
library to create fuzz targets for it. FuzzGen uses whole-system
analysis, through which it creates an \emph{Abstract API Dependence
Graph} (A\textsuperscript{2}DG). It uses the latter to automatically
generate LibFuzzer-compatible harnesses. For FuzzGen to work, the user
needs to provide both client code and/or tests for the API and the API
library's source code as well. FuzzGen uses the client code to infer the
\emph{correct usage} of the API and not its general structure, in
contrast to FUDGE. FuzzGen's workflow can be divided into three phases:
\textbf{1. API usage inference}. By consuming and analyzing client code
and tests that concern the library under test, FuzzGen recognizes which
functions belong to the library and learns its correct API usage
patterns. This process is done with the help of Clang. To test if a
function is actually a part of the library, a sample program is created
that uses it. If the program compiles successfully, then the function is
indeed a valid API call. \textbf{2. A\textsuperscript{2}DG construction
mechanism}. For all the existing API calls, FuzzGen builds an
A\textsuperscript{2}DG to record the API usages and infers its intended
structure. After completion, this directed graph contains all the valid
API call sequences found in the client code corpus. It is built in a
two-step process: First, many smaller A\textsuperscript{2}DGs are
created, one for each root function per client code snippet. Once such
graphs have been created for all the available client code instances,
they are combined to formulate the master A\textsuperscript{2}DG. This
graph can be seen as a template for correct usage of the library.
\textbf{3. Fuzzer generator}. Through the A\textsuperscript{2}DG, a
fuzzing harness is created. Contrary to FUDGE, FuzzGen does not create
multiple ``simple'' harnesses but a single complex one with the goal of
covering the whole of the A\textsuperscript{2}DG. In other words, while
FUDGE fuzzes a single API call at a time, FuzzGen's result is a single
harness that tries to fuzz the given library all at once through complex
API usage.

\subsection{IntelliGen}\label{intelligen}

\textbf{SAMPLE}

Zhang et al.~present IntelliGen \autocite{zhang2021}, a system for
automatically synthesizing fuzz drivers by statically identifying
potentially vulnerable entry-point functions within C projects.
Implemented using LLVM, IntelliGen focuses on improving fuzzing
efficiency by targeting code more likely to contain memory safety
issues, rather than exhaustively fuzzing all available functions.

The system comprises two main components: the \textbf{Entry Function
Locator} and the \textbf{Fuzz Driver Synthesizer}. The Entry Function
Locator analyzes the project's abstract syntax tree (AST) and classifies
functions based on heuristics that indicate vulnerability. These include
pointer dereferencing, calls to memory-related functions (e.g.,
\texttt{memcpy}, \texttt{memset}), and invocation of other internal
functions. Functions that score highly on these metrics are prioritized
for fuzz driver generation. The guiding insight is that entry points
with fewer argument checks and more direct memory operations expose more
useful program logic for fuzz testing.

The Fuzz Driver Synthesizer then generates harnesses for these entry
points. For each target function, it synthesizes a
\texttt{LLVMFuzzerTestOneInput} function that invokes the target with
arguments derived from the fuzzer input. This process involves inferring
argument types from the source code and ensuring that runtime behavior
does not violate memory safety---thus avoiding invalid inputs that would
cause crashes unrelated to genuine bugs.

IntelliGen stands out by integrating static vulnerability estimation
into the driver generation pipeline. Compared to prior tools like
FuzzGen and FUDGE, it uses a more targeted, heuristic-based selection of
functions, increasing the likelihood that fuzzing will exercise
meaningful and vulnerable code paths.

\subsection{CKGFuzzer}\label{ckgfuzzer}

\textbf{SAMPLE}

CKGFuzzer \autocite{xu2024} is a fuzzing framework designed to automate
the generation of effective fuzz drivers for C/C++ libraries by
leveraging static analysis and large language models. Its workflow
begins by parsing the target project along with any associated library
APIs to construct a code knowledge graph. This involves two primary
steps: first, parsing the abstract syntax tree (AST), and second,
performing interprocedural program analysis. Through this process,
CKGFuzzer extracts essential program elements such as data structures,
function signatures, function implementations, and call relationships.

Using the knowledge graph, CKGFuzzer then identifies and queries
meaningful API combinations, focusing on those that are either
frequently invoked together or exhibit functional similarity. It
generates candidate fuzz drivers for these combinations and attempts to
compile them. Any compilation errors encountered during this phase are
automatically repaired using heuristics and domain knowledge. A
dynamically updated knowledge base, constructed from prior library usage
patterns, guides both the generation and repair processes.

Once the drivers are successfully compiled, CKGFuzzer executes them
while monitoring code coverage at the file level. It uses coverage
feedback to iteratively mutate underperforming API combinations,
refining them until new execution paths are discovered or a preset
mutation budget is exhausted.

Finally, any crashes triggered during fuzzing are subjected to a
reasoning process based on chain-of-thought prompting. To help determine
their severity and root cause, CKGFuzzer consults an LLM-generated
knowledge base containing real-world examples of vulnerabilities mapped
to known Common Weakness Enumeration (CWE) entries.

\subsection{PromptFuzz}\label{promptfuzz}

\textbf{SAMPLE}

Lyu et al.~(2024) introduce PromptFuzz \autocite{lyu2024}, a system for
automatically generating fuzz drivers using LLMs, with a novel focus on
\textbf{prompt mutation} to improve coverage. The system is implemented
in Rust and targets C libraries, aiming to explore more of the API
surface with each iteration.

The workflow begins with the random selection of API functions,
extracted from header file declarations. These functions are used to
construct initial prompts that instruct the LLM to generate a simple
program utilizing the API. Each generated program is compiled, executed,
and monitored for code coverage. Programs that fail to compile or
violate runtime checks (e.g., sanitizers) are discarded.

A key innovation in PromptFuzz is \textbf{coverage-guided prompt
mutation}. Instead of mutating generated code directly, PromptFuzz
mutates the LLM prompts---selecting new combinations of API functions to
target unexplored code paths. This process is guided by a \textbf{power
scheduling} strategy that prioritizes underused or promising API
functions based on feedback from previous runs.

Once an effective program is produced, it is transformed into a fuzz
driver by replacing constants and arguments with variables derived from
the fuzzer input. Multiple such drivers are embedded into a single
harness, where the input determines which program variant to execute,
typically via a case-switch construct.

Overall, PromptFuzz demonstrates that prompt-level mutation enables more
effective exploration of complex APIs and achieves better coverage than
direct code mutations, offering a compelling direction for LLM-based
automated fuzzing systems.

\subsection{OSS-Fuzz}\label{oss-fuzz}

OSS-Fuzz \autocite{ossfuzzdocs2025,oss-fuzz} is a continuous, scalable
and distributed cloud fuzzing solution for critical and prominent
open-source projects. Developers of such software can submit their
projects to OSS-Fuzz's platform, where its harnesses are built and
constantly executed. This results in multiple bug findings that are
later disclosed to the primary developers and are later patched.

OSS-Fuzz started operating in 2016, an initiative in response to the
Heartbleed vulnerability
\autocite{heartbleed-cve,wheeler2014,heartbleed}. Its hope is that
through more extensive fuzzing such errors could be caught and corrected
before having the chance to be exploited and thus disrupt the public
digital infrastructure. So far, it has helped uncover over 10,000
security vulnerabilities and 36,000 bugs across more than 1,000
projects, significantly enhancing the quality and security of major
software like Chrome, OpenSSL, and systemd.

A project that's part of OSS-Fuzz must have been configured as a
ClusterFuzz \autocite{clusterfuzz} project. ClusterFuzz is the fuzzing
infrastructure that OSS-Fuzz uses under the hood and depends on Google
Cloud Platform services, although it can be hosted locally. Such an
integration requires setting up a build pipeline, fuzzing jobs and
expects a Google Developer account. Results are accessible through a web
interface. ClusterFuzz, and by extension OSS-Fuzz, supports fuzzing
through LibFuzzer, AFL++, Honggfuzz and FuzzTest---successor to
Centipede--- with the last two being Google projects
\autocite{libfuzzer,fuzztest,honggfuzz,aflpp}. C, C++, Rust, Go, Python
and Java/JVM projects are supported.

\subsection{OSS-Fuzz-Gen}\label{oss-fuzz-gen}

OSS-Fuzz-Gen (OFG) \autocite{liu2023,oss-fuzz-gen} is Google's current
State-Of-The-Art (SOTA) project regarding automatic harness generation
through LLMs. It's purpose is to improve the fuzzing infrastructure of
open-source projects that are already integrated into OSS-Fuzz. Given
such a project, OSS-Fuzz-Gen uses its preexisting fuzzing harnesses and
modifies them to produce new ones. Its architecture can be described as
follows: 1. With an OSS-Fuzz project's GitHub repository link,
OSS-Fuzz-Gen iterates through a set of predefined build templates and
generates potential build scripts for the project's harnesses. 2. If any
of them succeed they are once again compiled, this time through
fuzz-introspector \autocite{fuzz-introspector}. The latter constitutes a
static analysis tool, with fuzzer developers specifically in mind. 3.
Build results, old harness and fuzz-introspector report are included in
a template-generated prompt, through which an LLM is called to generate
a new harness. 4. The newly generated fuzz target is compiled and if it
is done so successfully it begins execution inside OSS-Fuzz's
infrastructure.

This method proved meaningful, with code coverage in fuzz campaigns
increasing thanks to the new generated fuzz drivers. In the case of
\autocite{thomason2025}, line coverage went from 38\% to 69\% without
any manual interventions \autocite{liu2023}.

In 2024, OSS-Fuzz-Gen introduced an experimental feature for generating
harnesses in previously unfuzzed projects
\autocite{oss-fuzzmaintainers2024}. The code for this feature resides in
the \texttt{experimental/from\_scratch} directory of the project's
GitHub repository \autocite{oss-fuzz-gen}, with the latest known working
commit being \texttt{171aac2} and the latest overall commit being four
months ago.

\subsection{AutoGen}\label{autogen}

AutoGen \autocite{sun2024} is a closed-source tool that generates new
fuzzing harnesses, given only the library code and documentation. It
works as following: The user specifies the function for which a harness
is to be generated. AutoGen gathers information for this function---such
as the function body, used header files, function calling
examples---from the source code and documentation. Through specific
prompt templates containing the above information, an LLM is tasked with
generating a new fuzz driver, while another is tasked with generating a
compilation command for said driver. If the compilation fails, both LLMs
are called again to fix the problem, whether it was on the driver's or
command's side. This loop iterates until a predefined maximum value or
until a fuzz driver is successfully generated and compiled. If the
latter is the case, it is then executed. If execution errors exist, the
LLM responsible for the driver generation is used to correct them. If
not, the pipeline has terminated and a new fuzz driver has been
successfully generated.

\section{Differences}\label{sec-differences}

OverHAuL differs, in some way, with each of the aforementioned works.
Firstly, although KLEE and IRIS \autocite{iris,klee} tackle the problem
of automated testing and both IRIS and OverHAuL can be considered
neurosymbolic AI tools, the similarities end there. None of them utilize
LLMs the same way we do---with KLEE not utilizing them by default, as it
precedes them chronologically---and neither are automating any part of
the fuzzing process.

When it comes to FUDGE, FuzzGen and UTopia
\autocite{utopia,fuzzgen,fudge}, all three depend on and demand existing
client code and/or unit tests. On the other hand, OverHAuL requires only
the bare minimum: the library code itself. Another point of difference
is that in contrast with OverHAuL, these tools operate in a linear
fashion. No feedback is produced or used in any step and any point
failure results in the termination of the entire run.

OverHAuL challenges a common principle of these tools, stated explicitly
in FUDGE's paper \autocite{fudge}: ``Choosing a suitable fuzz target
(still) requires a human''. OverHAuL chooses to let the LLM, instead of
the user, explore the available functions and choose one to target in
its fuzz driver.

OSS-Fuzz-Gen \autocite{oss-fuzz-gen} can be considered a close
counterpart of OverHAuL, and in some ways it is. A lot of inspiration
was gathered from it, like for example the inclusion of static analysis
and its usage in informing the LLM. Yet, OSS-Fuzz-Gen has a number of
disadvantages that make it in some cases an inferior option. For one,
OFG is tightly coupled with the OSS-Fuzz platform \autocite{oss-fuzz},
which even on its own creates a plethora of issues for the common
developer. To integrate their project into OSS-Fuzz, they would need to:
Transform it into a ClusterFuzz project \autocite{clusterfuzz} and take
time to write harnesses for it. Even if these prerequisites are carried
out, it probably would not be enough. Per OSS-Fuzz's documentation
\autocite{ossfuzzdocs2025}: ``To be accepted to OSS-Fuzz, an open-source
project must have a significant user base and/or be critical to the
global IT infrastructure''. This means that OSS-Fuzz is a viable option
only for a small minority of open-source developers and maintainers. One
countermeasure of the above shortcoming would be for a developer to run
OSS-Fuzz-Gen locally. This unfortunately proves to be an arduous task.
As it is not meant to be used standalone, OFG is not packaged in the
form of a self-contained application. This makes it hard to setup and
difficult to use interactively. Like in the case of FUDGE, OFG's actions
are performed linearly. No feedback is utilized nor is there graceful
error handling in the case of a step's failure. Even in the case of the
experimental feature for bootstrapping unfuzzed projects, OFG's
performance varies heavily. During experimentation, a lot of generated
harnesses were still wrapped either in Markdown backticks or
\texttt{\textless{}code\textgreater{}} tags, or were accompanied with
explanations inside the generated \texttt{.c} source file. Even if code
was formatted correctly, in many cases it missed necessary headers for
compilation or used undeclared functions.

Lastly, the closest counterpart to OverHAuL is AutoGen
\autocite{sun2024}. Their similarity stands in the implementation of a
feedback loop between LLM and generated harness. However, most other
implementation decisions remain distinct. One difference regards the
fuzzed function. While AutoGen requires a target function to be
specified by the user in which it narrows during its whole run, OverHAuL
delegates this to the LLM, letting it explore the codebase and decide by
itself the best candidate. Another difference lies in the need---and the
lack of---of documentation. While AutoGen requires it to gather
information for the given function, OverHAuL leans into the role of a
developer by reading the related code and comments and thus avoiding any
mismatches between documentation and code. Finally, the LLMs' input is
built based on predefined prompt templates, a technique also present in
OSS-Fuzz-Gen.~OverHAuL operates one abstraction level higher, leveraging
DSPy \autocite{dspy} for programming instead of prompting the LLMs used.

In conclusion, OverHAuL constitutes an \emph{open-source} tool that
offers new functionality by offering a straightforward installation
process, packaged as a self-contained Python package with minimal
external dependencies. It also introduces novel approaches compared to
previous work by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implementing a feedback mechanism between harness generation,
  compilation, and evaluation phases,
\item
  Using autonomous ReAct agents capable of codebase exploration,
\item
  Leveraging a vector store for code consumption and retrieval.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{OverHAuL}\label{overhaul}

In this thesis we present \emph{OverHAuL} (\textbf{H}arness
\textbf{Au}tomation with \textbf{L}LMs), a neurosymbolic AI tool that
automatically generates fuzzing harnesses for C libraries through LLM
agents. In its core, OverHAuL is comprised by three LLM ReAct agents
\autocite{reAct}---each with its own responsibility and scope---and a
vector store index reserving the given project's analyzed codebase. An
overview of OverHAuL's process is presented in
Figure~\ref{fig-flowchart}. The objective of OverHAuL is to streamline
the process of fuzz testing for C libraries. Given a link to a git
repository \autocite{torvalds2005} of a C library, OverHAuL
automatically generates a new fuzzing harness specifically designed for
the project. In addition to the harness, it produces a compilation
script to facilitate building the harness, generates a representative
input that can trigger crashes, and logs the output from the executed
harness.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapters/../resources/flowchart.png}}

}

\caption{\label{fig-flowchart}Overview of OverHAuL's automatic
harnessing process.}

\end{figure}%

As commented in Section~\ref{sec-differences}, OverHAuL does not expect
and depend on the existence of client code or unit tests
\autocite{utopia,fudge,fuzzgen} \emph{nor} does it require any
preexisting fuzzing harnesses \autocite{oss-fuzz-gen} or any
documentation present \autocite{sun2024}. Also importantly, OverHAuL is
decoupled from other fuzzing projects, thus lowering the barrier to
entry for new projects \autocite{oss-fuzz-gen,oss-fuzz}. Lastly, the
user isn't mandated to specify manually the function which the
harness-to-be-generated must fuzz. Instead, OverHAuL's agents examine
and assess the provided codebase, choosing after evaluation the most
optimal targeted function.

OverHAuL utilizes autonomous ReAct agents \autocite{reAct} which inspect
and analyze the project's source code. The latter is stored and
interacted with as a set of text embeddings \autocite{mikolov2013}, kept
in a vector store. Both approaches are, to the best of our knowledge,
novel in the field of automatic fuzzing harnesses generation. OverHAuL
also implements an evaluation component that assesses in real-time all
generated harnesses, making the results tenable, reproducible and
well-founded. Ideally, this methodology provides a comprehensive and
systematic framework for identifying previously unknown software
vulnerabilities in projects that have not yet been fuzz tested.

Finally, OverHAuL excels in its user-friendliness, as it constitutes a
simple and easily-installable Python package with minimal external
dependencies---only real dependency being Clang, a prevalent compiler
available across all primary operating systems. This contrasts most
other comparable systems, which are typically characterized by their
limited documentation, lack of extensive testing, and a focus primarily
on experimental functionality.\footnote{I.e. ``research code''.}

\section{Architecture}\label{sec-architecture}

OverHAuL can be compartmentalized in three stages: First, the project
analysis stage (Section~\ref{sec-analysis}), the harness creation stage
(Section~\ref{sec-creation}) and the harness evaluation stage
(Section~\ref{sec-evaluation}).

\subsection{Project Analysis}\label{sec-analysis}

In the project analysis stage (steps A.1--A.4), the project to be fuzzed
is ran through a static analysis tool and is sliced into function-level
chunks, which are stored in a vector store. The results of this stage
are a static analysis report and a vector store containing embeddings of
function-level code chunks, both of which are later available to the LLM
agents.

The static analysis tool Flawfinder \autocite{flawfinder} is executed
with the project directory as input and is responsible for the static
analysis report. This report is considered a meaningful resource, since
it provides the LLM agent with some starting points to explore,
regarding the occurrences of potentially vulnerable functions and/or
unsafe code practices.

The vector store is created in the following manner: The codebase is
first chunked in function-level pieces by traversing the code's Abstract
Syntax Tree (AST) through Clang. Each chunk is represented by an object
with the function's signature, the corresponding filepath and the
function's body. Afterwards, each function body is turned into a vector
embedding through an embedding model. Each embedding is stored in the
vector store. This structure is created and used for easier and more
semantically meaningful code retrieval, and to also combat context
window limitations present in the LLMs.

\subsection{Harness Creation}\label{sec-creation}

Second is the harness creation stage (steps B.1--B.2). In this part, a
``generator'' ReAct LLM agent is tasked with creating a fuzzing harness
for the project. The agent has access to a querying tool that acts as an
interface between it and the vector store. When the agent makes queries
like ``functions containing \texttt{strcpy()}'', the querying tool turns
the question into an embedding and through similarity search returns the
top \(k=3\) most similar results---in this case, functions of the
project. With this approach, the agent is able to explore the codebase
semantically and pinpoint potentially vulnerable usage patterns easily.

The harness generated by the agent is then compiled using Clang and
linked with the AddressSanitizer, LeakSanitizer, and
UndefinedBehaviorSanitizer. The compilation command used is generated
programmatically, according to the rules described in
Section~\ref{sec-assumptions}. If the compilation fails for any reason,
e.g.~a missing header include, then the generated faulty harness and its
compilation output are passed to a new ``fixer'' agent tasked with
repairing any errors in the harness (step B.2.a). This results in a
newly generated harness, presumably free from the previously shown
flaws. This process is iterated until a compilable harness has been
obtained. After success, a script is also exported in the project
directory, containing the generated compilation command.

\subsection{Harness Evaluation}\label{sec-evaluation}

Third comes the evaluation stage (steps C.1--C.3). During this step, the
compiled harness is executed and its results evaluated. Namely, a
generated harness passes the evaluation phase if and only if:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The harness has no memory leaks during its execution This is inferred
  by the existence of \texttt{leak-\textless{}hash\textgreater{}} files.
\item
  A new testcase was created \emph{or} the harness executed for at least
  \texttt{MIN\_EXECUTION\_TIME} (i.e.~did not crash on its own) When a
  crash happens, and thus a testcase is created, it results in a
  \texttt{crash-\textless{}hash\textgreater{}} file.
\item
  The created testcase is not empty This is examined through xxd's
  output given the crash-file.
\end{enumerate}

Similarly to the second stage's compilation phase (steps B.2--B.2.a), if
a harness does not pass the evaluation for whatever reason it is sent to
an ``improver'' agent. This agent is instructed to refine it based on
its code and cause of failing the evaluation. This process is also
iterative. If any of the improved harness versions fail to compile, the
aforementioned ``fixer'' agent is utilized again (steps C.2--C.2.a). All
produced crash files and the harness execution output are saved in the
project's directory.

\section{Main techniques}\label{sec-techniques}

The fundamental techniques that distinguish OverHAuL in its approach and
enhance its effectiveness in achieving its objectives are: The
implementation of an iterative feedback loop between the LLM agents, the
distribution of responsibility across a swarm of distinct agents and the
employment of a ``codebase oracle'' for interacting with the given
project's source code.

\subsection{Feedback loop}\label{feedback-loop}

The initial generated harness produced by OverHAuL is unlikely to be
successful from the get-go. The iterative feedback loop implemented
facilitates its enhancement, enabling the harness to be tested under
real-world conditions and subsequently refined based on the results of
these tests. This approach mirrors the typical workflow employed by
developers in the process of creating and optimizing fuzz targets.

In this iterative framework, the development process continues until
either an acceptable and functional harness is realized or the defined
\emph{iteration budget} is exhausted. The iteration budget \(N=10\) is
initialized at the onset of OverHAuL's execution and is shared between
both the compilation and evaluation phases of the harness development
process. This means that the iteration budget is decremented each time a
dashed arrow in the flowchart illustrated in Figure~\ref{fig-flowchart}
is followed. Such an approach allows for targeted improvements while
maintaining oversight of resource allocation throughout the harness
development cycle.

\subsection{ReAct agents swarm}\label{react-agents-swarm}

An integral design decision in our framework is the implementation of
each agent as a distinct LLM instance, although all utilizing the same
underlying model. This approach yields several advantages, particularly
in the context of maintaining separate and independent contexts for each
agent throughout each OverHAuL run.

By assigning individual contexts to the agents, we enable a broader
exploration of possibilities during each run. For instance, the
``improver'' agent can investigate alternative pathways or strategies
that the ``generator'' agent may have potentially overlooked or
internally deemed inadequate inaccurately. This separation not only
fosters a more diverse range of solutions but also enhances the overall
robustness of the system by allowing for iterative refinement based on
each agent's unique insights.

Furthermore, this design choice effectively addresses the limitations
imposed by context window sizes. By distributing the ``cognitive'' load
across multiple agents, we can manage and mitigate the risks associated
with exceeding these constraints. As a result, this architecture
promotes efficient utilization of available resources while maximizing
the potential for innovative outcomes in multi-agent interactions. This
layered approach ultimately contributes to a more dynamic and
exploratory research environment, facilitating a comprehensive
examination of the problem space.

\subsection{Codebase oracle}\label{codebase-oracle}

The third central technique employed is the creation and utilization of
a codebase oracle, which is effectively realized through a vector store.
This oracle is designed to contain the various functions within the
project, enabling it to return the most semantically similar functions
upon querying it. Such an approach serves to address the inherent
challenges associated with code exploration difficulties faced by LLM
agents, particularly in relation to their limited context window.

By structuring the codebase into chunks at the level of individual
functions, LLM agents can engage with the code more effectively by
focusing on its functional components. This methodology not only allows
for a more nuanced understanding of the codebase but also ensures that
the responses generated do not consume an excessive portion of the
limited context window available to the agents. In contrast, if the
codebase were organized and queried at the file level, the chunks of
information would inevitably become larger, leading to an increase in
noise and a dilution of meaningful content in each chunk
\autocite{zhao2024}. Given the constant size of the embeddings used in
processing, each progressively larger chunk would be less semantically
significant, ultimately compromising the quality of the retrieval
process.

Defining the function as the primary unit of analysis represents the
most proportionate balance between the size of the code segments and
their semantic significance. It serves as the ideal ``zoom-in'' level
for the exploration of code, allowing for greater clarity and precision
in understanding the functionality of individual code segments. This
same principle is widely recognized in the training of code-specific
LLMs, where a function-level approach has been shown to enhance
performance and comprehension \autocite{chen2021}. By adopting this
methodology, we aim to foster a more robust interaction between LLM
agents and the underlying codebase, ultimately facilitating a more
effective and efficient exploration process.

\section{High-Level Algorithm}\label{high-level-algorithm}

A pseudocode version of OverHAuL's main function can be seen in
Algorithm~\ref{alg-main}. It represents the workflow presented in
Figure~\ref{fig-flowchart} and uses the techniques described in sections
\ref{sec-architecture} and \ref{sec-techniques}. It is important to
emphasize that, within the context of this algorithm, the
\texttt{Harnesser()} function serves as an interface that bridges the
``generator'', ``fixer'' and ``improver'' LLM agents. The agent that is
used upon each function call depends on the values of the function's
arguments. This results in the \(harness\) variable representing all
generated, fixed or improved harnesses. This approach is adopted for
making the abstract algorithm simpler and easier to understand.

\begin{algorithm}[H]
\caption{OverHAuL}
\label{alg-main}
\begin{algorithmic}[1]
\Require $repository$
\Ensure $harness, compilation\_script, crash\_input, execution\_log$
  \State $path \gets$ \Call{RepoClone}{repository}
  \State $report \gets$ \Call{StaticAnalysis}{$path$}
  \State $vector\_store \gets$ \Call{CreateOracle}{$path$}
  \State $acceptable \gets$ False
  \State $compiled \gets$ False
  \State $error \gets$ None
  \State $violation \gets$ None
  \State $output \gets$ None
  \For{$i = 1$ to $MAX\_ITERATIONS$}
    \State $harness \gets$ \Call{Harnesser}{$path, report, vector\_store, error, violation, output$}
    \State $error, compiled \gets$ \Call{BuildHarness}{$harness$}
    \If{$\neg compiled$}
      \State \textbf{continue} \Comment{Regenerate harness}
    \EndIf
    \State $output, accepted \gets $\Call{EvaluateHarness}{$harness$}
    \If{$\neg accepted$}
      \State \textbf{continue} \Comment{Improve harness}
    \Else
      \State $acceptable \gets$ True
      \State \textbf{break}
    \EndIf
  \EndFor
  \State \Return $compiled \land acceptable$
\end{algorithmic}
\end{algorithm}

\section{Examples}\label{examples}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapters/../resources/successful-execution.png}}

}

\caption{\label{fig-success}A successful execution of OverHAuL,
harnessing the dateparse library.}

\end{figure}%

\section{Scope}\label{scope}

\begin{itemize}
\tightlist
\item
  Limited to C libraries
\item
  Expects relatively simple project structure

  \begin{itemize}
  \tightlist
  \item
    Code either in root or in common-named subdirs (e.g.~src/)
  \item
    Any file or directory with main, test or example substring is
    ignored
  \item
    No main() function, or only exists in some file that is ignored by
    the above
  \end{itemize}
\item
  Build systems not supported

  \begin{itemize}
  \tightlist
  \item
    Harness is compiled with a predefined command
  \end{itemize}
\end{itemize}

\subsection{Assumptions/Prerequisites}\label{sec-assumptions}

\begin{itemize}
\tightlist
\item
  Project structure
\item
  file/folder naming
\item
  building process
\end{itemize}

\section{Abandoned techniques}\label{abandoned-techniques}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Zero-shot harness generation
\item
  ChainOfThought modules for LLM instances \autocite{chainofthought}
\item
  Naive source code concatenation
\item
  manual \{index, read\}\_tool usage for ReAct agents
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Evaluation}\label{evaluation}

\section{Research questions}\label{research-questions}

\section{Benchmarks}\label{benchmarks}

10 open-source C/C++ projects.

\section{Performance}\label{performance}

\section{Issues}\label{issues}

\section{Future Work}\label{future-work}

\subsection{Technical Future Work}\label{technical-future-work}

\subsection{Architectural Future
Work/Extensions}\label{architectural-future-workextensions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build system
\item
  More (static) analysis tolls integrations
\item
  General \emph{localization} problem
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Results}\label{results}

Results from integration with 10/100 open-source C/C++ projects.

\bookmarksetup{startatroot}

\chapter{Implementation}\label{sec-implementation}

--depth 1 output/

embedder model openai Source code is processed and chunked using Clang
{[}33{]}. The chunks are function-level units, found to be a sweet-spot
between semantic significance and size \autocite{zhao2024,chen2021}.
This results in a list of Python dicts, each containing a function's
body, signature and filepath. Each chunk's function code is then turned
into an embedding using OpenAI's ``text-embedding-3-small'' model. faiss
store and index A FAISS {[}36{]} vector store is created. Each function
embedding is stored in it (with the same order, as to correspond with
the previous list containing the metadata).

same order code chunks

Prompting techniques used (callback to Section~\ref{sec-prompting}).
Sample prompt

\autocite{dspy}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Why instead of langchain or llamaindex?
  \autocite{langchain,llamaindex}
\end{enumerate}

libclang Python package

\section{Development environment}\label{development-environment}

uv, ruff, mypy, pytest, pdoc

\section{Equipment}\label{equipment}

desktop pc cpu, memory

\section{models used}\label{models-used}

gpt-4.1

\section{Reproducibility}\label{reproducibility}

github workflow actions, artifacts, summary

\bookmarksetup{startatroot}

\chapter{Future Work}\label{future-work-1}

The prototype implementation of OverHAuL offers a compelling
demonstration of its potential to automate the fuzzing process for
open-source libraries, providing tangible benefits to developers and
maintainers alike. This initial version successfully validates the core
design principles underpinning OverHAuL, showcasing its ability to
streamline and enhance the software testing workflow through automated
generation of fuzz drivers using large language models. Nevertheless,
while these foundational capabilities lay a solid groundwork, numerous
avenues exist for further expansion, refinement, and rigorous evaluation
to fully realize the tool's potential and adapt to evolving challenges
in software quality assurance.

\section{Enhancements to Core
Features}\label{enhancements-to-core-features}

Enhancing OverHAuL's core functionality represents a primary direction
for future development. First, expanding support to encompass a wider
array of build systems commonly employed in C and C++ projects---such as
GNU Make, CMake, Meson, and Ninja
\autocite{cedilnik2000,feldman1979,martin2025,pakkanen2025}---would
significantly broaden the scope of libraries amenable to automated
fuzzing using OverHAuL. This advancement would enable OverHAuL to scale
effectively and be applied to larger, more complex codebases, thereby
increasing its practical utility and impact.

Second, integrating additional fuzzing engines beyond LibFuzzer stands
out as a strategic enhancement. Incorporation of widely adopted fuzzers
like AFL++ \autocite{aflpp} could diversify the fuzzing strategies
available to OverHAuL, while exploring more experimental tools such as
GraphFuzz \autocite{green2022} may pioneer specialized approaches for
certain code patterns or architectures. Multi-engine support would also
facilitate extending language coverage, for instance by incorporating
fuzzers tailored to other programming ecosystems---for example, Google's
Atheris for Python projects \autocite{atheris}. Such versatility would
position OverHAuL as a more universal fuzzing automation platform.

Third, the evaluation component of OverHAuL presents an opportunity for
refinement through more sophisticated analysis techniques. Beyond the
current criteria, incorporating dynamic metrics such as differential
code coverage tracking between generated fuzz harnesses would yield
deeper insights into test quality and coverage completeness. This
quantitative evaluation could guide iterative improvements in fuzz
driver generation and overall testing effectiveness.

Finally, OverHAuL's methodology could be extended to leverage existing
client codebases and unit tests in addition to the library source code
itself, resources that for now OverHAuL leaves untapped. Inspired by
approaches like those found in FUDGE and FuzzGen
\autocite{fuzzgen,fudge}, this enhancement would enable the tool to
exploit programmer-written usage scenarios as seeds or contexts,
potentially generating more meaningful and targeted fuzz inputs.
Incorporating these richer information sources would likely improve the
efficacy of fuzzing campaigns and uncover subtler bugs.

\section{Experimentation with Large Language Models and Data
Representation}\label{experimentation-with-large-language-models-and-data-representation}

OverHAuL's reliance on large language models (LLMs) invites
comprehensive experimentation with different providers and architectures
to assess their comparative strengths and limitations. Conducting
empirical evaluations across leading models---such as OpenAI's o1 and o3
families and Anthropic's Claude Opus 4---will provide valuable insights
into their capabilities, cost-efficiency, and suitability for fuzz
driver synthesis. Additionally, specialized code-focused LLMs, including
generative and fill-in models like Codex-1 and CodeGen
\autocite{nijkamp2023a,nijkamp2023,openai2025a}, merit exploration due
to their targeted optimization for source code generation and
understanding.

Another dimension worthy of investigation concerns the granularity of
code chunking employed during the given project's code processing stage.
Whereas the current approach partitions code at the function level,
experimenting with more nuanced segmentation strategies---such as
splitting per step inside a function, as a finer-grained
technique---could improve the semantic coherence of stored
representations and enhance retrieval relevance during fuzz driver
generation. This line of inquiry has the potential to optimize model
input preparation and ultimately improve output quality.

\section{Comprehensive Evaluation and
Benchmarking}\label{comprehensive-evaluation-and-benchmarking}

To thoroughly establish OverHAuL's effectiveness, extensive large-scale
evaluation beyond the initial 10-project corpus is imperative. Applying
the tool to repositories indexed in the clib package manager
\autocite{clibs}, which encompasses hundreds of C libraries, would test
scalability and robustness across diverse real-world settings. Such a
broad benchmark would also enable systematic comparisons against
state-of-the-art automated fuzzing frameworks like OSS-Fuzz-Gen and
AutoGen, elucidating OverHAuL's relative strengths and identifying areas
for improvement \autocite{oss-fuzz-gen,sun2024}.

Complementing broad benchmarking, detailed ablation studies dissecting
the contributions of individual pipeline components and algorithmic
choices will yield critical insights into what drives OverHAuL's
performance. Understanding the impact of each module will guide targeted
optimizations and support evidence-based design decisions.

Furthermore, an economic analysis exploring resource consumption---such
as API token usage and associated monetary costs---relative to fuzzing
effectiveness would be valuable for assessing the practical viability of
integrating LLM-based fuzz driver generation into continuous integration
processes.

\section{Practical Deployment and Community
Engagement}\label{practical-deployment-and-community-engagement}

From a usability perspective, embedding OverHAuL within a GitHub Actions
workflow represents a practical and impactful enhancement, enabling
seamless integration with developers' existing toolchains and continuous
integration pipelines. This would promote wider adoption by reducing
barriers to entry and fostering real-time feedback during code
development cycles.

Additionally, establishing a mechanism to generate and submit automated
pull requests (PRs) to the maintainers of fuzzed
libraries---highlighting detected bugs and proposing patches---would not
only validate OverHAuL's findings but also contribute tangible
improvements to open-source software quality. This collaborative
feedback loop epitomizes the symbiosis between automated testing tools
and the open-source community. As an initial step, developing targeted
PRs for the projects where bugs were discovered during OverHAuL's
development would help facilitate practical follow-up and improvements.

\bookmarksetup{startatroot}

\chapter{Discussion}\label{discussion}

more powerful llms -\textgreater{} better results

open source libraries might have been in the training data results for
closed source libraries could be worse this could be mitigated with llm
fine-tuning

\bookmarksetup{startatroot}

\chapter{Conclusion}\label{conclusion}

Recap

Presented the algorithm \emph{and} the implementation.

generative AI disclaimer à la ACM?

\bookmarksetup{startatroot}

\chapter*{Bibliography}\label{bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

\markboth{Bibliography}{Bibliography}

\printbibliography[heading=none]





\end{document}
