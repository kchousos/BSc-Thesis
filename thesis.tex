% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \ifLuaTeX
    \usepackage{luaotfload}
    \directlua{luaotfload.add_fallback("mainfontfallback",{
      "Liberation Serif:","NotoColorEmoji:mode=harf"
    })}
  \fi
  \setmainfont[,RawFeature={fallback=mainfontfallback}]{Libertinus
Serif}
  \setsansfont[]{Libertinus Sans}
  \setmonofont[]{Iosevka}
  \setmathfont[]{Libertinus Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{254,254,254}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.85,0.12,0.09}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.47,0.16,0.63}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.65,0.35,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.00,0.50,0.00}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.41,0.41,0.41}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\ifLuaTeX
  \usepackage{luacolor}
  \usepackage[soul]{lua-ul}
\else
  \usepackage{soul}
\fi




\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[style=ieee,]{biblatex}
\addbibresource{resources/bibliography.bib}


\pagenumbering{roman}
\usepackage{lineno}
\linenumbers
\renewcommand*{\bibfont}{\small}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={OverHAuL},
  pdfauthor={Konstantinos Chousos},
  pdfkeywords={LLMs, Fuzzing, Automation, Security, Neurosymbolic AI},
  colorlinks=true,
  linkcolor={blue},
  filecolor={cyan},
  citecolor={red},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}


\title{OverHAuL}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Harnessing Automation for C Libraries via LLMs}
\author{Konstantinos Chousos}
\date{July, 2025}
\begin{document}
\maketitle
\begin{abstract}
Lorem ipsum odor amet, consectetuer adipiscing elit. Habitasse congue
tempus erat rhoncus sapien interdum dolor nec. Posuere habitant metus
tellus erat eu. Risus ultricies eu rhoncus, conubia euismod convallis
commodo per. Nam tellus quisque maximus duis eleifend; arcu aptent. Nisi
rutrum primis luctus tortor tempor maecenas. Donec curae cras dolor;
malesuada ultricies scelerisque. Molestie class tincidunt quis gravida
ut proin. Consequat lacinia arcu justo leo maecenas nunc neque ex.
Platea eros ullamcorper nullam rutrum facilisis.
\end{abstract}


\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}

\markboth{Preface}{Preface}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat
leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque
habitant morbi tristique senectus et netus et malesuada fames ac turpis
egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere
iaculis. Suspendisse et maximus elit. In fringilla gravida ornare.
Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel
neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet
vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor
lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque
ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse
potenti.

Etiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim
ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies.
Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit
amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit,
eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id,
tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis
efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus
ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum
ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia
curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam
id magna ultrices, sodales metus viverra, tempus turpis.

\newpage{}

\bookmarksetup{startatroot}

\chapter*{Acknowledgments}\label{acknowledgments}

\markboth{Acknowledgments}{Acknowledgments}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

\newpage{}

\tableofcontents

\newpage{}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

\pagenumbering{arabic}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat
leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque
habitant morbi tristique senectus et netus et malesuada fames ac turpis
egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere
iaculis. Suspendisse et maximus elit. In fringilla gravida ornare.
Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel
neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet
vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor
lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque
ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse
potenti.

\section{Motivation}\label{motivation}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

Nunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat
leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque
habitant morbi tristique senectus et netus et malesuada fames ac turpis
egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere
iaculis. Suspendisse et maximus elit. In fringilla gravida ornare.
Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel
neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet
vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor
lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque
ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse
potenti.

\autocite{perry2023}

\section{Preview of following sections
(rename)}\label{preview-of-following-sections-rename}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis
posuere ligula sit amet lacinia. Duis dignissim pellentesque magna,
rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum
in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit
amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit,
non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante
volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit
amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim,
at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis
parturient montes, nascetur ridiculus mus.

\bookmarksetup{startatroot}

\chapter{Background}\label{background}

\section{Fuzz Testing}\label{fuzz-testing}

\emph{Fuzzing} is an automated software-testing technique in which a
\emph{Program Under Test} (PUT) is executed with (pseudo-)random inputs
in the hope of exposing undefined behavior. When such behavior manifests
as a crash, hang, or memory-safety violation, the corresponding input
constitutes a \emph{test-case} that reveals a bug and often a
vulnerability \autocite{manes2019}. In essence, fuzzing is a form of
adversarial, penetration-style testing carried out by the defender
before the adversary has an opportunity to do so. Interest in the
technique surged after the publication of three practitioner-oriented
books in 2007--2008 \autocite{takanen2018,sutton2007,rathaus2007}.

Historically, the term was coined by Miller et al.~in 1990, who used
``fuzz'' to describe a program that ``generates a stream of random
characters to be consumed by a target program'' \autocite{miller1990}.
This informal usage captured the essence of what fuzzing aims to do:
stress test software by bombarding it with unexpected inputs to reveal
bugs. To formalize this concept, we adopt Manes et al.'s rigorous
definitions \autocite{manes2019}:

\begin{definition}[Fuzzing]\protect\hypertarget{def-fuzzing}{}\label{def-fuzzing}

Fuzzing is the execution of a Program Under Test (PUT) using input(s)
sampled from an input space (the \emph{fuzz input space}) that protrudes
the expected input space of the PUT.

\end{definition}

This means fuzzing involves running the target program on inputs that go
beyond those it is typically designed to handle, aiming to uncover
hidden issues. An individual instance of such execution---or a bounded
sequence thereof---is called a \emph{fuzzing run}. When these runs are
conducted systematically and at scale with the specific goal of
detecting violations of a security policy, the activity is known as
\emph{fuzz testing} (or simply \emph{fuzzing}):

\begin{definition}[Fuzz
Testing]\protect\hypertarget{def-fuzz-testing}{}\label{def-fuzz-testing}

Fuzz testing is the use of fuzzing to test whether a PUT violates a
security policy.

\end{definition}

This distinction highlights that fuzz testing is fuzzing with an
explicit focus on security properties and policy enforcement. Central to
managing this process is the \emph{fuzzer engine}, which orchestrates
the execution of one or more fuzzing runs as part of a \emph{fuzz
campaign}. A fuzz campaign represents a concrete instance of fuzz
testing tailored to a particular program and security policy:

\begin{definition}[Fuzzer, Fuzzer
Engine]\protect\hypertarget{def-fuzzer}{}\label{def-fuzzer}

A fuzzer is a program that performs fuzz testing on a PUT.

\end{definition}

\begin{definition}[Fuzz
Campaign]\protect\hypertarget{def-campaign}{}\label{def-campaign}

A fuzz campaign is a specific execution of a fuzzer on a PUT with a
specific security policy.

\end{definition}

Throughout each execution within a campaign, a \emph{bug oracle} plays a
critical role in evaluating the program's behavior to determine whether
it violates the defined security policy:

\begin{definition}[Bug
Oracle]\protect\hypertarget{def-oracle}{}\label{def-oracle}

A bug oracle is a component (often inside the fuzzer) that determines
whether a given execution of the PUT violates a specific security
policy.

\end{definition}

In practice, bug oracles often rely on runtime instrumentation
techniques, such as monitoring for fatal POSIX signals (e.g.,
\texttt{SIGSEGV}) or using sanitizers like AddressSanitizer (ASan)
\autocite{serebryany2012}. Tools like LibFuzzer \autocite{libfuzzer}
commonly incorporate such instrumentation to reliably identify crashes
or memory errors during fuzzing.

Most fuzz campaigns begin with a set of \emph{seeds}---inputs that are
well-formed and belong to the PUT's expected input space---called a
\emph{seed corpus}. These seeds serve as starting points from which the
fuzzer generates new test cases by applying transformations or
mutations, thereby exploring a broader input space:

\begin{definition}[Seed]\protect\hypertarget{def-seed}{}\label{def-seed}

An input given to the PUT that is mutated by the fuzzer to produce new
test cases. During a fuzz campaign (Definition~\ref{def-campaign}) all
seeds are stored in a seed \emph{pool} or \emph{corpus}.

\end{definition}

The process of selecting an effective initial corpus is crucial because
it directly impacts how quickly and thoroughly the fuzzer can cover the
target program's code. This challenge---studied as the
\emph{seed-selection problem}---involves identifying seeds that enable
rapid discovery of diverse execution paths and is non-trivial
\autocite{rebert2014}. A well-chosen seed set often accelerates bug
discovery and improves overall fuzzing efficiency.

\subsection{Motivation}\label{motivation-1}

\begin{quote}
The purpose of fuzzing relies on the assumption that there are bugs
within every program, which are waiting to be discovered. Therefore, a
systematic approach should find them sooner or later.

--- OWASP Foundation \autocite{owaspfoundation}
\end{quote}

Fuzz testing offers several tangible benefits:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Early vulnerability discovery}: Detecting defects during
  development is cheaper and safer than addressing exploits in
  production.
\item
  \textbf{Adversary-parity}: Performing the same randomised stress that
  attackers employ allows defenders to pre-empt zero-days.
\item
  \textbf{Robustness and correctness}: Beyond security, fuzzing exposes
  logic errors and stability issues in complex, high-throughput APIs
  (e.g., decompressors) even when inputs are \emph{expected} to be
  well-formed.
\item
  \textbf{Regression prevention}: Re-running a corpus of crashing inputs
  as part of continuous integration ensures that fixed bugs remain
  fixed.
\end{enumerate}

\subsubsection{Success Stories}\label{success-stories}

\emph{Heartbleed} (CVE-2014-0160) \autocite{heartbleed,heartbleed-cve}
arose from a buffer over-read\footnote{\url{https://xkcd.com/1354/}} in
OpenSSL \autocite{theopensslproject2025} introduced on 1 February 2012
and unnoticed until 1 April 2014. Post-mortem analyses showed that a
simple fuzz campaign exercising the TLS heartbeat extension would have
revealed the defect almost immediately \autocite{wheeler2014}.

Likewise, the \emph{Shellshock} (or \emph{Bashdoor}) family of bugs in
GNU Bash \autocite{bash} enabled arbitrary command execution on many
UNIX systems. While the initial flaw was fixed promptly, subsequent bug
variants were discovered by Google's Michał Zalewski using his own
fuzzer \autocite{afl} in late 2014 \autocite{saarinen2014}.

On the defensive tooling side, the security tool named
\emph{Mayhem}---developed by the company of the same name---has since
been adopted by the US Air Force, the Pentagon, Cloudflare, and numerous
open-source communities. It has found and facilitated the remediation of
thousands of previously unknown vulnerabilities
\autocite{simonite2020mayhem}.

These cases underscore the central thesis of fuzz testing: exhaustive
manual review is infeasible, but scalable stochastic exploration
reliably surfaces the critical few defects that matter most.

\subsection{Methodology}\label{methodology}

As previously discussed, fuzz testing of a program under test (PUT) is
typically conducted using a dedicated fuzzing engine (see
Definition~\ref{def-fuzzer}). Among the most widely adopted fuzzers for
C and C++ projects and libraries are AFL \autocite{afl}---which has
since evolved into AFL++ \autocite{aflpp}---and LibFuzzer
\autocite{libfuzzer}. Within the OverHAuL framework, LibFuzzer is
preferred owing to its superior suitability for library fuzzing, whereas
AFL++ predominantly targets executables and binary fuzzing.

\subsubsection{LibFuzzer}\label{libfuzzer}

LibFuzzer \autocite{libfuzzer} is an in-process, coverage-guided
evolutionary fuzzing engine primarily designed for testing libraries. It
forms part of the LLVM ecosystem \autocite{llvm} and operates by linking
directly with the library under evaluation. The fuzzer delivers mutated
input data to the library through a designated fuzzing entry point,
commonly referred to as the \emph{fuzz target}.

\begin{definition}[Fuzz
target]\protect\hypertarget{def-target}{}\label{def-target}

A function that accepts a byte array as input and exercises the
application programming interface (API) under test using these inputs
\autocite{libfuzzer}. This construct is also known as a \emph{fuzz
driver}, \emph{fuzzer entry point}, or \emph{fuzzing harness}.

\end{definition}

For the remainder of this thesis, the terms presented in
Definition~\ref{def-target} will be used interchangeably.

To effectively validate an implementation or library, developers are
required to author a fuzzing harness that invokes the target library's
API functions utilizing the fuzz-generated inputs. This harness serves
as the principal interface for the fuzzer and is executed iteratively,
each time with mutated input designed to maximize code coverage and
uncover defects. To comply with LibFuzzer's interface requirements, a
harness must conform to the following function signature:

\begin{codelisting}

\caption{\label{lst-basic-example}This function receives the fuzzing
input via a pointer to an array of bytes (\texttt{Data}) and its
associated size (\texttt{Size}). Efficiency in fuzzing is achieved by
invoking the API of interest within the body of this function, thereby
allowing the fuzzer to explore a broad spectrum of behavior through
systematic input mutation.}

\centering{

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\DataTypeTok{int}\NormalTok{ LLVMFuzzerTestOneInput}\OperatorTok{(}\DataTypeTok{const} \DataTypeTok{uint8\_t} \OperatorTok{*}\NormalTok{Data}\OperatorTok{,} \DataTypeTok{size\_t}\NormalTok{ Size}\OperatorTok{)} \OperatorTok{\{}
\NormalTok{  DoSomethingInterestingWithData}\OperatorTok{(}\NormalTok{Data}\OperatorTok{,}\NormalTok{ Size}\OperatorTok{);}
  \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

A more illustrative example of such a harness is provided in
Listing~\ref{lst-fuzzing-example}.

\begin{codelisting}

\caption{\label{lst-fuzzing-example}This example demonstrates a minimal
harness that triggers a controlled crash upon receiving \texttt{HI!} as
input.}

\centering{

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\CommentTok{// test\_fuzzer.cpp}
\PreprocessorTok{\#include }\ImportTok{\textless{}stdint.h\textgreater{}}
\PreprocessorTok{\#include }\ImportTok{\textless{}stddef.h\textgreater{}}

\AttributeTok{extern} \StringTok{"C"} \DataTypeTok{int}\NormalTok{ LLVMFuzzerTestOneInput}\OperatorTok{(}\AttributeTok{const} \DataTypeTok{uint8\_t} \OperatorTok{*}\NormalTok{data}\OperatorTok{,} \DataTypeTok{size\_t}\NormalTok{ size}\OperatorTok{)} \OperatorTok{\{}
  \ControlFlowTok{if} \OperatorTok{(}\NormalTok{size }\OperatorTok{\textgreater{}} \DecValTok{0} \OperatorTok{\&\&}\NormalTok{ data}\OperatorTok{[}\DecValTok{0}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}H\textquotesingle{}}\OperatorTok{)}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{size }\OperatorTok{\textgreater{}} \DecValTok{1} \OperatorTok{\&\&}\NormalTok{ data}\OperatorTok{[}\DecValTok{1}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}I\textquotesingle{}}\OperatorTok{)}
      \ControlFlowTok{if} \OperatorTok{(}\NormalTok{size }\OperatorTok{\textgreater{}} \DecValTok{2} \OperatorTok{\&\&}\NormalTok{ data}\OperatorTok{[}\DecValTok{2}\OperatorTok{]} \OperatorTok{==} \CharTok{\textquotesingle{}!\textquotesingle{}}\OperatorTok{)}
        \FunctionTok{\_\_builtin\_trap}\OperatorTok{();}
  \ControlFlowTok{return} \DecValTok{0}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

To compile and link such a harness with LibFuzzer, the Clang
compiler---also part of the LLVM project \autocite{llvm}---must be used
alongside appropriate compiler flags. For instance, compiling the
harness in Listing~\ref{lst-fuzzing-example} can be achieved as follows:

\begin{codelisting}

\caption{\label{lst-harness-compilation}This example illustrates the
compilation and execution workflow necessary for deploying a
LibFuzzer-based fuzzing harness.}

\centering{

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\CommentTok{\# Compile test\_fuzzer.cc with AddressSanitizer and link against LibFuzzer.}
\FunctionTok{clang}\NormalTok{++ }\AttributeTok{{-}fsanitize}\OperatorTok{=}\NormalTok{address,fuzzer test\_fuzzer.cc}
\CommentTok{\# Execute the fuzzer without any pre{-}existing seed corpus.}
\ExtensionTok{./a.out}
\end{Highlighting}
\end{Shaded}

}

\end{codelisting}%

\subsubsection{AFL and AFL++}\label{afl-and-afl}

\emph{American Fuzzy Lop} (AFL) \autocite{afl}, developed by Michał
Zalewski, is a seminal fuzzer targeting C and C++ applications. Its core
methodology relies on instrumented binaries to provide edge coverage
feedback, thereby guiding input mutation towards unexplored program
paths. AFL supports several emulation backends including QEMU
\autocite{bellard2025}---an open-source CPU emulator facilitating
fuzzing on diverse architectures---and Unicorn
\autocite{unicornengine2025}, a lightweight multi-platform CPU emulator.
While AFL established itself as a foundational tool within the fuzzing
community, its successor AFL++ \autocite{aflpp} incorporates numerous
enhancements and additional features to improve fuzzing efficacy.

AFL operates by ingesting seed inputs from a specified directory
(\texttt{seeds\_dir}), applying mutations, and then executing the target
binary to discover novel execution paths. Execution can be initiated
using the following command-line syntax:

\begin{Shaded}
\begin{Highlighting}[numbers=left,,]
\ExtensionTok{./afl{-}fuzz} \AttributeTok{{-}i}\NormalTok{ seeds\_dir }\AttributeTok{{-}o}\NormalTok{ output\_dir }\AttributeTok{{-}{-}}\NormalTok{ /path/to/tested/program}
\end{Highlighting}
\end{Shaded}

AFL is capable of fuzzing both black-box and instrumented binaries,
employing a fork-server mechanism to optimize performance. It
additionally supports persistent mode execution as well as modes
leveraging QEMU and Unicorn emulators, thereby providing extensive
flexibility for different testing environments.

Although AFL is traditionally utilized for fuzzing standalone programs
or binaries, it is also capable of fuzzing libraries and other software
components. In such scenarios, rather than implementing the
\texttt{LLVMFuzzerTestOneInput} style harness, AFL can use the standard
\texttt{main()} function as the fuzzing entry point. Nonetheless, AFL
also accommodates integration with \texttt{LLVMFuzzerTestOneInput}-based
harnesses, underscoring its adaptability across varied fuzzing use
cases.

\subsection{Challenges in Adoption}\label{challenges-in-adoption}

Despite its potential for uncovering software vulnerabilities, fuzzing
remains a relatively underutilized testing technique compared to more
established methodologies such as Test-Driven Development (TDD). This
limited adoption can be attributed, in part, to the substantial initial
investment required to design and implement appropriate test harnesses
that enable effective fuzzing processes. Furthermore, the interpretation
of fuzzing outcomes---particularly the identification, diagnostic
analysis, and prioritization of program crashes---demands considerable
resources and specialized expertise. These factors collectively pose
significant barriers to the widespread integration of fuzzing within
standard software development and testing practices.

\section{Large Language Models (LLMs)}\label{large-language-models-llms}

Natural Language Processing (NLP), a subfield of Artificial Intelligence
(AI), has a rich and ongoing history that has evolved significantly
since its beginning in the 1990s \autocite{li2022,wang2025}. Among the
most notable---and recent---advancements in this domain are Large
Language Models (LLMs), which have transformed the landscape of NLP and
AI in general.

At the core of many LLMs is the attention mechanism, which was
introduced by Bahdanau et al.~in 2014 \autocite{bahdanau2016}. This
pivotal innovation enabled models to focus on relevant parts of the
input sequence when making predictions, significantly improving language
understanding and generation tasks. Building on this foundation, the
Transformer architecture was proposed by Vaswani et al.~in 2017
\autocite{vaswani2023}. This architecture has become the backbone of
most contemporary LLMs, as it efficiently processes sequences of data,
capturing long-range dependencies without being hindered by sequential
processing limitations.

One of the first major breakthroughs utilizing the Transformer
architecture was BERT (Bidirectional Encoder Representations from
Transformers), developed by Devlin et al.~in 2019 \autocite{devlin2019}.
BERT's bi-directional understanding allowed it to capture the context of
words from both directions, which improved the accuracy of various NLP
tasks. Following this, the Generative Pre-trained Transformer (GPT)
series, initiated by OpenAI with the original GPT model in 2018
\autocite{radford2018}, further pushed the boundaries. Subsequent
iterations, including GPT-2 \autocite{radford2019}, GPT-3
\autocite{brown2020}, and the most current GPT-4 \autocite{openai2024},
have continued to enhance performance by scaling model size, data, and
training techniques.

In addition to OpenAI's contributions, other significant models have
emerged, such as Claude, DeepSeek-R1 and the Llama series (1 through 3)
\autocite{claude,deepseek-ai2025,grattafiori2024}. The proliferation of
LLMs has sparked an active discourse about their capabilities,
applications, and implications in various fields.

\subsection{Biggest GPTs}\label{biggest-gpts}

User-facing LLMs are generally categorized between closed-source and
open-source models. Closed-source LLMs like ChatGPT, Claude, and Gemini
\autocite{chatgpt,claude,gemini} represent commercially developed
systems often optimized for specific tasks without public access to
their underlying weights. In contrast, open-source models\footnote{The
  term ``open-source'' models is somewhat misleading, since these are
  better termed as \emph{open-weights} models. While their weights are
  publicly available, their training data and underlying code are often
  proprietary. This terminology reflects community usage but fails to
  capture the limitations of transparency and accessibility inherent in
  these models.}, including the Llama series \autocite{grattafiori2024}
and Deepseek \autocite{deepseek-ai2025}, provide researchers and
practitioners with access to model weights, allowing for greater
transparency and adaptability.

\subsection{Prompting}\label{sec-prompting}

Interaction with LLMs typically occurs through chat-like interfaces, a
process commonly referred to as \emph{prompting}. A critical aspect of
effective engagement with LLMs is the usage of different prompting
strategies, which can significantly influence the quality and relevance
of the generated outputs. Various approaches to prompting have been
developed and studied, including zero-shot and few-shot prompting. In
zero-shot prompting, the model is expected to perform a specific task
without any examples, while in few-shot prompting, the user provides a
limited number of examples to guide the model's responses
\autocite{brown2020}.

To enhance performance on more complex tasks, several advanced prompting
techniques have emerged. One notable strategy is the \emph{Chain of
Thought} approach \autocite{chainofthought}, which entails presenting
the model with sample thought processes for solving a given task. This
method encourages the model to generate more coherent and logical
reasoning by mimicking human-like cognitive pathways. A refined variant
of this approach is the \emph{Tree of Thoughts} technique
\autocite{yao2023}, which enables the LLM to explore multiple lines of
reasoning concurrently, thereby facilitating the selection of the most
promising train of thought for further exploration.

In addition to these cognitive strategies, Retrieval-Augmented
Generation (RAG) \autocite{lewis2021} is another innovative technique
that enhances the model's capacity to provide accurate information by
incorporating external knowledge not present in its training dataset.
RAG operates by integrating the LLM with an external storage
system---often a vector store containing relevant documents---that the
model can query in real-time. This allows the LLM to pull up pertinent
and/or proprietary information in response to user queries, resulting in
more comprehensive and accurate answers.

Moreover, the ReAct framework \autocite{reAct}, which stands for
Reasoning and Acting, empowers LLMs by granting access to external
tools. This capability allows LLM instances to function as intelligent
agents that can interact meaningfully with their environment through
user-defined tools. For instance, a ReAct tool could be a function that
returns a weather forecast based on the user's current location. In this
scenario, the LLM can provide accurate and truthful predictions, thereby
mitigating risks associated with hallucinated responses.

\subsection{LLMs for Coding}\label{sec-llm-coding}

The impact of LLMs in software development in recent years is apparent,
with hundreds of LLM-assistance extensions and Integrated Development
Environments (IDEs) being published. Notable instances include tools
like GitHub Copilot and IDEs such as Cursor, which leverage LLM
capabilities to provide developers with coding suggestions,
auto-completions, and even real-time debugging assistance
\autocite{cursor,ghcopilot}. Such innovations have introduced a layer of
interaction that enhances productivity and fosters a more intuitive
coding experience. Simultaneously, certain LLMs are trained themselves
with the code-generation task in mind
\autocite{nijkamp2023a,nijkamp2023,openai2025a}.

One exemplary product of this innovation is \emph{vibecoding} and the
no-code movement, which describe the development of software by only
prompting and tasking an LLM, i.e.~without any actual programming
required by the user. This constitutes a showcase of how LLMs can be
harnessed to elevate the coding experience by supporting developers as
they navigate complex programming tasks \autocite{sarkar2025}. By
analyzing the context of the code being written, these sophisticated
models can provide contextualized insights and relevant snippets,
effectively streamlining the development process. Developers can benefit
from reduced cognitive load, as they receive suggestions that not only
cater to immediate coding needs but also promote adherence to best
practices and coding standards.

Despite these advancements, it is crucial to recognize the inherent
limitations of LLMs when applied to software development. While they can
help in many aspects of coding, they are not immune to generating
erroneous outputs---a phenomenon often referred to as ``hallucination''.
Hallucinations occur when LLMs produce information that is unfounded or
inaccurate, which can stem from several factors, including the
limitations of their training data and the constrained context window
within which they operate. As LLMs generate code suggestions based on
the patterns learned from vast datasets, they may inadvertently propose
solutions that do not align with the specific requirements of a task or
that utilize outdated programming paradigms.

Moreover, the challenge of limited context windows can lead to
suboptimal suggestions. LLMs generally process a fixed amount of text
when generating responses, which can impact their ability to fully grasp
the nuances of complex coding scenarios. This may result in outputs that
lack the necessary depth and specificity required for successful
implementation. As a consequence, developers must exercise caution and
critically evaluate the suggestions offered by these models, as reliance
on them without due diligence could lead to the introduction of bugs or
other issues in the code.

\subsection{LLMs for Fuzzing}\label{llms-for-fuzzing}

While large language models (LLMs) demonstrate significant potential in
enhancing the software development process, the challenges highlighted
in Section~\ref{sec-llm-coding} become even more pronounced and
troublesome when these models are employed to generate fuzzing
harnesses. The task of writing a fuzzing harness inherently demands an
in-depth comprehension of both the library being tested and the
intricate interactions expected among its various components. This level
of understanding is often beyond the capabilities of LLMs, primarily due
to their context window limitations, which restrict the amount of
information they can effectively process and retain during code
generation.

In addition to this issue, the risk of error-prone code produced by LLMs
further complicates the fuzzing workflow. When a crash occurs during the
fuzzing process, it becomes imperative for developers to ascertain that
the root cause of the failure is not attributable to deficiencies or
bugs within the harness itself. This additional layer of verification
adds to the cognitive load placed upon developers, potentially
detracting from their ability to focus on testing and improving the
underlying software.

To enhance the reliability of LLM-generated harnesses in fuzzing
contexts, it is essential that these generated artifacts undergo
thorough evaluation and validation through programmatic means. This
involves the implementation of systematic techniques that assess the
accuracy and robustness of the generated code, ensuring that it aligns
with the expected behavior of the components it is intended to interact
with. This strategy can be conceptualized within the framework of
Neurosymbolic AI, which seeks to integrate the strengths of neural
networks with symbolic reasoning capabilities. By marrying these two
paradigms, it may be possible to improve the reliability and efficacy of
LLMs in the creation of fuzzing harnesses, ultimately leading to a more
seamless integration of automated testing methodologies into the
software development lifecycle.

\section{Neurosymbolic AI}\label{neurosymbolic-ai}

qqqq

\hl{TODO}
\autocite{ganguly2024,garcez2020,gaur2023,grov2024,sheth2023,tilwani2024}.

\subsection{What is it?}\label{what-is-it}

\autocite{sheth2023,gaur2023} Neurosymbolic AI for attribution in LLMs
\autocite{tilwani2024}

\subsection{What does it solve?}\label{what-does-it-solve}

\subsection{Its state}\label{its-state}

restatement of overarching goal, segue to section 2

\bookmarksetup{startatroot}

\chapter{Related work}\label{related-work}

Automated testing, automated fuzzing and automated harness creation have
a long research history. Still, a lot of ground remains to be covered
until true automation of these tasks is achieved. Until the introduction
of transformers \autocite{vaswani2023} and the 2020's boom of commercial
GPTs \autocite{chatgpt}, automation regarding testing and fuzzing was
mainly attempted through static and dynamic program analysis methods.
These approaches are still utilized, but the fuzzing community has
shifted almost entirely to researching the incorporation and employment
of LLMs in the last half decade, in the name of automation
\autocite{iris,sun2024,prophetfuzz,oss-fuzz-gen,green2022,utopia,fuzzgpt,titanfuzz,fuzzgen,fudge}.

\section{Previous Projects}\label{previous-projects}

\subsection{KLEE}\label{klee}

KLEE \autocite{klee} is a seminal and widely cited symbolic execution
engine introduced in 2008 by Cadar et al.~It was designed to
automatically generate high-coverage test cases for programs written in
C, using symbolic execution to systematically explore the control flow
of a program. KLEE operates on the LLVM \autocite{llvm} bytecode
representation of programs, allowing it to be applied to a wide range of
C programs compiled to the LLVM intermediate representation.

Instead of executing a program on concrete inputs, KLEE performs
symbolic execution---that is, it runs the program on symbolic inputs,
which represent all possible values simultaneously. At each conditional
branch, KLEE explores both paths by forking the execution and
accumulating path constraints (i.e., logical conditions on input
variables) along each path. This enables it to traverse many feasible
execution paths in the program, including corner cases that may be
difficult to reach through random testing or manual test creation.

When an execution path reaches a terminal state (e.g., a program exit,
an assertion failure, or a segmentation fault), KLEE uses a constraint
solver to compute concrete input values that satisfy the accumulated
constraints for that path. These values form a test case that will
deterministically drive the program down that specific path when
executed concretely.

\subsection{IRIS}\label{iris}

IRIS \autocite{iris} is a 2025 open-source neurosymbolic system for
static vulnerability analysis. Given a codebase and a list of
user-specified Common Weakness Enumerations (CWEs), it analyzes source
code to identify paths that may correspond to known vulnerability
classes. IRIS combines symbolic analysis---such as control- and
data-flow reasoning---with neural models trained to generalize over code
patterns. It outputs candidate vulnerable paths along with explanations
and CWE references. The system operates on full repositories and
supports extensible CWE definitions.

\subsection{FUDGE}\label{fudge}

FUDGE \autocite{fudge} is a closed-source tool, made by Google, for
automatic harness generation of C and C++ projects based on existing
client code. It was used in conjunction with and in the improvement of
Google's OSS-Fuzz \autocite{oss-fuzz}. Being deployed inside Google's
infrastructure, FUDGE continuously examines Google's internal code
repository, searching for code that uses external libraries in a
meaningful and ``fuzzable'' way (i.e.~predominantly for parsing). If
found, such code is \textbf{sliced} \autocite{sasirekha2011Slicing}, per
FUDGE, based on its Abstract Syntax Tree (AST) using LLVM's Clang tool
\autocite{llvm}. The above process results in a set of abstracted
mostly-self-contained code snippets that make use of a library's calls
and/or API. These snippets are later \textbf{synthesized} into the body
of a fuzz driver, with variables being replaced and the fuzz input being
utilized. Each is then injected in an \texttt{LLVMFuzzerTestOneInput}
function and finalized as a fuzzing harness. A building and evaluation
phase follows for each harness, where they are executed and examined.
Every passing harness along with its evaluation results is stored in
FUDGE's database, reachable to the user through a custom web-based UI.

\subsection{UTopia}\label{utopia}

UTopia \autocite{utopia} (stylized \textsc{UTopia}) is another
open-source automatic harness generation framework. Aside from the
library code, It operates solely on user-provided unit tests since,
according to \textcite{utopia}, they are a resource of complete and
correct API usage examples containing working library set-ups and
tear-downs. Additionally, each of them are already close to a fuzz
target, in the sense that they already examine a single and
self-contained API usage pattern. Each generated harness follows the
same data flow of the originating unit test. Static analysis is employed
to figure out what fuzz input placement would yield the most results. It
is also utilized in abstracting the tests away from the syntactical
differences between testing frameworks, along with slicing and AST
traversing using Clang.

\subsection{FuzzGen}\label{fuzzgen}

Another project of Google is FuzzGen \autocite{fuzzgen}, this time
open-source. Like FUDGE, it leverages existing client code of the target
library to create fuzz targets for it. FuzzGen uses whole-system
analysis, through which it creates an \emph{Abstract API Dependence
Graph} (A\textsuperscript{2}DG). It uses the latter to automatically
generate LibFuzzer-compatible harnesses. For FuzzGen to work, the user
needs to provide both client code and/or tests for the API and the API
library's source code as well. FuzzGen uses the client code to infer the
\emph{correct usage} of the API and not its general structure, in
contrast to FUDGE. FuzzGen's workflow can be divided into three phases:
\textbf{1. API usage inference}. By consuming and analyzing client code
and tests that concern the library under test, FuzzGen recognizes which
functions belong to the library and learns its correct API usage
patterns. This process is done with the help of Clang. To test if a
function is actually a part of the library, a sample program is created
that uses it. If the program compiles successfully, then the function is
indeed a valid API call. \textbf{2. A\textsuperscript{2}DG construction
mechanism}. For all the existing API calls, FuzzGen builds an
A\textsuperscript{2}DG to record the API usages and infers its intended
structure. After completion, this directed graph contains all the valid
API call sequences found in the client code corpus. It is built in a
two-step process: First, many smaller A\textsuperscript{2}DGs are
created, one for each root function per client code snippet. Once such
graphs have been created for all the available client code instances,
they are combined to formulate the master A\textsuperscript{2}DG. This
graph can be seen as a template for correct usage of the library.
\textbf{3. Fuzzer generator}. Through the A\textsuperscript{2}DG, a
fuzzing harness is created. Contrary to FUDGE, FuzzGen does not create
multiple ``simple'' harnesses but a single complex one with the goal of
covering the whole of the A\textsuperscript{2}DG. In other words, while
FUDGE fuzzes a single API call at a time, FuzzGen's result is a single
harness that tries to fuzz the given library all at once through complex
API usage.

\subsection{OSS-Fuzz}\label{oss-fuzz}

OSS-Fuzz \autocite{ossfuzzdocs2025,oss-fuzz} is a continuous, scalable
and distributed cloud fuzzing solution for critical and prominent
open-source projects. Developers of such software can submit their
projects to OSS-Fuzz's platform, where its harnesses are built and
constantly executed. This results in multiple bug findings that are
later disclosed to the primary developers and are later patched.

OSS-Fuzz started operating in 2016, an initiative in response to the
Heartbleed vulnerability
\autocite{heartbleed-cve,wheeler2014,heartbleed}. Its hope is that
through more extensive fuzzing such errors could be caught and corrected
before having the chance to be exploited and thus disrupt the public
digital infrastructure. So far, it has helped uncover over 10,000
security vulnerabilities and 36,000 bugs across more than 1,000
projects, significantly enhancing the quality and security of major
software like Chrome, OpenSSL, and systemd.

A project that's part of OSS-Fuzz must have been configured as a
ClusterFuzz \autocite{clusterfuzz} project. ClusterFuzz is the fuzzing
infrastructure that OSS-Fuzz uses under the hood and depends on Google
Cloud Platform services, although it can be hosted locally. Such an
integration requires setting up a build pipeline, fuzzing jobs and
expects a Google Developer account. Results are accessible through a web
interface. ClusterFuzz, and by extension OSS-Fuzz, supports fuzzing
through LibFuzzer, AFL++, Honggfuzz and FuzzTest---successor to
Centipede--- with the last two being Google projects
\autocite{libfuzzer,fuzztest,honggfuzz,aflpp}. C, C++, Rust, Go, Python
and Java/JVM projects are supported.

\subsection{OSS-Fuzz-Gen}\label{oss-fuzz-gen}

OSS-Fuzz-Gen (OFG) \autocite{liu2023,oss-fuzz-gen} is Google's current
State-Of-The-Art (SOTA) project regarding automatic harness generation
through LLMs. It's purpose is to improve the fuzzing infrastructure of
open-source projects that are already integrated into OSS-Fuzz. Given
such a project, OSS-Fuzz-Gen uses its preexisting fuzzing harnesses and
modifies them to produce new ones. Its architecture can be described as
follows: 1. With an OSS-Fuzz project's GitHub repository link,
OSS-Fuzz-Gen iterates through a set of predefined build templates and
generates potential build scripts for the project's harnesses. 2. If any
of them succeed they are once again compiled, this time through
fuzz-introspector \autocite{fuzz-introspector}. The latter constitutes a
static analysis tool, with fuzzer developers specifically in mind. 3.
Build results, old harness and fuzz-introspector report are included in
a template-generated prompt, through which an LLM is called to generate
a new harness. 4. The newly generated fuzz target is compiled and if it
is done so successfully it begins execution inside OSS-Fuzz's
infrastructure.

This method proved meaningful, with code coverage in fuzz campaigns
increasing thanks to the new generated fuzz drivers. In the case of
\autocite{thomason2025}, line coverage went from 38\% to 69\% without
any manual interventions \autocite{liu2023}.

In 2024, OSS-Fuzz-Gen introduced an experimental feature for generating
harnesses in previously unfuzzed projects
\autocite{oss-fuzzmaintainers2024}. The code for this feature resides in
the \texttt{experimental/from\_scratch} directory of the project's
GitHub repository \autocite{oss-fuzz-gen}, with the latest known working
commit being \texttt{171aac2} and the latest overall commit being four
months ago.

\subsection{AutoGen}\label{autogen}

AutoGen \autocite{sun2024} is a closed-source tool that generates new
fuzzing harnesses, given only the library code and documentation. It
works as following: The user specifies the function for which a harness
is to be generated. AutoGen gathers information for this function---such
as the function body, used header files, function calling
examples---from the source code and documentation. Through specific
prompt templates containing the above information, an LLM is tasked with
generating a new fuzz driver, while another is tasked with generating a
compilation command for said driver. If the compilation fails, both LLMs
are called again to fix the problem, whether it was on the driver's or
command's side. This loop iterates until a predefined maximum value or
until a fuzz driver is successfully generated and compiled. If the
latter is the case, it is then executed. If execution errors exist, the
LLM responsible for the driver generation is used to correct them. If
not, the pipeline has terminated and a new fuzz driver has been
successfully generated.

\section{Differences}\label{differences}

OverHAuL differs, in some way, with each of the aforementioned works.
Firstly, although KLEE and IRIS \autocite{iris,klee} tackle the problem
of automated testing and both IRIS and OverHAuL can be considered
neurosymbolic AI tools, the similarities end there. None of them utilize
LLMs the same way we do---with KLEE not utilizing them by default, as it
precedes them chronologically---and neither are automating any part of
the fuzzing process.

When it comes to FUDGE, FuzzGen and UTopia
\autocite{utopia,fuzzgen,fudge}, all three depend on and demand existing
client code and/or unit tests. On the other hand, OverHAuL requires only
the bare minimum: the library code itself. Another point of difference
is that in contrast with OverHAuL, these tools operate in a linear
fashion. No feedback is produced or used in any step and any point
failure results in the termination of the entire run.

OverHAuL challenges a common principle of these tools, stated explicitly
in FUDGE's paper \autocite{fudge}: ``Choosing a suitable fuzz target
(still) requires a human''. OverHAuL chooses to let the LLM, instead of
the user, explore the available functions and choose one to target in
its fuzz driver.

OSS-Fuzz-Gen \autocite{oss-fuzz-gen} can be considered a close
counterpart of OverHAuL, and in some ways it is. A lot of inspiration
was gathered from it, like for example the inclusion of static analysis
and its usage in informing the LLM. Yet, OSS-Fuzz-Gen has a number of
disadvantages that make it in some cases an inferior option. For one,
OFG is tightly coupled with the OSS-Fuzz platform \autocite{oss-fuzz},
which even on its own creates a plethora of issues for the common
developer. To integrate their project into OSS-Fuzz, they would need to:
Transform it into a ClusterFuzz project \autocite{clusterfuzz} and take
time to write harnesses for it. Even if these prerequisites are carried
out, it probably would not be enough. Per OSS-Fuzz's documentation
\autocite{ossfuzzdocs2025}: ``To be accepted to OSS-Fuzz, an open-source
project must have a significant user base and/or be critical to the
global IT infrastructure''. This means that OSS-Fuzz is a viable option
only for a small minority of open-source developers and maintainers. One
countermeasure of the above shortcoming would be for a developer to run
OSS-Fuzz-Gen locally. This unfortunately proves to be an arduous task.
As it is not meant to be used standalone, OFG is not packaged in the
form of a self-contained application. This makes it hard to setup and
difficult to use interactively. Like in the case of FUDGE, OFG's actions
are performed linearly. No feedback is utilized nor is there graceful
error handling in the case of a step's failure. Even in the case of the
experimental feature for bootstrapping unfuzzed projects, OFG's
performance varies heavily. During experimentation, a lot of generated
harnesses were still wrapped either in Markdown backticks or
\texttt{\textless{}code\textgreater{}} tags, or were accompanied with
explanations inside the generated \texttt{.c} source file. Even if code
was formatted correctly, in many cases it missed necessary headers for
compilation or used undeclared functions.

Lastly, the closest counterpart to OverHAuL is AutoGen
\autocite{sun2024}. Their similarity stands in the implementation of a
feedback loop between LLM and generated harness. However, most other
implementation decisions remain distinct. One difference regards the
fuzzed function. While AutoGen requires a target function to be
specified by the user in which it narrows during its whole run, OverHAuL
delegates this to the LLM, letting it explore the codebase and decide by
itself the best candidate. Another difference lies in the need---and the
lack of---of documentation. While AutoGen requires it to gather
information for the given function, OverHAuL leans into the role of a
developer by reading the related code and comments and thus avoiding any
mismatches between documentation and code. Finally, the LLMs' input is
built based on predefined prompt templates, a technique also present in
OSS-Fuzz-Gen.~OverHAuL operates one abstraction level higher, leveraging
DSPy \autocite{dspy} for programming instead of prompting the LLMs used.

In conclusion, OverHAuL constitutes an \emph{open-source} tool that
offers new functionality by offering a straightforward installation
process, packaged as a self-contained Python package with minimal
external dependencies. It also introduces novel approaches compared to
previous work by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implementing a feedback mechanism between harness generation,
  compilation, and evaluation phases,
\item
  Using autonomous ReAct agents capable of codebase exploration,
\item
  Leveraging a vector store for code consumption and retrieval.
\end{enumerate}

\textbf{TODO} να συμπεριλάβω και τα:

\subsection{IntelliGen
{[}{[}20250711141156{]}{]}}\label{intelligen-20250711141156}

\textbf{SAMPLE}

\textbf{IntelliGen: Automatic Fuzz Driver Synthesis Based on
Vulnerability Heuristics} Zhang et al.~(2021) present
\textbf{IntelliGen}, a system for automatically synthesizing fuzz
drivers by statically identifying potentially vulnerable entry-point
functions within C projects. Implemented using LLVM, IntelliGen focuses
on improving fuzzing efficiency by targeting code more likely to contain
memory safety issues, rather than exhaustively fuzzing all available
functions.

The system comprises two main components: the \textbf{Entry Function
Locator} and the \textbf{Fuzz Driver Synthesizer}. The Entry Function
Locator analyzes the project's abstract syntax tree (AST) and classifies
functions based on heuristics that indicate vulnerability. These include
pointer dereferencing, calls to memory-related functions (e.g.,
\texttt{memcpy}, \texttt{memset}), and invocation of other internal
functions. Functions that score highly on these metrics are prioritized
for fuzz driver generation. The guiding insight is that entry points
with fewer argument checks and more direct memory operations expose more
useful program logic for fuzz testing.

The Fuzz Driver Synthesizer then generates harnesses for these entry
points. For each target function, it synthesizes a
\texttt{LLVMFuzzerTestOneInput} function that invokes the target with
arguments derived from the fuzzer input. This process involves inferring
argument types from the source code and ensuring that runtime behavior
does not violate memory safety---thus avoiding invalid inputs that would
cause crashes unrelated to genuine bugs.

IntelliGen stands out by integrating static vulnerability estimation
into the driver generation pipeline. Compared to prior tools like
FuzzGen and FUDGE, it uses a more targeted, heuristic-based selection of
functions, increasing the likelihood that fuzzing will exercise
meaningful and vulnerable code paths.

\subsection{CKGFuzzer
{[}{[}20250711203054{]}{]}}\label{ckgfuzzer-20250711203054}

\textbf{SAMPLE}

CKGFuzzer is a fuzzing framework designed to automate the generation of
effective fuzz drivers for C/C++ libraries by leveraging static analysis
and large language models. Its workflow begins by parsing the target
project along with any associated library APIs to construct a code
knowledge graph. This involves two primary steps: first, parsing the
abstract syntax tree (AST), and second, performing interprocedural
program analysis. Through this process, CKGFuzzer extracts essential
program elements such as data structures, function signatures, function
implementations, and call relationships.

Using the knowledge graph, CKGFuzzer then identifies and queries
meaningful API combinations, focusing on those that are either
frequently invoked together or exhibit functional similarity. It
generates candidate fuzz drivers for these combinations and attempts to
compile them. Any compilation errors encountered during this phase are
automatically repaired using heuristics and domain knowledge. A
dynamically updated knowledge base, constructed from prior library usage
patterns, guides both the generation and repair processes.

Once the drivers are successfully compiled, CKGFuzzer executes them
while monitoring code coverage at the file level. It uses coverage
feedback to iteratively mutate underperforming API combinations,
refining them until new execution paths are discovered or a preset
mutation budget is exhausted.

Finally, any crashes triggered during fuzzing are subjected to a
reasoning process based on chain-of-thought prompting. To help determine
their severity and root cause, CKGFuzzer consults an LLM-generated
knowledge base containing real-world examples of vulnerabilities mapped
to known Common Weakness Enumeration (CWE) entries.

\subsection{PromptFuzz
{[}{[}20250713225436{]}{]}}\label{promptfuzz-20250713225436}

\textbf{SAMPLE}

Lyu et al.~(2024) introduce PromptFuzz \autocite{lyu2024}, a system for
automatically generating fuzz drivers using LLMs, with a novel focus on
\textbf{prompt mutation} to improve coverage. The system is implemented
in Rust and targets C libraries, aiming to explore more of the API
surface with each iteration.

The workflow begins with the random selection of API functions,
extracted from header file declarations. These functions are used to
construct initial prompts that instruct the LLM to generate a simple
program utilizing the API. Each generated program is compiled, executed,
and monitored for code coverage. Programs that fail to compile or
violate runtime checks (e.g., sanitizers) are discarded.

A key innovation in PromptFuzz is \textbf{coverage-guided prompt
mutation}. Instead of mutating generated code directly, PromptFuzz
mutates the LLM prompts---selecting new combinations of API functions to
target unexplored code paths. This process is guided by a \textbf{power
scheduling} strategy that prioritizes underused or promising API
functions based on feedback from previous runs.

Once an effective program is produced, it is transformed into a fuzz
driver by replacing constants and arguments with variables derived from
the fuzzer input. Multiple such drivers are embedded into a single
harness, where the input determines which program variant to execute,
typically via a case-switch construct.

Overall, PromptFuzz demonstrates that prompt-level mutation enables more
effective exploration of complex APIs and achieves better coverage than
direct code mutations, offering a compelling direction for LLM-based
automated fuzzing systems.

\bookmarksetup{startatroot}

\chapter{OverHAuL}\label{overhaul}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How is it different?
\item
  What does it offer?
\item
  Example uses
\item
  Scope of Usage

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    In what contexts does it work?
  \item
    Prerequisites
  \end{enumerate}
\end{enumerate}

\section{Architecture}\label{architecture}

\begin{itemize}
\item
  \hl{System diagram}
\item
  Main Library Architecture/Structure
\item
  LLM usage

  \begin{itemize}
  \tightlist
  \item
    Prompting techniques used (callback to Section~\ref{sec-prompting}).
  \end{itemize}
\item
  Static analysis
\item
  Code localization(?)
\item
  Fuzzers
\item
  GitHub Workflow/Usage
\item
  ``Ieration budget''
\item
  LLM Programming Libraries (?) Langchain \& LangGraph, LlamaIndex
  \autocite{langchain,langgraph,llamaindex}. DSPy \autocite{dspy}.
  Comparison, relevance to our usecase.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Evaluation}\label{evaluation}

\section{Benchmarks}\label{benchmarks}

Results from integration with 10/100 open-source C/C++ projects.

\section{Performance}\label{performance}

\section{Issues}\label{issues}

\section{Future Work}\label{future-work}

\subsection{Technical Future Work}\label{technical-future-work}

\subsection{Architectural Future
Work/Extensions}\label{architectural-future-workextensions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build system
\item
  More (static) analysis tolls integrations
\item
  General \emph{localization} problem
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Future Work}\label{future-work-1}

The prototype implementation of OverHAuL offers a compelling
demonstration of its potential to automate the fuzzing process for
open-source libraries, providing tangible benefits to developers and
maintainers alike. This initial version successfully validates the core
design principles underpinning OverHAuL, showcasing its ability to
streamline and enhance the software testing workflow through automated
generation of fuzz drivers using large language models. Nevertheless,
while these foundational capabilities lay a solid groundwork, numerous
avenues exist for further expansion, refinement, and rigorous evaluation
to fully realize the tool's potential and adapt to evolving challenges
in software quality assurance.

\section{Enhancements to Core
Features}\label{enhancements-to-core-features}

Enhancing OverHAuL's core functionality represents a primary direction
for future development. First, expanding support to encompass a wider
array of build systems commonly employed in C and C++ projects---such as
GNU Make, CMake, Meson, and Ninja
\autocite{cedilnik2000,feldman1979,martin2025,pakkanen2025}---would
significantly broaden the scope of libraries amenable to automated
fuzzing using OverHAuL. This advancement would enable OverHAuL to scale
effectively and be applied to larger, more complex codebases, thereby
increasing its practical utility and impact.

Second, integrating additional fuzzing engines beyond LibFuzzer stands
out as a strategic enhancement. Incorporation of widely adopted fuzzers
like AFL++ \autocite{aflpp} could diversify the fuzzing strategies
available to OverHAuL, while exploring more experimental tools such as
GraphFuzz \autocite{green2022} may pioneer specialized approaches for
certain code patterns or architectures. Multi-engine support would also
facilitate extending language coverage, for instance by incorporating
fuzzers tailored to other programming ecosystems---for example, Google's
Atheris for Python projects \autocite{atheris}. Such versatility would
position OverHAuL as a more universal fuzzing automation platform.

Third, the evaluation component of OverHAuL presents an opportunity for
refinement through more sophisticated analysis techniques. Beyond the
current criteria, incorporating dynamic metrics such as differential
code coverage tracking between generated fuzz harnesses would yield
deeper insights into test quality and coverage completeness. This
quantitative evaluation could guide iterative improvements in fuzz
driver generation and overall testing effectiveness.

Finally, OverHAuL's methodology could be extended to leverage existing
client codebases and unit tests in addition to the library source code
itself, resources that for now OverHAuL leaves untapped. Inspired by
approaches like those found in FUDGE and FuzzGen
\autocite{fuzzgen,fudge}, this enhancement would enable the tool to
exploit programmer-written usage scenarios as seeds or contexts,
potentially generating more meaningful and targeted fuzz inputs.
Incorporating these richer information sources would likely improve the
efficacy of fuzzing campaigns and uncover subtler bugs.

\section{Experimentation with Large Language Models and Data
Representation}\label{experimentation-with-large-language-models-and-data-representation}

OverHAuL's reliance on large language models (LLMs) invites
comprehensive experimentation with different providers and architectures
to assess their comparative strengths and limitations. Conducting
empirical evaluations across leading models---such as OpenAI's o1 and o3
families and Anthropic's Claude Opus 4---will provide valuable insights
into their capabilities, cost-efficiency, and suitability for fuzz
driver synthesis. Additionally, specialized code-focused LLMs, including
generative and fill-in models like Codex-1 and CodeGen
\autocite{nijkamp2023a,nijkamp2023,openai2025a}, merit exploration due
to their targeted optimization for source code generation and
understanding.

Another dimension worthy of investigation concerns the granularity of
code chunking employed during the given project's code processing stage.
Whereas the current approach partitions code at the function level,
experimenting with more nuanced segmentation strategies---such as
splitting per step inside a function, as a finer-grained
technique---could improve the semantic coherence of stored
representations and enhance retrieval relevance during fuzz driver
generation. This line of inquiry has the potential to optimize model
input preparation and ultimately improve output quality.

\section{Comprehensive Evaluation and
Benchmarking}\label{comprehensive-evaluation-and-benchmarking}

To thoroughly establish OverHAuL's effectiveness, extensive large-scale
evaluation beyond the initial 10-project corpus is imperative. Applying
the tool to repositories indexed in the clib package manager
\autocite{clibs}, which encompasses hundreds of C libraries, would test
scalability and robustness across diverse real-world settings. Such a
broad benchmark would also enable systematic comparisons against
state-of-the-art automated fuzzing frameworks like OSS-Fuzz-Gen and
AutoGen, elucidating OverHAuL's relative strengths and identifying areas
for improvement \autocite{oss-fuzz-gen,sun2024}.

Complementing broad benchmarking, detailed ablation studies dissecting
the contributions of individual pipeline components and algorithmic
choices will yield critical insights into what drives OverHAuL's
performance. Understanding the impact of each module will guide targeted
optimizations and support evidence-based design decisions.

Furthermore, an economic analysis exploring resource consumption---such
as API token usage and associated monetary costs---relative to fuzzing
effectiveness would be valuable for assessing the practical viability of
integrating LLM-based fuzz driver generation into continuous integration
processes.

\section{Practical Deployment and Community
Engagement}\label{practical-deployment-and-community-engagement}

From a usability perspective, embedding OverHAuL within a GitHub Actions
workflow represents a practical and impactful enhancement, enabling
seamless integration with developers' existing toolchains and continuous
integration pipelines. This would promote wider adoption by reducing
barriers to entry and fostering real-time feedback during code
development cycles.

Additionally, establishing a mechanism to generate and submit automated
pull requests (PRs) to the maintainers of fuzzed
libraries---highlighting detected bugs and proposing patches---would not
only validate OverHAuL's findings but also contribute tangible
improvements to open-source software quality. This collaborative
feedback loop epitomizes the symbiosis between automated testing tools
and the open-source community. As an initial step, developing targeted
PRs for the projects where bugs were discovered during OverHAuL's
development would help facilitate practical follow-up and improvements.

\bookmarksetup{startatroot}

\chapter{Discussion}\label{discussion}

more powerful llms -\textgreater{} better results

open source libraries might have been in the training data results for
closed source libraries could be worse this could be mitigated with llm
fine-tuning

\bookmarksetup{startatroot}

\chapter{Conclusion}\label{conclusion}

Recap

\bookmarksetup{startatroot}

\chapter*{Bibliography}\label{bibliography}
\addcontentsline{toc}{chapter}{Bibliography}

\markboth{Bibliography}{Bibliography}

\printbibliography[heading=none]





\end{document}
