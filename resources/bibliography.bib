@online{2024MagenticOne,
  title = {Magentic-{{One}}: {{A Generalist Multi-Agent System}} for {{Solving Complex Tasks}}},
  shorttitle = {Magentic-{{One}}},
  date = {2024-11-04},
  url = {https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/},
  abstract = {By Adam Fourney, Principal Researcher; Gagan Bansal, Senior Researcher; Hussein Mozannar, Senior Researcher; Victor Dibia, Principal Research Software Engineer; Saleema Amershi, Partner Research Manager Contributors: Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang (Eric) Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor […]},
  langid = {american},
  organization = {Microsoft Research},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-05-22T18:17:39.315Z},
  file = {/home/kchou/HDD/Library/References/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks.html}
}

@software{2025a,
  title = {Google/Clusterfuzz},
  date = {2025-04-09T13:01:59Z},
  origdate = {2019-01-29T00:19:40Z},
  url = {https://github.com/google/clusterfuzz},
  abstract = {Scalable fuzzing infrastructure.},
  organization = {Google},
  keywords = {fuzzing,repo,security,stability,vulnerabilities}
}

@software{2025pydantic,
  title = {Pydantic/Pydantic-Ai},
  date = {2025-05-26T18:39:53Z},
  origdate = {2024-06-21T15:55:04Z},
  url = {https://github.com/pydantic/pydantic-ai},
  abstract = {Agent Framework / shim to use Pydantic with LLMs},
  organization = {Pydantic},
  keywords = {agent-framework,llms,pydantic,python}
}

@online{afl,
  title = {American Fuzzy Lop},
  url = {https://lcamtuf.coredump.cx/afl/},
  keywords = {project/thesis},
  file = {/home/kchou/HDD/Library/References/afl.html}
}

@software{afl++,
  title = {{{AFL}}++},
  author = {Heuse, Marc and Eißfeldt, Heiko and Fioraldi, Andrea and Maier, Dominik},
  date = {2022-01},
  origdate = {2019-05-28T14:29:06Z},
  url = {https://github.com/AFLplusplus/AFLplusplus},
  abstract = {The fuzzer afl++ is afl with community patches, qemu 5.1 upgrade, collision-free coverage, enhanced laf-intel \& redqueen, AFLfast++ power schedules, MOpt mutators, unicorn\_mode, and a lot more!},
  version = {4.00c},
  keywords = {project/thesis,repo}
}

@inproceedings{afl++paper,
  title = {{{AFL}}++: {{Combining}} Incremental Steps of Fuzzing Research},
  booktitle = {14th {{USENIX}} Workshop on Offensive Technologies ({{WOOT}} 20)},
  author = {Fioraldi, Andrea and Maier, Dominik and Eißfeldt, Heiko and Heuse, Marc},
  date = {2020-08},
  publisher = {USENIX Association},
  keywords = {suggested},
  file = {/home/kchou/HDD/Library/References/Fioraldi et al. - 2020 - AFL++ Combining incremental steps of fuzzing research - @AFLplusplus-Woot20.pdf}
}

@inproceedings{anderson1993,
  title = {Why Cryptosystems Fail},
  booktitle = {Proceedings of the 1st {{ACM}} Conference on {{Computer}} and Communications Security  - {{CCS}} '93},
  author = {Anderson, Ross},
  date = {1993},
  pages = {215--227},
  publisher = {ACM Press},
  location = {Fairfax, Virginia, United States},
  doi = {10.1145/168588.168615},
  url = {http://portal.acm.org/citation.cfm?doid=168588.168615},
  eventtitle = {The 1st {{ACM}} Conference},
  isbn = {978-0-89791-629-5},
  langid = {english},
  file = {/home/kchou/HDD/Library/References/Anderson - 1993 - Why cryptosystems fail - @andersonWhyCryptosystemsFail1993.pdf}
}

@software{atheris,
  title = {Google/Atheris},
  date = {2025-04-09T10:53:48Z},
  origdate = {2020-11-16T22:43:28Z},
  url = {https://github.com/google/atheris},
  organization = {Google},
  keywords = {repo}
}

@online{chainofthought,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  date = {2023-01-10},
  eprint = {2201.11903},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,llm,prompting},
  file = {/home/kchou/HDD/Library/References/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models - wei2023.pdf}
}

@online{cheng2025,
  title = {Towards {{Reliable LLM-Driven Fuzz Testing}}: {{Vision}} and {{Road Ahead}}},
  shorttitle = {Towards {{Reliable LLM-Driven Fuzz Testing}}},
  author = {Cheng, Yiran and Kang, Hong Jin and Shar, Lwin Khin and Dong, Chaopeng and Shi, Zhiqiang and Lv, Shichao and Sun, Limin},
  date = {2025-03-02},
  eprint = {2503.00795},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.00795},
  url = {http://arxiv.org/abs/2503.00795},
  abstract = {Fuzz testing is a crucial component of software security assessment, yet its effectiveness heavily relies on valid fuzz drivers and diverse seed inputs. Recent advancements in Large Language Models (LLMs) offer transformative potential for automating fuzz testing (LLM4Fuzz), particularly in generating drivers and seeds. However, current LLM4Fuzz solutions face critical reliability challenges, including low driver validity rates and seed quality trade-offs, hindering their practical adoption. This paper aims to examine the reliability bottlenecks of LLM-driven fuzzing and explores potential research directions to address these limitations. It begins with an overview of the current development of LLM4SE and emphasizes the necessity for developing reliable LLM4Fuzz solutions. Following this, the paper envisions a vision where reliable LLM4Fuzz transforms the landscape of software testing and security for industry, software development practitioners, and economic accessibility. It then outlines a road ahead for future research, identifying key challenges and offering specific suggestions for the researchers to consider. This work strives to spark innovation in the field, positioning reliable LLM4Fuzz as a fundamental component of modern software testing.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering,fuzzing,llm,llm fuzzing},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-06T10:56:05.200Z},
  file = {/home/kchou/HDD/Library/References/Cheng et al. - 2025 - Towards Reliable LLM-Driven Fuzz Testing Vision and Road Ahead - @chengReliableLLMDrivenFuzz2025.pdf;/home/kchou/HDD/Library/References/2503.html}
}

@online{dspy,
  title = {{{DSPy}}: {{Compiling Declarative Language Model Calls}} into {{Self-Improving Pipelines}}},
  shorttitle = {{{DSPy}}},
  author = {Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  date = {2023-10-05},
  eprint = {2310.03714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.03714},
  url = {http://arxiv.org/abs/2310.03714},
  abstract = {The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25\% and 65\%, respectively) and pipelines with expert-created demonstrations (by up to 5-46\% and 16-40\%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/kchou/HDD/Library/References/Khattab et al. - 2023 - DSPy Compiling Declarative Language Model Calls into Self-Improving Pipelines - @khattabDSPyCompilingDeclarative2023.pdf;/home/kchou/HDD/Library/References/2310.html}
}

@inproceedings{fudge,
  title = {{{FUDGE}}: Fuzz Driver Generation at Scale},
  shorttitle = {{{FUDGE}}},
  booktitle = {Proceedings of the 2019 27th {{ACM Joint Meeting}} on {{European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  author = {Babić, Domagoj and Bucur, Stefan and Chen, Yaohui and Ivančić, Franjo and King, Tim and Kusano, Markus and Lemieux, Caroline and Szekeres, László and Wang, Wei},
  date = {2019-08-12},
  pages = {975--985},
  publisher = {ACM},
  location = {Tallinn Estonia},
  doi = {10.1145/3338906.3340456},
  url = {https://dl.acm.org/doi/10.1145/3338906.3340456},
  eventtitle = {{{ESEC}}/{{FSE}} '19: 27th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  isbn = {978-1-4503-5572-8},
  langid = {english},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-05-04T14:28:48.237Z},
  file = {/home/kchou/HDD/Library/Zotero data/storage/VMLR5CAA/Babić et al. - 2019 - FUDGE fuzz driver generation at scale - @babicFUDGEFuzzDriver2019.pdf}
}

@inproceedings{fuzzgen,
  title = {{{FuzzGen}}: {{Automatic}} Fuzzer Generation},
  shorttitle = {{{FuzzGen}}},
  booktitle = {29th {{USENIX Security Symposium}} ({{USENIX Security}} 20)},
  author = {Ispoglou, Kyriakos and Austin, Daniel and Mohan, Vishwath and Payer, Mathias},
  date = {2020},
  pages = {2271--2287},
  url = {https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou},
  abstract = {Fuzzing is a testing technique to discover unknown vulnerabilities in software. When applying fuzzing to libraries, the core idea of supplying random input remains unchanged, yet it is non-trivial to achieve good code coverage. Libraries cannot run as standalone programs, but instead are invoked through another application. Triggering code deep in a library remains challenging as specific sequences of API calls are required to build up the necessary state. Libraries are diverse and have unique interfaces that require unique fuzzers, so far written by a human analyst. To address this issue, we present FuzzGen, a tool for automatically synthesizing fuzzers for complex libraries in a given environment. FuzzGen leverages a whole system analysis to infer the library’s interface and synthesizes fuzzers specifically for that library. FuzzGen requires no human interaction and can be applied to a wide range of libraries. Furthermore, the generated fuzzers leverage LibFuzzer to achieve better code coverage and expose bugs that reside deep in the library. FuzzGen was evaluated on Debian and the Android Open Source Project (AOSP) selecting 7 libraries to generate fuzzers. So far, we have found 17 previously unpatched vulnerabilities with 6 assigned CVEs. The generated fuzzers achieve an average of 54.94\% code coverage; an improvement of 6.94\% when compared to manually written fuzzers, demonstrating the effectiveness and generality of FuzzGen.},
  keywords = {fuzzing},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-05-09T15:42:23.379Z},
  file = {/home/kchou/HDD/Library/References/Ispoglou et al. - 2020 - FuzzGen Automatic fuzzer generation - ispoglou2020.pdf}
}

@online{fuzzgpt,
  title = {Large {{Language Models}} Are {{Edge-Case Fuzzers}}: {{Testing Deep Learning Libraries}} via {{FuzzGPT}}},
  shorttitle = {Large {{Language Models}} Are {{Edge-Case Fuzzers}}},
  author = {Deng, Yinlin and Xia, Chunqiu Steven and Yang, Chenyuan and Zhang, Shizhuo Dylan and Yang, Shujing and Zhang, Lingming},
  date = {2023-04-04},
  eprint = {2304.02014},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.02014},
  url = {http://arxiv.org/abs/2304.02014},
  abstract = {Bugs in Deep Learning (DL) libraries may affect almost all downstream DL applications, and it is crucial to ensure the quality of such systems. It is challenging to generate valid input programs for fuzzing DL libraries, since the input programs need to satisfy both the syntax/semantics of the supported languages (e.g., Python) and the tensor/operator constraints for constructing valid computational graphs. Recently, the TitanFuzz work demonstrates that modern Large Language Models (LLMs) can be directly leveraged to implicitly learn all the language and DL computation constraints to generate valid programs for fuzzing DL libraries. However, LLMs tend to generate ordinary programs following similar patterns/tokens with typical programs seen in their massive training corpora (e.g., GitHub), while fuzzing favors unusual inputs that cover edge cases or are unlikely to be manually produced. To fill this gap, this paper proposes FuzzGPT, the first technique to prime LLMs to synthesize unusual programs for fuzzing. FuzzGPT is built on the well-known hypothesis that historical bug-triggering programs may include rare/valuable code ingredients important for bug finding. Meanwhile, while traditional techniques leveraging such historical information require intensive human efforts to both design dedicated generators and ensure the syntactic/semantic validity of generated programs, FuzzGPT demonstrates that this process can be fully automated via the intrinsic capabilities of LLMs (including fine-tuning and in-context learning), while being generalizable and applicable to challenging domains. While FuzzGPT can be applied with different LLMs, this paper focuses on the powerful GPT-style models: Codex and CodeGen. Moreover, FuzzGPT also shows the potential of directly leveraging the instruct-following capability of the recent ChatGPT for effective fuzzing. The experimental study on two popular DL libraries (PyTorch and TensorFlow) shows that FuzzGPT can substantially outperform TitanFuzz, detecting 76 bugs, with 49 already confirmed as previously unknown bugs, including 11 high-priority bugs or security vulnerabilities.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Software Engineering},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-05-08T13:33:14.524Z},
  file = {/home/kchou/HDD/Library/References/Deng et al. - 2023 - Large Language Models are Edge-Case Fuzzers Testing Deep Learning Libraries via FuzzGPT - deng2023.pdf}
}

@online{ganguly2024,
  title = {Proof of {{Thought}} : {{Neurosymbolic Program Synthesis}} Allows {{Robust}} and {{Interpretable Reasoning}}},
  shorttitle = {Proof of {{Thought}}},
  author = {Ganguly, Debargha and Iyengar, Srinivasan and Chaudhary, Vipin and Kalyanaraman, Shivkumar},
  date = {2024-09-25},
  eprint = {2409.17270},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.17270},
  url = {http://arxiv.org/abs/2409.17270},
  abstract = {Large Language Models (LLMs) have revolutionized natural language processing, yet they struggle with inconsistent reasoning, particularly in novel domains and complex logical sequences. This research introduces Proof of Thought, a framework that enhances the reliability and transparency of LLM outputs. Our approach bridges LLM-generated ideas with formal logic verification, employing a custom interpreter to convert LLM outputs into First Order Logic constructs for theorem prover scrutiny. Central to our method is an intermediary JSON-based Domain-Specific Language, which by design balances precise logical structures with intuitive human concepts. This hybrid representation enables both rigorous validation and accessible human comprehension of LLM reasoning processes. Key contributions include a robust type system with sort management for enhanced logical integrity, explicit representation of rules for clear distinction between factual and inferential knowledge, and a flexible architecture that allows for easy extension to various domain-specific applications. We demonstrate Proof of Thought's effectiveness through benchmarking on StrategyQA and a novel multimodal reasoning task, showing improved performance in open-ended scenarios. By providing verifiable and interpretable results, our technique addresses critical needs for AI system accountability and sets a foundation for human-in-the-loop oversight in high-stakes domains.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:07.288Z},
  file = {/home/kchou/HDD/Library/References/Ganguly et al. - 2024 - Proof of Thought  Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning - @gangulyProofThoughtNeurosymbolic2024.pdf;/home/kchou/HDD/Library/References/2409 1.html}
}

@inproceedings{gao2023,
  title = {Beyond the {{Coverage Plateau}}: {{A Comprehensive Study}} of {{Fuzz Blockers}} ({{Registered Report}})},
  shorttitle = {Beyond the {{Coverage Plateau}}},
  booktitle = {Proceedings of the 2nd {{International Fuzzing Workshop}}},
  author = {Gao, Wentao and Pham, Van-Thuan and Liu, Dongge and Chang, Oliver and Murray, Toby and Rubinstein, Benjamin I.P.},
  date = {2023-07-17},
  series = {{{FUZZING}} 2023},
  pages = {47--55},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3605157.3605177},
  url = {https://dl.acm.org/doi/10.1145/3605157.3605177},
  abstract = {Fuzzing and particularly code coverage-guided greybox fuzzing is highly successful in automated vulnerability discovery, as evidenced by the multitude of vulnerabilities uncovered in real-world software systems. However, results on large benchmarks such as FuzzBench indicate that the state-of-the-art fuzzers often reach a plateau after a certain period, typically around 12 hours. With the aid of the newly introduced FuzzIntrospector platform, this study aims to analyze and categorize the fuzz blockers that impede the progress of fuzzers. Such insights can shed light on future fuzzing research, suggesting areas that require further attention. Our preliminary findings reveal that the majority of top fuzz blockers are not directly related to the program input, emphasizing the need for enhanced techniques in automated fuzz driver generation and modification.},
  isbn = {979-8-4007-0247-1},
  keywords = {project/thesis},
  file = {/home/kchou/HDD/Library/References/Gao et al. - 2023 - Beyond the Coverage Plateau A Comprehensive Study of Fuzz Blockers (Registered Report) - @gaoCoveragePlateauComprehensive2023.pdf}
}

@online{gao2024,
  title = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Retrieval-{{Augmented Generation}} for {{Large Language Models}}},
  author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  date = {2024-03-27},
  eprint = {2312.10997},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.10997},
  url = {http://arxiv.org/abs/2312.10997},
  abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,RAG},
  file = {/home/kchou/HDD/Library/References/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey - @gaoRetrievalAugmentedGenerationLarge2024.pdf;/home/kchou/HDD/Library/References/2312 2.html}
}

@online{garcez2020,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {family=Garcez, given=Artur, prefix=d'Avila, useprefix=false and Lamb, Luis C.},
  date = {2020-12-16},
  eprint = {2012.05876},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2012.05876},
  url = {http://arxiv.org/abs/2012.05876},
  abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
  pubstate = {prepublished},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-02-24T20:57:10.231Z},
  file = {/home/kchou/HDD/Library/References/Garcez and Lamb - 2020 - Neurosymbolic AI The 3rd Wave - @garcezNeurosymbolicAI3rd2020.pdf;/home/kchou/HDD/Library/References/2012 1.html}
}

@online{gaur2023,
  title = {Building {{Trustworthy NeuroSymbolic AI Systems}}: {{Consistency}}, {{Reliability}}, {{Explainability}}, and {{Safety}}},
  shorttitle = {Building {{Trustworthy NeuroSymbolic AI Systems}}},
  author = {Gaur, Manas and Sheth, Amit},
  date = {2023-12-05},
  eprint = {2312.06798},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2312.06798},
  url = {http://arxiv.org/abs/2312.06798},
  abstract = {Explainability and Safety engender Trust. These require a model to exhibit consistency and reliability. To achieve these, it is necessary to use and analyze data and knowledge with statistical and symbolic AI methods relevant to the AI application - neither alone will do. Consequently, we argue and seek to demonstrate that the NeuroSymbolic AI approach is better suited for making AI a trusted AI system. We present the CREST framework that shows how Consistency, Reliability, user-level Explainability, and Safety are built on NeuroSymbolic methods that use data and knowledge to support requirements for critical applications such as health and well-being. This article focuses on Large Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs have garnered substantial attention from researchers due to their versatility in handling a broad array of natural language processing (NLP) scenarios. For example, ChatGPT and Google's MedPaLM have emerged as highly promising platforms for providing information in general and health-related queries, respectively. Nevertheless, these models remain black boxes despite incorporating human feedback and instruction-guided tuning. For instance, ChatGPT can generate unsafe responses despite instituting safety guardrails. CREST presents a plausible approach harnessing procedural and graph-based knowledge within a NeuroSymbolic framework to shed light on the challenges associated with LLMs.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:16.527Z},
  file = {/home/kchou/HDD/Library/References/Gaur and Sheth - 2023 - Building Trustworthy NeuroSymbolic AI Systems Consistency, Reliability, Explainability, and Safety - @gaurBuildingTrustworthyNeuroSymbolic2023.pdf;/home/kchou/HDD/Library/References/2312.html}
}

@article{gazzola2019Automatic,
  title = {Automatic {{Software Repair}}: {{A Survey}}},
  shorttitle = {Automatic {{Software Repair}}},
  author = {Gazzola, Luca and Micucci, Daniela and Mariani, Leonardo},
  date = {2019-01},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {45},
  number = {1},
  pages = {34--67},
  issn = {1939-3520},
  doi = {10.1109/TSE.2017.2755013},
  url = {https://ieeexplore.ieee.org/document/8089448/},
  abstract = {Despite their growing complexity and increasing size, modern software applications must satisfy strict release requirements that impose short bug fixing and maintenance cycles, putting significant pressure on developers who are responsible for timely producing high-quality software. To reduce developers workload, repairing and healing techniques have been extensively investigated as solutions for efficiently repairing and maintaining software in the last few years. In particular, repairing solutions have been able to automatically produce useful fixes for several classes of bugs that might be present in software programs. A range of algorithms, techniques, and heuristics have been integrated, experimented, and studied, producing a heterogeneous and articulated research framework where automatic repair techniques are proliferating. This paper organizes the knowledge in the area by surveying a body of 108 papers about automatic software repair techniques, illustrating the algorithms and the approaches, comparing them on representative examples, and discussing the open challenges and the empirical evidence reported so far.},
  keywords = {Automatic program repair,Computer bugs,Conferences,correct by construction,Debugging,Fault diagnosis,generate and validate,localization,Maintenance engineering,program synthesis,search-based,self-repairing,semantics-driven repair,Software,Software algorithms},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-04-30T09:46:00.498Z},
  file = {/home/kchou/HDD/Library/References/Gazzola et al. - 2019 - Automatic Software Repair A Survey - @gazzolaAutomaticSoftwareRepair2019.pdf}
}

@online{giannone2025react,
  title = {Demystifying {{AI Agents}}: {{ReAct-Style Agents}} vs {{Agentic Workflows}}},
  shorttitle = {Demystifying {{AI Agents}}},
  author = {Giannone, Dan},
  date = {2025-02-09T20:37:04},
  url = {https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471},
  abstract = {Understanding the current industry trends towards a “default” agent architecture and how to thoughtfully think through agent design.},
  langid = {english},
  organization = {Medium},
  keywords = {llm,llm-agents,reAct},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-22T07:06:25.712Z},
  file = {/home/kchou/HDD/Library/References/GiannoneD/2025/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471.html}
}

@online{grov2024,
  title = {On the Use of Neurosymbolic {{AI}} for Defending against Cyber Attacks},
  author = {Grov, Gudmund and Halvorsen, Jonas and Eckhoff, Magnus Wiik and Hansen, Bjørn Jervell and Eian, Martin and Mavroeidis, Vasileios},
  date = {2024-08-09},
  eprint = {2408.04996},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2408.04996},
  url = {http://arxiv.org/abs/2408.04996},
  abstract = {It is generally accepted that all cyber attacks cannot be prevented, creating a need for the ability to detect and respond to cyber attacks. Both connectionist and symbolic AI are currently being used to support such detection and response. In this paper, we make the case for combining them using neurosymbolic AI. We identify a set of challenges when using AI today and propose a set of neurosymbolic use cases we believe are both interesting research directions for the neurosymbolic AI community and can have an impact on the cyber security field. We demonstrate feasibility through two proof-of-concept experiments.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:20:10.404Z},
  file = {/home/kchou/HDD/Library/References/Grov et al. - 2024 - On the use of neurosymbolic AI for defending against cyber attacks - @grovUseNeurosymbolicAI2024.pdf;/home/kchou/HDD/Library/References/2408 2.html}
}

@online{heartbleed,
  title = {Heartbleed {{Bug}}},
  url = {https://heartbleed.com/},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-04T14:28:15.555Z},
  file = {/home/kchou/HDD/Library/References/heartbleed.com.html}
}

@inproceedings{herrera2021,
  title = {Seed Selection for Successful Fuzzing},
  booktitle = {Proceedings of the 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Herrera, Adrian and Gunadi, Hendra and Magrath, Shane and Norrish, Michael and Payer, Mathias and Hosking, Antony L.},
  date = {2021-07-11},
  pages = {230--243},
  publisher = {ACM},
  location = {Virtual Denmark},
  doi = {10.1145/3460319.3464795},
  url = {https://dl.acm.org/doi/10.1145/3460319.3464795},
  abstract = {Mutation-based greybox fuzzingÐunquestionably the most widelyused fuzzing techniqueÐrelies on a set of non-crashing seed inputs (a corpus) to bootstrap the bug-finding process. When evaluating a fuzzer, common approaches for constructing this corpus include: (i) using an empty file; (ii) using a single seed representative of the target’s input format; or (iii) collecting a large number of seeds (e.g., by crawling the Internet). Little thought is given to how this seed choice affects the fuzzing process, and there is no consensus on which approach is best (or even if a best approach exists).},
  eventtitle = {{{ISSTA}} '21: 30th {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  isbn = {978-1-4503-8459-9},
  langid = {english},
  file = {/home/kchou/HDD/Library/References/Herrera et al. - 2021 - Seed selection for successful fuzzing - @herreraSeedSelectionSuccessful2021.pdf}
}

@online{huang2024,
  title = {Large Language Models Based Fuzzing Techniques: A Survey},
  shorttitle = {Large Language Models Based Fuzzing Techniques},
  author = {Huang, Linghan and Zhao, Peizhou and Chen, Huaming and Ma, Lei},
  date = {2024},
  url = {https://arxiv.org/abs/2402.00350},
  pubstate = {prepublished},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-02-24T20:29:10.448Z},
  file = {/home/kchou/HDD/Library/References/Huang et al. - 2024 - Large language models based fuzzing techniques a survey - @huangLargeLanguageModels2024.pdf}
}

@online{iris,
  title = {{{IRIS}}: {{LLM-Assisted Static Analysis}} for {{Detecting Security Vulnerabilities}}},
  shorttitle = {{{IRIS}}},
  author = {Li, Ziyang and Dutta, Saikat and Naik, Mayur},
  date = {2025-04-06},
  eprint = {2405.17238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.17238},
  url = {http://arxiv.org/abs/2405.17238},
  abstract = {Software is prone to security vulnerabilities. Program analysis tools to detect them have limited effectiveness in practice due to their reliance on human labeled specifications. Large language models (or LLMs) have shown impressive code generation capabilities but they cannot do complex reasoning over code to detect such vulnerabilities especially since this task requires whole-repository analysis. We propose IRIS, a neuro-symbolic approach that systematically combines LLMs with static analysis to perform whole-repository reasoning for security vulnerability detection. Specifically, IRIS leverages LLMs to infer taint specifications and perform contextual analysis, alleviating needs for human specifications and inspection. For evaluation, we curate a new dataset, CWE-Bench-Java, comprising 120 manually validated security vulnerabilities in real-world Java projects. A state-of-the-art static analysis tool CodeQL detects only 27 of these vulnerabilities whereas IRIS with GPT-4 detects 55 (+28) and improves upon CodeQL's average false discovery rate by 5\% points. Furthermore, IRIS identifies 4 previously unknown vulnerabilities which cannot be found by existing tools. IRIS is available publicly at https://github.com/iris-sast/iris.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Programming Languages,Computer Science - Software Engineering,llm fuzzing},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-05-05T11:45:47.764Z},
  file = {/home/kchou/HDD/Library/References/Li et al. - 2025 - IRIS LLM-Assisted Static Analysis for Detecting Security Vulnerabilities - @liIRISLLMAssistedStatic2025.pdf;/home/kchou/HDD/Library/References/2405 2.html}
}

@online{jensen2024,
  title = {Software {{Vulnerability}} and {{Functionality Assessment}} Using {{LLMs}}},
  author = {Jensen, Rasmus Ingemann Tuffveson and Tawosi, Vali and Alamir, Salwa},
  date = {2024-03-13},
  eprint = {2403.08429},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.08429},
  url = {http://arxiv.org/abs/2403.08429},
  abstract = {While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7\% of LLM-generated descriptions can be associated with true CWE vulnerabilities.},
  pubstate = {prepublished},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-04T15:42:14.215Z},
  file = {/home/kchou/HDD/Library/References/Jensen et al. - 2024 - Software Vulnerability and Functionality Assessment using LLMs - @jensenSoftwareVulnerabilityFunctionality2024.pdf;/home/kchou/HDD/Library/References/Jensen et al. - 2024 - Software Vulnerability and Functionality Assessmen.html}
}

@online{kim2024,
  title = {Codexity: {{Secure AI-assisted Code Generation}}},
  shorttitle = {Codexity},
  author = {Kim, Sung Yong and Fan, Zhiyu and Noller, Yannic and Roychoudhury, Abhik},
  date = {2024-05-07},
  eprint = {2405.03927},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.03927},
  url = {http://arxiv.org/abs/2405.03927},
  abstract = {Despite the impressive performance of Large Language Models (LLMs) in software development activities, recent studies show the concern of introducing vulnerabilities into software codebase by AI programming assistants (e.g., Copilot, CodeWhisperer). In this work, we present Codexity, a security-focused code generation framework integrated with five LLMs. Codexity leverages the feedback of static analysis tools such as Infer and CppCheck to mitigate security vulnerabilities in LLM-generated programs. Our evaluation in a real-world benchmark with 751 automatically generated vulnerable subjects demonstrates Codexity can prevent 60\% of the vulnerabilities being exposed to the software developer.},
  pubstate = {prepublished},
  version = {1},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-05-04T14:32:41.923Z},
  file = {/home/kchou/HDD/Library/References/Kim et al. - 2024 - Codexity Secure AI-assisted Code Generation - @kimCodexitySecureAIassisted2024.pdf;/home/kchou/HDD/Library/References/2405 1.html}
}

@inproceedings{klee,
  title = {{{KLEE}}: {{Unassisted}} and {{Automatic Generation}} of {{High-Coverage Tests}} for {{Complex Systems Programs}}},
  shorttitle = {{{KLEE}}},
  author = {Cadar, Cristian and Dunbar, Daniel and Engler, D.},
  date = {2008-12-08},
  url = {https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8},
  abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.    We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  eventtitle = {{{USENIX Symposium}} on {{Operating Systems Design}} and {{Implementation}}},
  keywords = {suggested},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-03-19T10:38:46.368Z},
  file = {/home/kchou/HDD/Library/References/Cadar et al. - 2008 - KLEE Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs - @cadarKLEEUnassistedAutomatic2008.pdf}
}

@online{laban2025,
  title = {{{LLMs Get Lost In Multi-Turn Conversation}}},
  author = {Laban, Philippe and Hayashi, Hiroaki and Zhou, Yingbo and Neville, Jennifer},
  date = {2025-05-09},
  eprint = {2505.06120},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2505.06120},
  url = {http://arxiv.org/abs/2505.06120},
  abstract = {Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39\% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,suggested,with-notes},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-16T16:26:54.456Z},
  file = {/home/kchou/HDD/Library/References/Laban et al. - 2025 - LLMs Get Lost In Multi-Turn Conversation - laban2025.pdf;/home/kchou/HDD/Library/References/2505.html}
}

@software{langchain,
  title = {{{LangChain}}},
  author = {Chase, Harrison},
  date = {2022-10},
  url = {https://github.com/langchain-ai/langchain}
}

@software{langgraph,
  title = {Langchain-Ai/Langgraph},
  date = {2025-05-21T17:09:48Z},
  origdate = {2023-08-09T18:33:12Z},
  url = {https://github.com/langchain-ai/langgraph},
  abstract = {Build resilient language agents as graphs.},
  organization = {LangChain}
}

@inproceedings{lazar2014,
  title = {Why Does Cryptographic Software Fail? A Case Study and Open Problems},
  shorttitle = {Why Does Cryptographic Software Fail?},
  booktitle = {Proceedings of 5th {{Asia-Pacific Workshop}} on {{Systems}}},
  author = {Lazar, David and Chen, Haogang and Wang, Xi and Zeldovich, Nickolai},
  date = {2014-06-25},
  series = {{{APSys}} '14},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2637166.2637237},
  url = {https://doi.org/10.1145/2637166.2637237},
  abstract = {Mistakes in cryptographic software implementations often undermine the strong security guarantees offered by cryptography. This paper presents a systematic study of cryptographic vulnerabilities in practice, an examination of state-of-the-art techniques to prevent such vulnerabilities, and a discussion of open problems and possible future research directions. Our study covers 269 cryptographic vulnerabilities reported in the CVE database from January 2011 to May 2014. The results show that just 17\% of the bugs are in cryptographic libraries (which often have devastating consequences), and the remaining 83\% are misuses of cryptographic libraries by individual applications. We observe that preventing bugs in different parts of a system requires different techniques, and that no effective techniques exist to deal with certain classes of mistakes, such as weak key generation.},
  isbn = {978-1-4503-3024-4},
  keywords = {suggested},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-10-12T16:34:56.082Z},
  file = {/home/kchou/HDD/Library/References/Lazar et al. - 2014 - Why does cryptographic software fail a case study and open problems - @lazarWhyDoesCryptographic2014.pdf}
}

@article{lee2025,
  title = {The {{Impact}} of {{Generative AI}} on {{Critical Thinking}}: {{Self-Reported Reductions}} in {{Cognitive Effort}} and {{Confidence Effects From}} a {{Survey}} of {{Knowledge Workers}}},
  shorttitle = {The {{Impact}} of {{Generative AI}} on {{Critical Thinking}}},
  author = {Lee, Hao-Ping Hank and Sarkar, Advait and Tankelevitch, Lev and Drosos, Ian and Rintel, Sean and Banks, Richard and Wilson, Nicholas},
  date = {2025},
  url = {https://hankhplee.com/papers/genai_critical_thinking.pdf},
  keywords = {cognition,LLM,project/thesis,study},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-04-07T08:11:49.006Z},
  file = {/home/kchou/HDD/Library/References/Lee et al. - 2025 - The Impact of Generative AI on Critical Thinking Self-Reported Reductions in Cognitive Effort and C - @leeImpactGenerativeAI2025.pdf}
}

@article{legoues2013Repair,
  title = {Current Challenges in Automatic Software Repair},
  author = {Le Goues, Claire and Forrest, Stephanie and Weimer, Westley},
  date = {2013-09},
  journaltitle = {Software Quality Journal},
  shortjournal = {Software Qual J},
  volume = {21},
  number = {3},
  pages = {421--443},
  issn = {0963-9314, 1573-1367},
  doi = {10.1007/s11219-013-9208-0},
  url = {http://link.springer.com/10.1007/s11219-013-9208-0},
  langid = {english},
  keywords = {localization},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-04-30T09:46:00.505Z},
  file = {/home/kchou/HDD/Library/References/Le Goues et al. - 2013 - Current challenges in automatic software repair - @.pdf}
}

@article{li2022,
  title = {Language Models: Past, Present, and Future},
  shorttitle = {Language Models},
  author = {Li, Hang},
  date = {2022-06-21},
  journaltitle = {Commun. ACM},
  volume = {65},
  number = {7},
  pages = {56--63},
  issn = {0001-0782},
  doi = {10.1145/3490443},
  url = {https://dl.acm.org/doi/10.1145/3490443},
  abstract = {A language modeling overview, highlighting basic concepts, intuitive explanations, technical achievements, and fundamental challenges.},
  file = {/home/kchou/HDD/Library/References/Li - 2022 - Language models past, present, and future - @liLanguageModelsPresent2022.pdf}
}

@online{libfuzzer,
  title = {{{libFuzzer}} – a Library for Coverage-Guided Fuzz Testing. — {{LLVM}} 21.0.0git Documentation},
  url = {https://llvm.org/docs/LibFuzzer.html},
  keywords = {project/thesis},
  file = {/home/kchou/HDD/Library/References/LibFuzzer.html}
}

@article{licklider1960,
  title = {Man-{{Computer Symbiosis}}},
  author = {Licklider, J. C. R.},
  date = {1960-03},
  journaltitle = {IRE Transactions on Human Factors in Electronics},
  volume = {HFE-1},
  number = {1},
  pages = {4--11},
  issn = {2168-2836},
  doi = {10.1109/THFE2.1960.4503259},
  url = {https://ieeexplore.ieee.org/document/4503259},
  abstract = {Man-computer symbiosis is an expected development in cooperative interaction between men and electronic computers. It will involve very close coupling between the human and the electronic members of the partnership. The main aims are 1) to let computers facilitate formulative thinking as they now facilitate the solution of formulated problems, and 2) to enable men and computers to cooperate in making decisions and controlling complex situations without inflexible dependence on predetermined programs. In the anticipated symbiotic partnership, men will set the goals, formulate the hypotheses, determine the criteria, and perform the evaluations. Computing machines will do the routinizable work that must be done to prepare the way for insights and decisions in technical and scientific thinking. Preliminary analyses indicate that the symbiotic partnership will perform intellectual operations much more effectively than man alone can perform them. Prerequisites for the achievement of the effective, cooperative association include developments in computer time sharing, in memory components, in memory organization, in programming languages, and in input and output equipment.},
  eventtitle = {{{IRE Transactions}} on {{Human Factors}} in {{Electronics}}},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-04-02T08:50:08.357Z},
  file = {/home/kchou/HDD/Library/References/Licklider - 1960 - Man-Computer Symbiosis - @lickliderManComputerSymbiosis1960.pdf;/home/kchou/HDD/Library/References/4503259.html}
}

@online{liu2025,
  title = {Can {{LLM Generate Regression Tests}} for {{Software Commits}}?},
  author = {Liu, Jing and Lee, Seongmin and Losiouk, Eleonora and Böhme, Marcel},
  date = {2025-01-19},
  eprint = {2501.11086},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.11086},
  url = {http://arxiv.org/abs/2501.11086},
  abstract = {Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regression test generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regression test generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars: \$\textbackslash bullet\$ Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied. \$\textbackslash bullet\$ Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regression test suite to catch similar bugs in the future. We implement Cleverest, a feedback-directed, zero-shot LLM-based regression test generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering,llm fuzzing},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-05-05T11:45:47.763Z},
  file = {/home/kchou/HDD/Library/References/Liu et al. - 2025 - Can LLM Generate Regression Tests for Software Commits - @liuCanLLMGenerate2025.pdf;/home/kchou/HDD/Library/References/2501 3.html}
}

@software{llamaindex,
  title = {{{LlamaIndex}}},
  author = {Liu, Jerry},
  date = {2022-11},
  doi = {10.5281/zenodo.1234},
  url = {https://github.com/jerryjliu/llama_index},
  abstract = {LlamaIndex is the leading framework for building LLM-powered agents over your data.}
}

@online{lyu2024,
  title = {Prompt {{Fuzzing}} for {{Fuzz Driver Generation}}},
  author = {Lyu, Yunlong and Xie, Yuxuan and Chen, Peng and Chen, Hao},
  date = {2024-05-29},
  eprint = {2312.17677},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.17677},
  url = {http://arxiv.org/abs/2312.17677},
  abstract = {Crafting high-quality fuzz drivers not only is time-consuming but also requires a deep understanding of the library. However, the state-of-the-art automatic fuzz driver generation techniques fall short of expectations. While fuzz drivers derived from consumer code can reach deep states, they have limited coverage. Conversely, interpretative fuzzing can explore most API calls but requires numerous attempts within a large search space. We propose PromptFuzz, a coverage-guided fuzzer for prompt fuzzing that iteratively generates fuzz drivers to explore undiscovered library code. To explore API usage in fuzz drivers during prompt fuzzing, we propose several key techniques: instructive program generation, erroneous program validation, coverage-guided prompt mutation, and constrained fuzzer scheduling. We implemented PromptFuzz and evaluated it on 14 real-world libraries. Compared with OSS-Fuzz and Hopper (the state-of-the-art fuzz driver generation tool), fuzz drivers generated by PromptFuzz achieved 1.61 and 1.63 times higher branch coverage than those by OSS-Fuzz and Hopper, respectively. Moreover, the fuzz drivers generated by PromptFuzz detected 33 genuine, new bugs out of a total of 49 crashes, out of which 30 bugs have been confirmed by their respective communities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Software Engineering},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-05-06T12:49:04.257Z},
  file = {/home/kchou/HDD/Library/References/Lyu et al. - 2024 - Prompt Fuzzing for Fuzz Driver Generation - lyuPromptFuzzingFuzz2024.pdf;/home/kchou/HDD/Library/References/2312 1 1.html}
}

@online{manes2019,
  title = {The {{Art}}, {{Science}}, and {{Engineering}} of {{Fuzzing}}: {{A Survey}}},
  shorttitle = {The {{Art}}, {{Science}}, and {{Engineering}} of {{Fuzzing}}},
  author = {Manes, Valentin J. M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
  date = {2019-04-07},
  eprint = {1812.00140},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.00140},
  url = {http://arxiv.org/abs/1812.00140},
  abstract = {Among the many software vulnerability discovery techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.},
  pubstate = {prepublished},
  keywords = {suggested},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2024-10-19T09:03:55.881Z},
  file = {/home/kchou/HDD/Library/References/Manes et al. - 2019 - The Art, Science, and Engineering of Fuzzing A Survey - @manesArtScienceEngineering2019.pdf;/home/kchou/HDD/Library/References/1812.html}
}

@online{meyer2013,
  title = {Lessons {{Learned From Previous SSL}}/{{TLS Attacks}} - {{A Brief Chronology Of Attacks And Weaknesses}}},
  author = {Meyer, Christopher and Schwenk, Jörg},
  date = {2013},
  number = {2013/049},
  url = {https://eprint.iacr.org/2013/049},
  abstract = {Since its introduction in 1994 the Secure Socket Layer (SSL) protocol (later renamed to Transport Layer Security (TLS)) evolved to the de facto standard for securing the transport layer. SSL/TLS can be used for ensuring data confidentiality, integrity and authenticity during transport. A main feature of the protocol is its flexibility. Modes of operation and security aims can easily be configured through different cipher suites. During its evolutionary development process several flaws were found. However, the flexible architecture of SSL/TLS allowed efficient fixes in order to counter the issues. This paper presents an overview on theoretical and practical attacks of the last 15 years, in chronological order and four categories: Attacks on the TLS Handshake protocol, on the TLS Record and Application Data Protocols, on the PKI infrastructure of TLS, and on various other attacks. We try to give a short ”Lessons Learned” at the end of each paragraph.},
  pubstate = {prepublished},
  keywords = {suggested},
  annotation = {Publication info: Published elsewhere. SSL, TLS, Handshake Protocol, Record Layer, Public Key Infrastructures, Bleichenbacher Attack, Padding Oracles\\
Read\_Status: Read\\
Read\_Status\_Date: 2024-12-20T14:54:14.658Z},
  file = {/home/kchou/HDD/Library/References/Meyer and Schwenk - 2013 - Lessons Learned From Previous SSLTLS Attacks - A Brief Chronology Of Attacks And Weaknesses - @meyerLessonsLearnedPrevious2013.pdf}
}

@article{nethercote2007,
  title = {Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation},
  shorttitle = {Valgrind},
  author = {Nethercote, Nicholas and Seward, Julian},
  date = {2007-06-10},
  journaltitle = {SIGPLAN Not.},
  volume = {42},
  number = {6},
  pages = {89--100},
  issn = {0362-1340},
  doi = {10.1145/1273442.1250746},
  url = {https://doi.org/10.1145/1273442.1250746},
  abstract = {Dynamic binary instrumentation (DBI) frameworks make it easy to build dynamic binary analysis (DBA) tools such as checkers and profilers. Much of the focus on DBI frameworks has been on performance; little attention has been paid to their capabilities. As a result, we believe the potential of DBI has not been fully exploited.In this paper we describe Valgrind, a DBI framework designed for building heavyweight DBA tools. We focus on its unique support for shadow values-a powerful but previously little-studied and difficult-to-implement DBA technique, which requires a tool to shadow every register and memory value with another value that describes it. This support accounts for several crucial design features that distinguish Valgrind from other DBI frameworks. Because of these features, lightweight tools built with Valgrind run comparatively slowly, but Valgrind can be used to build more interesting, heavyweight tools that are difficult or impossible to build with other DBI frameworks such as Pin and DynamoRIO.},
  keywords = {suggested},
  file = {/home/kchou/HDD/Library/References/Nethercote and Seward - 2007 - Valgrind a framework for heavyweight dynamic binary instrumentation - @nethercoteValgrindFrameworkHeavyweight2007.pdf}
}

@software{oss-fuzz,
  title = {{{OSS-Fuzz}}},
  author = {Arya, Abhishek and Chang, Oliver and Metzman, Jonathan and Serebryany, Kostya and Liu, Dongge},
  date = {2025-04-08T14:23:14Z},
  origdate = {2016-07-20T19:39:50Z},
  url = {https://github.com/google/oss-fuzz},
  abstract = {OSS-Fuzz - continuous fuzzing for open source software.},
  keywords = {project/thesis,repo}
}

@software{oss-fuzz-gen,
  title = {{{OSS-fuzz-gen}}: {{Automated}} Fuzz Target Generation},
  author = {Liu, Dongge and Chang, Oliver and {metzman}, Jonathan and Sablotny, Martin and Maruseac, Mihai},
  date = {2024-05},
  url = {https://github.com/google/oss-fuzz-gen},
  version = {https://github.com/google/oss-fuzz-gen/tree/v1.0},
  keywords = {project/thesis,repo}
}

@online{oss-fuzzmaintainers2024,
  title = {Introducing {{LLM-based}} Harness Synthesis for Unfuzzed Projects},
  author = {{OSS-Fuzz Maintainers}},
  date = {2024-05-27T00:00:00+00:00},
  url = {https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/},
  abstract = {The primary goal of our efforts are to take as input a GitHub repository and output an OSS-Fuzz project as well as a ClusterFuzzLite project with a meaningful fuzz harness. In this blog post we will describe how we automatically build projects, how we generate fuzzing harnesses using LLMs, how these are evaluated and list a selection of 15 projects that we generated OSS-Fuzz/ClusterFuzzLite integrations for and have upstreamed the results. Introducing LLM-based harness generation for unfuzzed projects.},
  langid = {english},
  organization = {OSS-Fuzz blog},
  keywords = {important},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-04-01T09:51:26.317Z},
  file = {/home/kchou/HDD/Library/References/index.html}
}

@online{perry2023,
  title = {Do {{Users Write More Insecure Code}} with {{AI Assistants}}?},
  author = {Perry, Neil and Srivastava, Megha and Kumar, Deepak and Boneh, Dan},
  date = {2023-12-18},
  eprint = {2211.03622},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2211.03622},
  url = {http://arxiv.org/abs/2211.03622},
  abstract = {We conduct the first large-scale user study examining how users interact with an AI Code assistant to solve a variety of security related tasks across different programming languages. Overall, we find that participants who had access to an AI assistant based on OpenAI's codex-davinci-002 model wrote significantly less secure code than those without access. Additionally, participants with access to an AI assistant were more likely to believe they wrote secure code than those without access to the AI assistant. Furthermore, we find that participants who trusted the AI less and engaged more with the language and format of their prompts (e.g. re-phrasing, adjusting temperature) provided code with fewer security vulnerabilities. Finally, in order to better inform the design of future AI-based Code assistants, we provide an in-depth analysis of participants' language and interaction behavior, as well as release our user interface as an instrument to conduct similar studies in the future.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-04-02T09:03:10.656Z},
  file = {/home/kchou/HDD/Library/References/Perry et al. - 2023 - Do Users Write More Insecure Code with AI Assistants - @perryUsersWriteMore2023.pdf;/home/kchou/HDD/Library/References/2211.html}
}

@online{prophetfuzz,
  title = {{{ProphetFuzz}}: {{Fully Automated Prediction}} and {{Fuzzing}} of {{High-Risk Option Combinations}} with {{Only Documentation}} via {{Large Language Model}}},
  shorttitle = {{{ProphetFuzz}}},
  author = {Wang, Dawei and Zhou, Geng and Chen, Li and Li, Dan and Miao, Yukai},
  date = {2024-09-01},
  eprint = {2409.00922},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.1145/3658644.3690231},
  url = {http://arxiv.org/abs/2409.00922},
  abstract = {Vulnerabilities related to option combinations pose a significant challenge in software security testing due to their vast search space. Previous research primarily addressed this challenge through mutation or filtering techniques, which inefficiently treated all option combinations as having equal potential for vulnerabilities, thus wasting considerable time on non-vulnerable targets and resulting in low testing efficiency. In this paper, we utilize carefully designed prompt engineering to drive the large language model (LLM) to predict high-risk option combinations (i.e., more likely to contain vulnerabilities) and perform fuzz testing automatically without human intervention. We developed a tool called ProphetFuzz and evaluated it on a dataset comprising 52 programs collected from three related studies. The entire experiment consumed 10.44 CPU years. ProphetFuzz successfully predicted 1748 high-risk option combinations at an average cost of only \textbackslash\$8.69 per program. Results show that after 72 hours of fuzzing, ProphetFuzz discovered 364 unique vulnerabilities associated with 12.30\textbackslash\% of the predicted high-risk option combinations, which was 32.85\textbackslash\% higher than that found by state-of-the-art in the same timeframe. Additionally, using ProphetFuzz, we conducted persistent fuzzing on the latest versions of these programs, uncovering 140 vulnerabilities, with 93 confirmed by developers and 21 awarded CVE numbers.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-19T08:53:01.845Z},
  file = {/home/kchou/HDD/Library/References/Wang et al. - 2024 - ProphetFuzz Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Docum - @wangProphetFuzzFullyAutomated2024.pdf;/home/kchou/HDD/Library/References/2409.html}
}

@online{reAct,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  date = {2023-03-10},
  eprint = {2210.03629},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  url = {http://arxiv.org/abs/2210.03629},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-21T16:44:45.166Z},
  file = {/home/kchou/HDD/Library/References/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models - @yaoReActSynergizingReasoning2023.pdf;/home/kchou/HDD/Library/References/2210.html}
}

@article{sasirekha2011Slicing,
  title = {Program {{Slicing Techniques}} and Its {{Applications}}},
  author = {Sasirekha, N and Edwin Robert, A and Hemalatha, M},
  date = {2011-07-31},
  journaltitle = {International Journal of Software Engineering \& Applications},
  shortjournal = {IJSEA},
  volume = {2},
  number = {3},
  pages = {50--64},
  issn = {09762221},
  doi = {10.5121/ijsea.2011.2304},
  url = {http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf},
  file = {/home/kchou/HDD/Library/References/Sasirekha et al. - 2011 - Program Slicing Techniques and its Applications - sasirekha2011Program.pdf}
}

@online{sheth2023,
  title = {Neurosymbolic {{AI}} -- {{Why}}, {{What}}, and {{How}}},
  author = {Sheth, Amit and Roy, Kaushik and Gaur, Manas},
  date = {2023-05-01},
  eprint = {2305.00813},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.00813},
  url = {http://arxiv.org/abs/2305.00813},
  abstract = {Humans interact with the environment using a combination of perception - transforming sensory inputs from their environment into symbols, and cognition - mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Human perception-inspired machine perception, in the context of AI, refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition. On the other hand, machine cognition encompasses more complex computations, such as using knowledge of the environment to guide reasoning, analogy, and long-term planning. Humans can also control and explain their cognitive functions. This seems to require the retention of symbolic mappings from perception outputs to knowledge about their environment. For example, humans can follow and explain the guidelines and safety constraints driving their decision-making in safety-critical applications such as healthcare, criminal justice, and autonomous driving. This article introduces the rapidly emerging paradigm of Neurosymbolic AI combines neural networks and knowledge-guided symbolic approaches to create more capable and flexible AI systems. These systems have immense potential to advance both algorithm-level (e.g., abstraction, analogy, reasoning) and application-level (e.g., explainable and safety-constrained decision-making) capabilities of AI systems.},
  pubstate = {prepublished},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-02-19T16:36:56.658Z},
  file = {/home/kchou/HDD/Library/References/Sheth et al. - 2023 - Neurosymbolic AI -- Why, What, and How - @shethNeurosymbolicAIWhy2023.pdf;/home/kchou/HDD/Library/References/2305.html}
}

@article{simonite2020mayhem,
  entrysubtype = {magazine},
  title = {This {{Bot Hunts Software Bugs}} for the {{Pentagon}}},
  author = {Simonite, Tom},
  date = {2020-06-01},
  journaltitle = {Wired},
  issn = {1059-1028},
  url = {https://www.wired.com/story/bot-hunts-software-bugs-pentagon/},
  abstract = {Mayhem emerged from a 2016 government-sponsored contest at a Las Vegas casino hotel. Now it's used by the military.},
  langid = {american},
  keywords = {artificial intelligence,cybersecurity,darpa,malware,vulnerabilities},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-04T14:28:05.460Z}
}

@online{tilwani2024,
  title = {Neurosymbolic {{AI}} Approach to {{Attribution}} in {{Large Language Models}}},
  author = {Tilwani, Deepa and Venkataramanan, Revathy and Sheth, Amit P.},
  date = {2024-09-30},
  eprint = {2410.03726},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.03726},
  url = {http://arxiv.org/abs/2410.03726},
  abstract = {Attribution in large language models (LLMs) remains a significant challenge, particularly in ensuring the factual accuracy and reliability of the generated outputs. Current methods for citation or attribution, such as those employed by tools like Perplexity.ai and Bing Search-integrated LLMs, attempt to ground responses by providing real-time search results and citations. However, so far, these approaches suffer from issues such as hallucinations, biases, surface-level relevance matching, and the complexity of managing vast, unfiltered knowledge sources. While tools like Perplexity.ai dynamically integrate web-based information and citations, they often rely on inconsistent sources such as blog posts or unreliable sources, which limits their overall reliability. We present that these challenges can be mitigated by integrating Neurosymbolic AI (NesyAI), which combines the strengths of neural networks with structured symbolic reasoning. NesyAI offers transparent, interpretable, and dynamic reasoning processes, addressing the limitations of current attribution methods by incorporating structured symbolic knowledge with flexible, neural-based learning. This paper explores how NesyAI frameworks can enhance existing attribution models, offering more reliable, interpretable, and adaptable systems for LLMs.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-28T22:19:58.738Z},
  file = {/home/kchou/HDD/Library/References/Tilwani et al. - 2024 - Neurosymbolic AI approach to Attribution in Large Language Models - @tilwaniNeurosymbolicAIApproach2024.pdf;/home/kchou/HDD/Library/References/2410 1.html}
}

@inproceedings{titanfuzz,
  title = {Large {{Language Models Are Zero-Shot Fuzzers}}: {{Fuzzing Deep-Learning Libraries}} via {{Large Language Models}}},
  shorttitle = {Large {{Language Models Are Zero-Shot Fuzzers}}},
  booktitle = {Proceedings of the 32nd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Deng, Yinlin and Xia, Chunqiu Steven and Peng, Haoran and Yang, Chenyuan and Zhang, Lingming},
  date = {2023-07-13},
  series = {{{ISSTA}} 2023},
  pages = {423--435},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3597926.3598067},
  url = {https://dl.acm.org/doi/10.1145/3597926.3598067},
  abstract = {Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.   To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38\%/50.84\% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.   This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.},
  isbn = {979-8-4007-0221-1},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-05-06T12:49:07.180Z},
  file = {/home/kchou/HDD/Library/References/Deng et al. - 2023 - Large Language Models Are Zero-Shot Fuzzers Fuzzing Deep-Learning Libraries via Large Language Mode - @dengLargeLanguageModels2023.pdf}
}

@online{tzachristas2024,
  title = {Creating an {{LLM-based AI-agent}}: {{A}} High-Level Methodology towards Enhancing {{LLMs}} with {{APIs}}},
  shorttitle = {Creating an {{LLM-based AI-agent}}},
  author = {Tzachristas, Ioannis},
  date = {2024-12-21},
  eprint = {2412.13233},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.13233},
  url = {http://arxiv.org/abs/2412.13233},
  abstract = {Large Language Models (LLMs) have revolutionized various aspects of engineering and science. Their utility is often bottlenecked by the lack of interaction with the external digital environment. To overcome this limitation and achieve integration of LLMs and Artificial Intelligence (AI) into real-world applications, customized AI agents are being constructed. Based on the technological trends and techniques, we extract a high-level approach for constructing these AI agents, focusing on their underlying architecture. This thesis serves as a comprehensive guide that elucidates a multi-faceted approach for empowering LLMs with the capability to leverage Application Programming Interfaces (APIs). We present a 7-step methodology that begins with the selection of suitable LLMs and the task decomposition that is necessary for complex problem-solving. This methodology includes techniques for generating training data for API interactions and heuristics for selecting the appropriate API among a plethora of options. These steps eventually lead to the generation of API calls that are both syntactically and semantically aligned with the LLM's understanding of a given task. Moreover, we review existing frameworks and tools that facilitate these processes and highlight the gaps in current attempts. In this direction, we propose an on-device architecture that aims to exploit the functionality of carry-on devices by using small models from the Hugging Face community. We examine the effectiveness of these approaches on real-world applications of various domains, including the generation of a piano sheet. Through an extensive analysis of the literature and available technologies, this thesis aims to set a compass for researchers and practitioners to harness the full potential of LLMs augmented with external tool capabilities, thus paving the way for more autonomous, robust, and context-aware AI agents.},
  pubstate = {prepublished},
  version = {2},
  keywords = {suggested},
  annotation = {Read\_Status: Not Reading\\
Read\_Status\_Date: 2025-05-04T14:30:45.262Z},
  file = {/home/kchou/HDD/Library/References/Tzachristas - 2024 - Creating an LLM-based AI-agent A high-level methodology towards enhancing LLMs with APIs - @tzachristasCreatingLLMbasedAIagent2024.pdf;/home/kchou/HDD/Library/References/2412.html}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-02-21T21:12:48.910Z},
  file = {/home/kchou/HDD/Library/References/Vaswani et al_2023_Attention Is All You Need.pdf;/home/kchou/HDD/Library/Zotero data/storage/VWIT3A5A/1706.html}
}

@online{yao2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  date = {2023-12-03},
  eprint = {2305.10601},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.10601},
  url = {http://arxiv.org/abs/2305.10601},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  pubstate = {prepublished},
  keywords = {suggested},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2024-10-15T11:27:12.812Z},
  file = {/home/kchou/HDD/Library/References/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with Large Language Models - @yaoTreeThoughtsDeliberate2023.pdf;/home/kchou/HDD/Library/References/2305 1.html}
}

@online{zebaze2024,
  title = {Tree of {{Problems}}: {{Improving}} Structured Problem Solving with Compositionality},
  shorttitle = {Tree of {{Problems}}},
  author = {Zebaze, Armel and Sagot, Benoît and Bawden, Rachel},
  date = {2024-10-09},
  eprint = {2410.06634},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.06634},
  url = {http://arxiv.org/abs/2410.06634},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable performance across multiple tasks through in-context learning. For complex reasoning tasks that require step-by-step thinking, Chain-of-Thought (CoT) prompting has given impressive results, especially when combined with self-consistency. Nonetheless, some tasks remain particularly difficult for LLMs to solve. Tree of Thoughts (ToT) and Graph of Thoughts (GoT) emerged as alternatives, dividing the complex problem into paths of subproblems. In this paper, we propose Tree of Problems (ToP), a simpler version of ToT, which we hypothesise can work better for complex tasks that can be divided into identical subtasks. Our empirical results show that our approach outperforms ToT and GoT, and in addition performs better than CoT on complex reasoning tasks. All code for this paper is publicly available here: https://github.com/ArmelRandy/tree-of-problems.},
  pubstate = {prepublished},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2024-10-19T08:55:10.104Z},
  file = {/home/kchou/HDD/Library/References/Zebaze et al. - 2024 - Tree of Problems Improving structured problem solving with compositionality - @zebazeTreeProblemsImproving2024.pdf}
}

@inproceedings{zhang2024,
  title = {How {{Effective Are They}}? {{Exploring Large Language Model Based Fuzz Driver Generation}}},
  shorttitle = {How {{Effective Are They}}?},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Cen and Zheng, Yaowen and Bai, Mingqiang and Li, Yeting and Ma, Wei and Xie, Xiaofei and Li, Yuekang and Sun, Limin and Liu, Yang},
  date = {2024-09-11},
  eprint = {2307.12469},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1223--1235},
  doi = {10.1145/3650212.3680355},
  url = {http://arxiv.org/abs/2307.12469},
  abstract = {LLM-based (Large Language Model) fuzz driver generation is a promising research area. Unlike traditional program analysis-based method, this text-based approach is more general and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges. To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs (\$8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that: - While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications; - LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process; - While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection. Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.},
  keywords = {important},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-04-02T09:02:31.545Z},
  file = {/home/kchou/HDD/Library/References/Zhang et al. - 2024 - How Effective Are They Exploring Large Language Model Based Fuzz Driver Generation - @zhangHowEffectiveAre2024.pdf;/home/kchou/HDD/Documents/Zotero Library/2307 1.html}
}

@inproceedings{zhang2024a,
  title = {How {{Effective Are They}}? {{Exploring Large Language Model Based Fuzz Driver Generation}}},
  shorttitle = {How {{Effective Are They}}?},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Zhang, Cen and Zheng, Yaowen and Bai, Mingqiang and Li, Yeting and Ma, Wei and Xie, Xiaofei and Li, Yuekang and Sun, Limin and Liu, Yang},
  date = {2024-09-11},
  series = {{{ISSTA}} 2024},
  pages = {1223--1235},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3650212.3680355},
  url = {https://dl.acm.org/doi/10.1145/3650212.3680355},
  abstract = {Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code. An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research. Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers. However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges.     To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers. Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects. Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings. In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs (\$8,000+ charged tokens). Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year). Our study uncovered that:     1) While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications;   2) LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics. Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process;   3) While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection.     Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.},
  isbn = {979-8-4007-0612-7},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-05-06T12:49:05.698Z},
  file = {/home/kchou/HDD/Library/References/Zhang et al. - 2024 - How Effective Are They Exploring Large Language Model Based Fuzz Driver Generation - @zhangHowEffectiveAre2024a.pdf}
}

@online{zhang2025,
  title = {Your {{Fix Is My Exploit}}: {{Enabling Comprehensive DL Library API Fuzzing}} with {{Large Language Models}}},
  shorttitle = {Your {{Fix Is My Exploit}}},
  author = {Zhang, Kunpeng and Wang, Shuai and Han, Jitao and Zhu, Xiaogang and Li, Xian and Wang, Shaohua and Wen, Sheng},
  date = {2025-01-08},
  eprint = {2501.04312},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.04312},
  url = {http://arxiv.org/abs/2501.04312},
  abstract = {Deep learning (DL) libraries, widely used in AI applications, often contain vulnerabilities like buffer overflows and use-after-free errors. Traditional fuzzing struggles with the complexity and API diversity of DL libraries such as TensorFlow and PyTorch, which feature over 1,000 APIs. Testing all these APIs is challenging due to complex inputs and varied usage patterns. While large language models (LLMs) show promise in code understanding and generation, existing LLM-based fuzzers lack deep knowledge of API edge cases and struggle with test input generation. To address this, we propose DFUZZ, an LLM-driven fuzzing approach for DL libraries. DFUZZ leverages two insights: (1) LLMs can reason about error-triggering edge cases from API code and apply this knowledge to untested APIs, and (2) LLMs can accurately synthesize test programs to automate API testing. By providing LLMs with a "white-box view" of APIs, DFUZZ enhances reasoning and generation for comprehensive fuzzing. Experimental results show that DFUZZ outperforms state-of-the-art fuzzers in API coverage for TensorFlow and PyTorch, uncovering 37 bugs, with 8 fixed and 19 under developer investigation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Software Engineering,llm fuzzing},
  annotation = {Read\_Status: To Read\\
Read\_Status\_Date: 2025-05-05T11:45:47.764Z},
  file = {/home/kchou/HDD/Library/References/Zhang et al. - 2025 - Your Fix Is My Exploit Enabling Comprehensive DL Library API Fuzzing with Large Language Models - @zhangYourFixMy2025.pdf;/home/kchou/HDD/Library/References/2501 2.html}
}

@online{zhong2024,
  title = {A {{Guide}} to {{Large Language Model Abstractions}}},
  author = {Zhong, Peter Yong and He, Haoze and Khattab, Omar and Potts, Christopher and Zaharia, Matei and Miller, Heather},
  date = {2024-01-16},
  url = {https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/},
  abstract = {A map of frameworks for abstracting interactions with and between large language models, plus two systems of organization for reasoning about LLM approaches and philosophies.},
  langid = {american},
  organization = {Two Sigma},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-04-01T17:25:06.645Z},
  file = {/home/kchou/HDD/Library/References/a-guide-to-large-language-model-abstractions.html;/home/kchou/HDD/Library/References/a-guide-to-large-language-model-abstractions.html}
}

@online{zibaeirad2025,
  title = {Reasoning with {{LLMs}} for {{Zero-Shot Vulnerability Detection}}},
  author = {Zibaeirad, Arastoo and Vieira, Marco},
  date = {2025-03-22},
  eprint = {2503.17885},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.17885},
  url = {http://arxiv.org/abs/2503.17885},
  abstract = {Automating software vulnerability detection (SVD) remains a critical challenge in an era of increasingly complex and interdependent software systems. Despite significant advances in Large Language Models (LLMs) for code analysis, prevailing evaluation methodologies often lack the \textbackslash textbf\{context-aware robustness\} necessary to capture real-world intricacies and cross-component interactions. To address these limitations, we present \textbackslash textbf\{VulnSage\}, a comprehensive evaluation framework and a dataset curated from diverse, large-scale open-source system software projects developed in C/C++. Unlike prior datasets, it leverages a heuristic noise pre-filtering approach combined with LLM-based reasoning to ensure a representative and minimally noisy spectrum of vulnerabilities. The framework supports multi-granular analysis across function, file, and inter-function levels and employs four diverse zero-shot prompt strategies: Baseline, Chain-of-Thought, Think, and Think \& Verify. Through this evaluation, we uncover that structured reasoning prompts substantially improve LLM performance, with Think \& Verify reducing ambiguous responses from 20.3\% to 9.1\% while increasing accuracy. We further demonstrate that code-specialized models consistently outperform general-purpose alternatives, with performance varying significantly across vulnerability types, revealing that no single approach universally excels across all security contexts. Link to dataset and codes: https://github.com/Erroristotle/VulnSage.git},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering,llm fuzzing},
  annotation = {Read\_Status: In Progress\\
Read\_Status\_Date: 2025-05-06T12:01:14.135Z},
  file = {/home/kchou/HDD/Library/References/Zibaeirad and Vieira - 2025 - Reasoning with LLMs for Zero-Shot Vulnerability Detection - @zibaeiradReasoningLLMsZeroShot2025.pdf;/home/kchou/HDD/Library/References/2503 1.html}
}

@online{zotero-item-3751,
  title = {How to {{Prevent}} the next {{Heartbleed}}},
  url = {https://dwheeler.com/essays/heartbleed.html},
  file = {/home/kchou/HDD/Library/References/heartbleed.html}
}

@online{zotero-item-4182,
  title = {{{AI-Powered Fuzzing}}: {{Breaking}} the {{Bug Hunting Barrier}}},
  shorttitle = {{{AI-Powered Fuzzing}}},
  url = {https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html},
  abstract = {Dongge Liu, Jonathan Metzman, Oliver Chang, Google Open Source Security Team~ Since 2016, OSS-Fuzz  has been at the forefront of automated v...},
  langid = {english},
  organization = {Google Online Security Blog},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-04T14:27:49.683Z},
  file = {/home/kchou/HDD/Library/References/ai-powered-fuzzing-breaking-bug-hunting.html}
}

@online{zotero-item-4392,
  title = {{{OSS-Fuzz Documentation}}},
  url = {https://google.github.io/oss-fuzz/},
  abstract = {Documentation for OSS-Fuzz},
  langid = {american},
  organization = {OSS-Fuzz},
  annotation = {Read\_Status: Read\\
Read\_Status\_Date: 2025-05-04T14:28:02.098Z},
  file = {/home/kchou/HDD/Library/References/oss-fuzz.html}
}
