[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OverHAuL",
    "section": "",
    "text": "Preface\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\n\n\n\nAcknowledgments\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\n\n\n\n\nCitation\n\n\n\nBibTeX citation:\n\n@thesis{chousos2025,\n  title = {LLM-Driven Fuzzing: Automatic Harness Generation for Crypto Libraries},\n  shorttitle = {LLM-Driven Fuzzing},\n  author = {Chousos, Konstantinos},\n  date = {2025-07},\n  institution = {{National and Kapodistrian University of Athens}},\n  location = {Athens, Greece},\n  url = {https://kchousos.github.io/BSc-Thesis/},\n  langid = {en, el}\n}\n\nFor attribution, please cite this work as:\n\n\nK. Chousos, “LLM-Driven Fuzzing: Automatic Harness Generation for Crypto Libraries,” Bachelor Thesis, National and Kapodistrian University of Athens, Athens, Greece, 2025. [Online]. Available: https://kchousos.github.io/BSc-Thesis/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-intro.html",
    "href": "chapters/01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Motivation\nResult: Bugs exist",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-intro.html#motivation",
    "href": "chapters/01-intro.html#motivation",
    "title": "1  Introduction",
    "section": "",
    "text": "Memory unsafety is and will be prevalent\nSoftware is safe until it’s not\nHumans make mistakes\nHumans now use Large Language Models (LLMs) to write software\nLLMs make mistakes [1]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-intro.html#goal",
    "href": "chapters/01-intro.html#goal",
    "title": "1  Introduction",
    "section": "1.2 Goal",
    "text": "1.2 Goal\nA system that:\n\nTakes a bare C project as input\nGenerates a new fuzzing harness from scratch using LLMs\nCompiles it\nExecutes it and evaluates it",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-intro.html#preview-of-following-sections-rename",
    "href": "chapters/01-intro.html#preview-of-following-sections-rename",
    "title": "1  Introduction",
    "section": "1.3 Preview of following sections (rename)",
    "text": "1.3 Preview of following sections (rename)\n\n\n\n\n[1] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html",
    "href": "chapters/02-background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Fuzz Testing\nFuzzing is an automated software-testing technique in which a Program Under Test (PUT) is executed with (pseudo-)random inputs in the hope of exposing undefined behavior. When such behavior manifests as a crash, hang, or memory-safety violation, the corresponding input constitutes a test-case that reveals a bug and often a vulnerability [1]. In essence, fuzzing is a form of adversarial, penetration-style testing carried out by the defender before the adversary has an opportunity to do so. Interest in the technique surged after the publication of three practitioner-oriented books in 2007–2008 [2], [3], [4].\nHistorically, the term was coined by Miller et al. in 1990, who used “fuzz” to describe a program that “generates a stream of random characters to be consumed by a target program” [5]. This informal usage captured the essence of what fuzzing aims to do: stress test software by bombarding it with unexpected inputs to reveal bugs. To formalize this concept, we adopt Manes et al.’s rigorous definitions [1]:\nThis means fuzzing involves running the target program on inputs that go beyond those it is typically designed to handle, aiming to uncover hidden issues. An individual instance of such execution—or a bounded sequence thereof—is called a fuzzing run. When these runs are conducted systematically and at scale with the specific goal of detecting violations of a security policy, the activity is known as fuzz testing (or simply fuzzing):\nThis distinction highlights that fuzz testing is fuzzing with an explicit focus on security properties and policy enforcement. Central to managing this process is the fuzzer engine, which orchestrates the execution of one or more fuzzing runs as part of a fuzz campaign. A fuzz campaign represents a concrete instance of fuzz testing tailored to a particular program and security policy:\nThroughout each execution within a campaign, a bug oracle plays a critical role in evaluating the program’s behavior to determine whether it violates the defined security policy:\nIn practice, bug oracles often rely on runtime instrumentation techniques, such as monitoring for fatal POSIX signals (e.g., SIGSEGV) or using sanitizers like AddressSanitizer (ASan) [6]. Tools like LibFuzzer [7] commonly incorporate such instrumentation to reliably identify crashes or memory errors during fuzzing.\nMost fuzz campaigns begin with a set of seeds—inputs that are well-formed and belong to the PUT’s expected input space—called a seed corpus. These seeds serve as starting points from which the fuzzer generates new test cases by applying transformations or mutations, thereby exploring a broader input space:\nThe process of selecting an effective initial corpus is crucial because it directly impacts how quickly and thoroughly the fuzzer can cover the target program’s code. This challenge—studied as the seed-selection problem—involves identifying seeds that enable rapid discovery of diverse execution paths and is non-trivial [8]. A well-chosen seed set often accelerates bug discovery and improves overall fuzzing efficiency.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html#fuzz-testing",
    "href": "chapters/02-background.html#fuzz-testing",
    "title": "2  Background",
    "section": "",
    "text": "Definition 2.1 (Fuzzing) Fuzzing is the execution of a Program Under Test (PUT) using input(s) sampled from an input space (the fuzz input space) that protrudes the expected input space of the PUT.\n\n\n\nDefinition 2.2 (Fuzz Testing) Fuzz testing is the use of fuzzing to test whether a PUT violates a security policy.\n\n\n\nDefinition 2.3 (Fuzzer, Fuzzer Engine) A fuzzer is a program that performs fuzz testing on a PUT.\n\n\nDefinition 2.4 (Fuzz Campaign) A fuzz campaign is a specific execution of a fuzzer on a PUT with a specific security policy.\n\n\n\nDefinition 2.5 (Bug Oracle) A bug oracle is a component (often inside the fuzzer) that determines whether a given execution of the PUT violates a specific security policy.\n\n\n\n\nDefinition 2.6 (Seed) An input given to the PUT that is mutated by the fuzzer to produce new test cases. During a fuzz campaign (Definition 2.4) all seeds are stored in a seed pool or corpus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.1 Motivation\n\nThe purpose of fuzzing relies on the assumption that there are bugs within every program, which are waiting to be discovered. Therefore, a systematic approach should find them sooner or later.\n— OWASP Foundation [9]\n\nFuzz testing offers several tangible benefits:\n\nEarly vulnerability discovery: Detecting defects during development is cheaper and safer than addressing exploits in production.\nAdversary-parity: Performing the same randomised stress that attackers employ allows defenders to pre-empt zero-days.\nRobustness and correctness: Beyond security, fuzzing exposes logic errors and stability issues in complex, high-throughput APIs (e.g., decompressors) even when inputs are expected to be well-formed.\nRegression prevention: Re-running a corpus of crashing inputs as part of continuous integration ensures that fixed bugs remain fixed.\n\n\n2.1.1.1 Success Stories\nHeartbleed (CVE-2014-0160) [10], [11] arose from a buffer over-read1 in OpenSSL [12] introduced on 1 February 2012 and unnoticed until 1 April 2014. Post-mortem analyses showed that a simple fuzz campaign exercising the TLS heartbeat extension would have revealed the defect almost immediately [13].\n\n\nhttps://xkcd.com/1354/↩︎\n\n\nLikewise, the Shellshock (or Bashdoor) family of bugs in GNU Bash [14] enabled arbitrary command execution on many UNIX systems. While the initial flaw was fixed promptly, subsequent bug variants were discovered by Google’s Michał Zalewski using his own fuzzer [15] in late 2014 [16].\nOn the defensive tooling side, the security tool named Mayhem—developed by the company of the same name—has since been adopted by the US Air Force, the Pentagon, Cloudflare, and numerous open-source communities. It has found and facilitated the remediation of thousands of previously unknown vulnerabilities [17].\nThese cases underscore the central thesis of fuzz testing: exhaustive manual review is infeasible, but scalable stochastic exploration reliably surfaces the critical few defects that matter most.\n\n\n\n2.1.2 Methodology\nAs previously discussed, fuzz testing of a program under test (PUT) is typically conducted using a dedicated fuzzing engine (see Definition 2.3). Among the most widely adopted fuzzers for C and C++ projects and libraries are AFL [15]—which has since evolved into AFL++ [18]—and LibFuzzer [7]. Within the OverHAuL framework, LibFuzzer is preferred owing to its superior suitability for library fuzzing, whereas AFL++ predominantly targets executables and binary fuzzing.\n\n2.1.2.1 LibFuzzer\nLibFuzzer [7] is an in-process, coverage-guided evolutionary fuzzing engine primarily designed for testing libraries. It forms part of the LLVM ecosystem [19] and operates by linking directly with the library under evaluation. The fuzzer delivers mutated input data to the library through a designated fuzzing entry point, commonly referred to as the fuzz target.\n\nDefinition 2.7 (Fuzz target) A function that accepts a byte array as input and exercises the application programming interface (API) under test using these inputs [7]. This construct is also known as a fuzz driver, fuzzer entry point, or fuzzing harness.\n\nFor the remainder of this thesis, the terms presented in Definition 2.7 will be used interchangeably.\nTo effectively validate an implementation or library, developers are required to author a fuzzing harness that invokes the target library’s API functions utilizing the fuzz-generated inputs. This harness serves as the principal interface for the fuzzer and is executed iteratively, each time with mutated input designed to maximize code coverage and uncover defects. To comply with LibFuzzer’s interface requirements, a harness must conform to the following function signature:\n\n\n\nListing 2.1: This function receives the fuzzing input via a pointer to an array of bytes (Data) and its associated size (Size). Efficiency in fuzzing is achieved by invoking the API of interest within the body of this function, thereby allowing the fuzzer to explore a broad spectrum of behavior through systematic input mutation.\n\n\nint LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {\n  DoSomethingInterestingWithData(Data, Size);\n  return 0;\n}\n\n\n\nA more illustrative example of such a harness is provided in Listing 2.2.\n\n\n\nListing 2.2: This example demonstrates a minimal harness that triggers a controlled crash upon receiving HI! as input.\n\n\n// test_fuzzer.cpp\n#include &lt;stdint.h&gt;\n#include &lt;stddef.h&gt;\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n  if (size &gt; 0 && data[0] == 'H')\n    if (size &gt; 1 && data[1] == 'I')\n      if (size &gt; 2 && data[2] == '!')\n        __builtin_trap();\n  return 0;\n}\n\n\n\nTo compile and link such a harness with LibFuzzer, the Clang compiler—also part of the LLVM project [19]—must be used alongside appropriate compiler flags. For instance, compiling the harness in Listing 2.2 can be achieved as follows:\n\n\n\nListing 2.3: This example illustrates the compilation and execution workflow necessary for deploying a LibFuzzer-based fuzzing harness.\n\n\n# Compile test_fuzzer.cc with AddressSanitizer and link against LibFuzzer.\nclang++ -fsanitize=address,fuzzer test_fuzzer.cc\n# Execute the fuzzer without any pre-existing seed corpus.\n./a.out\n\n\n\n\n\n2.1.2.2 AFL and AFL++\nAmerican Fuzzy Lop (AFL) [15], developed by Michał Zalewski, is a seminal fuzzer targeting C and C++ applications. Its core methodology relies on instrumented binaries to provide edge coverage feedback, thereby guiding input mutation towards unexplored program paths. AFL supports several emulation backends including QEMU [20]—an open-source CPU emulator facilitating fuzzing on diverse architectures—and Unicorn [21], a lightweight multi-platform CPU emulator. While AFL established itself as a foundational tool within the fuzzing community, its successor AFL++ [18] incorporates numerous enhancements and additional features to improve fuzzing efficacy.\nAFL operates by ingesting seed inputs from a specified directory (seeds_dir), applying mutations, and then executing the target binary to discover novel execution paths. Execution can be initiated using the following command-line syntax:\n./afl-fuzz -i seeds_dir -o output_dir -- /path/to/tested/program\nAFL is capable of fuzzing both black-box and instrumented binaries, employing a fork-server mechanism to optimize performance. It additionally supports persistent mode execution as well as modes leveraging QEMU and Unicorn emulators, thereby providing extensive flexibility for different testing environments.\nAlthough AFL is traditionally utilized for fuzzing standalone programs or binaries, it is also capable of fuzzing libraries and other software components. In such scenarios, rather than implementing the LLVMFuzzerTestOneInput style harness, AFL can use the standard main() function as the fuzzing entry point. Nonetheless, AFL also accommodates integration with LLVMFuzzerTestOneInput-based harnesses, underscoring its adaptability across varied fuzzing use cases.\n\n\n\n2.1.3 Challenges in Adoption\nDespite its potential for uncovering software vulnerabilities, fuzzing remains a relatively underutilized testing technique compared to more established methodologies such as Test-Driven Development (TDD). This limited adoption can be attributed, in part, to the substantial initial investment required to design and implement appropriate test harnesses that enable effective fuzzing processes. Furthermore, the interpretation of fuzzing outcomes—particularly the identification, diagnostic analysis, and prioritization of program crashes—demands considerable resources and specialized expertise. These factors collectively pose significant barriers to the widespread integration of fuzzing within standard software development and testing practices.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html#large-language-models",
    "href": "chapters/02-background.html#large-language-models",
    "title": "2  Background",
    "section": "2.2 Large Language Models",
    "text": "2.2 Large Language Models\nNatural Language Processing (NLP), a subfield of Artificial Intelligence (AI), has a rich and ongoing history that has evolved significantly since its beginning in the 1990s [22], [23]. Among the most notable—and recent—advancements in this domain are Large Language Models (LLMs), which have transformed the landscape of NLP and AI in general.\nAt the core of many LLMs is the attention mechanism, which was introduced by Bahdanau et al. in 2014 [24]. This pivotal innovation enabled models to focus on relevant parts of the input sequence when making predictions, significantly improving language understanding and generation tasks. Building on this foundation, the Transformer architecture was proposed by Vaswani et al. in 2017 [25]. This architecture has become the backbone of most contemporary LLMs, as it efficiently processes sequences of data, capturing long-range dependencies without being hindered by sequential processing limitations.\nOne of the first major breakthroughs utilizing the Transformer architecture was BERT (Bidirectional Encoder Representations from Transformers), developed by Devlin et al. in 2019 [26]. BERT’s bi-directional understanding allowed it to capture the context of words from both directions, which improved the accuracy of various NLP tasks. Following this, the Generative Pre-trained Transformer (GPT) series, initiated by OpenAI with the original GPT model in 2018 [27], further pushed the boundaries. Subsequent iterations, including GPT-2 [28], GPT-3 [29], and the most current GPT-4 [30], have continued to enhance performance by scaling model size, data, and training techniques.\nIn addition to OpenAI’s contributions, other significant models have emerged, such as Claude, DeepSeek-R1 and the Llama series (1 through 3) [31], [32], [33]. The proliferation of LLMs has sparked an active discourse about their capabilities, applications, and implications in various fields.\n\n2.2.1 Biggest GPTs\nUser-facing LLMs are generally categorized between closed-source and open-source models. Closed-source LLMs like ChatGPT, Claude, and Gemini [31], [34], [35] represent commercially developed systems often optimized for specific tasks without public access to their underlying weights. In contrast, open-source models2, including the Llama series [33] and Deepseek [32], provide researchers and practitioners with access to model weights, allowing for greater transparency and adaptability.\n\n\nThe term “open-source” models is somewhat misleading, since these are better termed as open-weights models. While their weights are publicly available, their training data and underlying code are often proprietary. This terminology reflects community usage but fails to capture the limitations of transparency and accessibility inherent in these models.↩︎\n\n\n\n\n2.2.2 Prompting\nInteraction with LLMs typically occurs through chat-like interfaces, a process commonly referred to as prompting. A critical aspect of effective engagement with LLMs is the usage of different prompting strategies, which can significantly influence the quality and relevance of the generated outputs. Various approaches to prompting have been developed and studied, including zero-shot and few-shot prompting. In zero-shot prompting, the model is expected to perform a specific task without any examples, while in few-shot prompting, the user provides a limited number of examples to guide the model’s responses [29].\nTo enhance performance on more complex tasks, several advanced prompting techniques have emerged. One notable strategy is the Chain of Thought approach [36], which entails presenting the model with sample thought processes for solving a given task. This method encourages the model to generate more coherent and logical reasoning by mimicking human-like cognitive pathways. A refined variant of this approach is the Tree of Thoughts technique [37], which enables the LLM to explore multiple lines of reasoning concurrently, thereby facilitating the selection of the most promising train of thought for further exploration.\nIn addition to these cognitive strategies, Retrieval-Augmented Generation (RAG) [38] is another innovative technique that enhances the model’s capacity to provide accurate information by incorporating external knowledge not present in its training dataset. RAG operates by integrating the LLM with an external storage system—often a vector store containing relevant documents—that the model can query in real-time. This allows the LLM to pull up pertinent and/or proprietary information in response to user queries, resulting in more comprehensive and accurate answers.\nMoreover, the ReAct framework [39], which stands for Reasoning and Acting, empowers LLMs by granting access to external tools. This capability allows LLM instances to function as intelligent agents that can interact meaningfully with their environment through user-defined tools. For instance, a ReAct tool could be a function that returns a weather forecast based on the user’s current location. In this scenario, the LLM can provide accurate and truthful predictions, thereby mitigating risks associated with hallucinated responses.\n\n\n2.2.3 LLMs for Coding\nThe impact of LLMs in software development in recent years is apparent, with hundreds of LLM-assistance extensions and Integrated Development Environments (IDEs) being published. Notable instances include tools like GitHub Copilot and IDEs such as Cursor, which leverage LLM capabilities to provide developers with coding suggestions, auto-completions, and even real-time debugging assistance [40], [41]. Such innovations have introduced a layer of interaction that enhances productivity and fosters a more intuitive coding experience. Simultaneously, certain LLMs are trained themselves with the code-generation task in mind [42], [43], [44].\nOne exemplary product of this innovation is vibecoding and the no-code movement, which describe the development of software by only prompting and tasking an LLM, i.e. without any actual programming required by the user. This constitutes a showcase of how LLMs can be harnessed to elevate the coding experience by supporting developers as they navigate complex programming tasks [45]. By analyzing the context of the code being written, these sophisticated models can provide contextualized insights and relevant snippets, effectively streamlining the development process. Developers can benefit from reduced cognitive load, as they receive suggestions that not only cater to immediate coding needs but also promote adherence to best practices and coding standards.\nDespite these advancements, it is crucial to recognize the inherent limitations of LLMs when applied to software development. While they can help in many aspects of coding, they are not immune to generating erroneous outputs—a phenomenon often referred to as “hallucination”. Hallucinations occur when LLMs produce information that is unfounded or inaccurate, which can stem from several factors, including the limitations of their training data and the constrained context window within which they operate. As LLMs generate code suggestions based on the patterns learned from vast datasets, they may inadvertently propose solutions that do not align with the specific requirements of a task or that utilize outdated programming paradigms.\nMoreover, the challenge of limited context windows can lead to suboptimal suggestions. LLMs generally process a fixed amount of text when generating responses, which can impact their ability to fully grasp the nuances of complex coding scenarios. This may result in outputs that lack the necessary depth and specificity required for successful implementation. As a consequence, developers must exercise caution and critically evaluate the suggestions offered by these models, as reliance on them without due diligence could lead to the introduction of bugs or other issues in the code.\n\n\n2.2.4 LLMs for Fuzzing\nWhile large language models (LLMs) demonstrate significant potential in enhancing the software development process, the challenges highlighted in Section 2.2.3 become even more pronounced and troublesome when these models are employed to generate fuzzing harnesses. The task of writing a fuzzing harness inherently demands an in-depth comprehension of both the library being tested and the intricate interactions expected among its various components. This level of understanding is often beyond the capabilities of LLMs, primarily due to their context window limitations, which restrict the amount of information they can effectively process and retain during code generation.\nIn addition to this issue, the risk of error-prone code produced by LLMs further complicates the fuzzing workflow. When a crash occurs during the fuzzing process, it becomes imperative for developers to ascertain that the root cause of the failure is not attributable to deficiencies or bugs within the harness itself. This additional layer of verification adds to the cognitive load placed upon developers, potentially detracting from their ability to focus on testing and improving the underlying software.\nTo enhance the reliability of LLM-generated harnesses in fuzzing contexts, it is essential that these generated artifacts undergo thorough evaluation and validation through programmatic means. This involves the implementation of systematic techniques that assess the accuracy and robustness of the generated code, ensuring that it aligns with the expected behavior of the components it is intended to interact with. This strategy can be conceptualized within the framework of Neurosymbolic AI (Section 2.3), which seeks to integrate the strengths of neural networks with symbolic reasoning capabilities. By marrying these two paradigms, it may be possible to improve the reliability and efficacy of LLMs in the creation of fuzzing harnesses, ultimately leading to a more seamless integration of automated testing methodologies into the software development lifecycle.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html#sec-nesy",
    "href": "chapters/02-background.html#sec-nesy",
    "title": "2  Background",
    "section": "2.3 Neurosymbolic AI",
    "text": "2.3 Neurosymbolic AI\nNeurosymbolic AI (NeSy AI) represents a groundbreaking fusion of neural network methodologies with symbolic execution techniques and tools, providing a multi-faceted approach to overcoming the inherent limitations of traditional AI paradigms [46], [47]. This innovative synthesis seeks to combine the strengths of both neural networks, which excel in pattern recognition and data-driven learning, and symbolic systems, which offer structured reasoning and interpretability. By integrating these two approaches, NeSy AI aims to create cognitive models that are not only more accurate but also more robust in problem-solving contexts.\nAt its core, NeSy AI facilitates the development of AI systems that are capable of understanding and interpreting feedback in real-world scenarios [48]. This characteristic is particularly significant in the current landscape of artificial intelligence, where LLMs are predominant. In this context, NeSy AI is increasingly viewed as a critical solution to pressing issues related to explainability, attribution, and reliability in AI systems [49], [50]. These challenges are essential for ensuring that AI systems can be trusted and effectively utilized in various applications, from business to healthcare.\nThe burgeoning field of neurosymbolic AI is still in its nascent stages, with ongoing research and development actively exploring its potential to enhance attribution methodologies within large language models. By addressing these critical challenges, NeSy AI can significantly contribute to the broader landscape of trustworthy AI systems, allowing for more transparent and accountable decision-making processes [46], [49], [50].\nMoreover, the application of neurosymbolic AI within the domain of fuzzing is gaining traction, paving the way for innovative explorations. This integration of LLMs with symbolic systems opens up new avenues for research. Currently, there are only a limited number of tools that support such hybrid approaches (Chapter 3). Among these, OverHAuL constitutes a Neuro[Symbolic] tool, as classified by Henry Kautz’s taxonomy [51], [52]. This means that the neural model—specifically the LLM—can leverage symbolic reasoning tools—in this case a source code explorer (Chapter 7)—to augment its reasoning capabilities. This symbiotic relationship enhances the overall efficacy and versatility of LLMs for fuzzing harnesses generation, demonstrating the profound potential held by the fusion of neural and symbolic methodologies.\n\n\n\n\n[1] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[2] A. Takanen, J. DeMott, C. Miller, and A. Kettunen, Fuzzing for software security testing and quality assurance, Second edition. in Information security and privacy library. Boston London Norwood, MA: Artech House, 2018.\n\n\n[3] M. Sutton, A. Greene, and P. Amini, Fuzzing: Brute force vulnerabilty discovery. Upper Saddle River, NJ: Addison-Wesley, 2007.\n\n\n[4] N. Rathaus and G. Evron, Open source fuzzing tools. Burlington, MA: Syngress Pub, 2007.\n\n\n[5] B. P. Miller, L. Fredriksen, and B. So, “An empirical study of the reliability of UNIX utilities,” Commun. ACM, vol. 33, no. 12, pp. 32–44, Dec. 1990, doi: 10.1145/96267.96279. Available: https://dl.acm.org/doi/10.1145/96267.96279\n\n\n[6] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “AddressSanitizer: A fast address sanity checker,” in 2012 USENIX annual technical conference (USENIX ATC 12), 2012, pp. 309–318. Available: https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany\n\n\n[7] LLVM Project, “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[8] A. Rebert et al., “Optimizing seed selection for fuzzing,” in Proceedings of the 23rd USENIX conference on Security Symposium, in SEC’14. USA: USENIX Association, Aug. 2014, pp. 861–875.\n\n\n[9] OWASP Foundation, “Fuzzing.” Available: https://owasp.org/www-community/Fuzzing\n\n\n[10] Blackduck, Inc., “Heartbleed Bug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[11] CVE Program, “CVE - CVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[12] The OpenSSL Project, “Openssl/openssl.” OpenSSL, Jul. 15, 2025. Available: https://github.com/openssl/openssl\n\n\n[13] D. Wheeler, “How to Prevent the next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[14] GNU Project, “Bash - GNU Project - Free Software Foundation.” Available: https://www.gnu.org/software/bash/\n\n\n[15] M. Zalewski, “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[16] J. Saarinen, “Further flaws render Shellshock patch ineffective,” Sep. 29, 2014. Available: https://www.itnews.com.au/news/further-flaws-render-shellshock-patch-ineffective-396256\n\n\n[17] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[18] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[19] LLVM Project, “The LLVM Compiler Infrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[20] F. Bellard, P. Maydell, and QEMU Team, “QEMU.” May 29, 2025. Available: https://www.qemu.org/\n\n\n[21] Unicorn Engine, “Unicorn-engine/unicorn.” Unicorn Engine, Jul. 15, 2025. Available: https://github.com/unicorn-engine/unicorn\n\n\n[22] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[23] Z. Wang, Z. Chu, T. V. Doan, S. Ni, M. Yang, and W. Zhang, “History, development, and principles of large language models: An introductory survey,” AI Ethics, vol. 5, no. 3, pp. 1955–1971, Jun. 2025, doi: 10.1007/s43681-024-00583-7. Available: https://doi.org/10.1007/s43681-024-00583-7\n\n\n[24] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” May 19, 2016. doi: 10.48550/arXiv.1409.0473. Available: http://arxiv.org/abs/1409.0473\n\n\n[25] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” May 24, 2019. doi: 10.48550/arXiv.1810.04805. Available: http://arxiv.org/abs/1810.04805\n\n\n[27] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” 2018, Available: https://www.mikecaptain.com/resources/pdf/GPT-1.pdf\n\n\n[28] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019, Available: https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf\n\n\n[29] T. B. Brown et al., “Language Models are Few-Shot Learners,” Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165. Available: http://arxiv.org/abs/2005.14165\n\n\n[30] OpenAI et al., “GPT-4 Technical Report,” Mar. 04, 2024. doi: 10.48550/arXiv.2303.08774. Available: http://arxiv.org/abs/2303.08774\n\n\n[31] Anthropic, “Claude,” 2025. Available: https://claude.ai/new\n\n\n[32] DeepSeek-AI et al., “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,” Jan. 22, 2025. doi: 10.48550/arXiv.2501.12948. Available: http://arxiv.org/abs/2501.12948\n\n\n[33] A. Grattafiori et al., “The Llama 3 Herd of Models,” Nov. 23, 2024. doi: 10.48550/arXiv.2407.21783. Available: http://arxiv.org/abs/2407.21783\n\n\n[34] OpenAI, “ChatGPT,” 2025. Available: https://chatgpt.com\n\n\n[35] Google, “‎Google Gemini,” 2025. Available: https://gemini.google.com\n\n\n[36] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[37] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[38] P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” Apr. 12, 2021. doi: 10.48550/arXiv.2005.11401. Available: http://arxiv.org/abs/2005.11401\n\n\n[39] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[40] Anysphere, “Cursor - The AI Code Editor,” 2025. Available: https://cursor.com/\n\n\n[41] Microsoft, “GitHub Copilot · Your AI pair programmer,” 2025. Available: https://github.com/features/copilot\n\n\n[42] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “CodeGen2: Lessons for training llms on programming and natural languages,” ICLR, 2023.\n\n\n[43] E. Nijkamp et al., “CodeGen: An open large language model for code with multi-turn program synthesis,” ICLR, 2023.\n\n\n[44] OpenAI, “Introducing Codex,” May 16, 2025. Available: https://openai.com/index/introducing-codex/\n\n\n[45] A. Sarkar and I. Drosos, “Vibe coding: Programming through conversation with artificial intelligence,” Jun. 29, 2025. doi: 10.48550/arXiv.2506.23253. Available: http://arxiv.org/abs/2506.23253\n\n\n[46] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[47] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[48] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[49] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[50] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[51] M. K. Sarker, L. Zhou, A. Eberhart, and P. Hitzler, “Neuro-symbolic artificial intelligence: Current trends,” AIC, vol. 34, no. 3, pp. 197–209, Mar. 2022, doi: 10.3233/aic-210084. Available: https://journals.sagepub.com/doi/full/10.3233/AIC-210084\n\n\n[52] H. Kautz, “The Third AI Summer,” presented at the 34th Annual Meeting of the Association for the Advancement of Artificial Intelligence, Feb. 10, 2020. Available: https://www.youtube.com/watch?v=_cQITY0SPiw",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/03-related.html",
    "href": "chapters/03-related.html",
    "title": "3  Related work",
    "section": "",
    "text": "3.1 Previous Projects\nAutomated testing, automated fuzzing and automated harness creation have a long research history. Still, a lot of ground remains to be covered until true automation of these tasks is achieved. Until the introduction of transformers [1] and the 2020’s boom of commercial GPTs [2], automation regarding testing and fuzzing was mainly attempted through static and dynamic program analysis methods. These approaches are still utilized, but the fuzzing community has shifted almost entirely to researching the incorporation and employment of LLMs in the last half decade, in the name of automation [3], [4], [5], [6], [7], [8], [9], [10], [11], [12].",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/03-related.html#previous-projects",
    "href": "chapters/03-related.html#previous-projects",
    "title": "3  Related work",
    "section": "",
    "text": "3.1.1 KLEE\nKLEE [13] is a seminal and widely cited symbolic execution engine introduced in 2008 by Cadar et al. It was designed to automatically generate high-coverage test cases for programs written in C, using symbolic execution to systematically explore the control flow of a program. KLEE operates on the LLVM [14] bytecode representation of programs, allowing it to be applied to a wide range of C programs compiled to the LLVM intermediate representation.\nInstead of executing a program on concrete inputs, KLEE performs symbolic execution—that is, it runs the program on symbolic inputs, which represent all possible values simultaneously. At each conditional branch, KLEE explores both paths by forking the execution and accumulating path constraints (i.e., logical conditions on input variables) along each path. This enables it to traverse many feasible execution paths in the program, including corner cases that may be difficult to reach through random testing or manual test creation.\nWhen an execution path reaches a terminal state (e.g., a program exit, an assertion failure, or a segmentation fault), KLEE uses a constraint solver to compute concrete input values that satisfy the accumulated constraints for that path. These values form a test case that will deterministically drive the program down that specific path when executed concretely.\n\n\n3.1.2 IRIS\nIRIS [3] is a 2025 open-source neurosymbolic system for static vulnerability analysis. Given a codebase and a list of user-specified Common Weakness Enumerations (CWEs), it analyzes source code to identify paths that may correspond to known vulnerability classes. IRIS combines symbolic analysis—such as control- and data-flow reasoning—with neural models trained to generalize over code patterns. It outputs candidate vulnerable paths along with explanations and CWE references. The system operates on full repositories and supports extensible CWE definitions.\n\n\n3.1.3 FUDGE\nFUDGE [12] is a closed-source tool, made by Google, for automatic harness generation of C and C++ projects based on existing client code. It was used in conjunction with and in the improvement of Google’s OSS-Fuzz [15]. Being deployed inside Google’s infrastructure, FUDGE continuously examines Google’s internal code repository, searching for code that uses external libraries in a meaningful and “fuzzable” way (i.e. predominantly for parsing). If found, such code is sliced [16], per FUDGE, based on its Abstract Syntax Tree (AST) using LLVM’s Clang tool [14]. The above process results in a set of abstracted mostly-self-contained code snippets that make use of a library’s calls and/or API. These snippets are later synthesized into the body of a fuzz driver, with variables being replaced and the fuzz input being utilized. Each is then injected in an LLVMFuzzerTestOneInput function and finalized as a fuzzing harness. A building and evaluation phase follows for each harness, where they are executed and examined. Every passing harness along with its evaluation results is stored in FUDGE’s database, reachable to the user through a custom web-based UI.\n\n\n3.1.4 UTopia\nUTopia [8] (stylized UTopia) is another open-source automatic harness generation framework. Aside from the library code, It operates solely on user-provided unit tests since, according to [8], they are a resource of complete and correct API usage examples containing working library set-ups and tear-downs. Additionally, each of them are already close to a fuzz target, in the sense that they already examine a single and self-contained API usage pattern. Each generated harness follows the same data flow of the originating unit test. Static analysis is employed to figure out what fuzz input placement would yield the most results. It is also utilized in abstracting the tests away from the syntactical differences between testing frameworks, along with slicing and AST traversing using Clang.\n\n\n3.1.5 FuzzGen\nAnother project of Google is FuzzGen [11], this time open-source. Like FUDGE, it leverages existing client code of the target library to create fuzz targets for it. FuzzGen uses whole-system analysis, through which it creates an Abstract API Dependence Graph (A2DG). It uses the latter to automatically generate LibFuzzer-compatible harnesses. For FuzzGen to work, the user needs to provide both client code and/or tests for the API and the API library’s source code as well. FuzzGen uses the client code to infer the correct usage of the API and not its general structure, in contrast to FUDGE. FuzzGen’s workflow can be divided into three phases: 1. API usage inference. By consuming and analyzing client code and tests that concern the library under test, FuzzGen recognizes which functions belong to the library and learns its correct API usage patterns. This process is done with the help of Clang. To test if a function is actually a part of the library, a sample program is created that uses it. If the program compiles successfully, then the function is indeed a valid API call. 2. A2DG construction mechanism. For all the existing API calls, FuzzGen builds an A2DG to record the API usages and infers its intended structure. After completion, this directed graph contains all the valid API call sequences found in the client code corpus. It is built in a two-step process: First, many smaller A2DGs are created, one for each root function per client code snippet. Once such graphs have been created for all the available client code instances, they are combined to formulate the master A2DG. This graph can be seen as a template for correct usage of the library. 3. Fuzzer generator. Through the A2DG, a fuzzing harness is created. Contrary to FUDGE, FuzzGen does not create multiple “simple” harnesses but a single complex one with the goal of covering the whole of the A2DG. In other words, while FUDGE fuzzes a single API call at a time, FuzzGen’s result is a single harness that tries to fuzz the given library all at once through complex API usage.\n\n\n3.1.6 IntelliGen\nSAMPLE\nZhang et al. present IntelliGen [17], a system for automatically synthesizing fuzz drivers by statically identifying potentially vulnerable entry-point functions within C projects. Implemented using LLVM, IntelliGen focuses on improving fuzzing efficiency by targeting code more likely to contain memory safety issues, rather than exhaustively fuzzing all available functions.\nThe system comprises two main components: the Entry Function Locator and the Fuzz Driver Synthesizer. The Entry Function Locator analyzes the project’s abstract syntax tree (AST) and classifies functions based on heuristics that indicate vulnerability. These include pointer dereferencing, calls to memory-related functions (e.g., memcpy, memset), and invocation of other internal functions. Functions that score highly on these metrics are prioritized for fuzz driver generation. The guiding insight is that entry points with fewer argument checks and more direct memory operations expose more useful program logic for fuzz testing.\nThe Fuzz Driver Synthesizer then generates harnesses for these entry points. For each target function, it synthesizes a LLVMFuzzerTestOneInput function that invokes the target with arguments derived from the fuzzer input. This process involves inferring argument types from the source code and ensuring that runtime behavior does not violate memory safety—thus avoiding invalid inputs that would cause crashes unrelated to genuine bugs.\nIntelliGen stands out by integrating static vulnerability estimation into the driver generation pipeline. Compared to prior tools like FuzzGen and FUDGE, it uses a more targeted, heuristic-based selection of functions, increasing the likelihood that fuzzing will exercise meaningful and vulnerable code paths.\n\n\n3.1.7 CKGFuzzer\nSAMPLE\nCKGFuzzer [18] is a fuzzing framework designed to automate the generation of effective fuzz drivers for C/C++ libraries by leveraging static analysis and large language models. Its workflow begins by parsing the target project along with any associated library APIs to construct a code knowledge graph. This involves two primary steps: first, parsing the abstract syntax tree (AST), and second, performing interprocedural program analysis. Through this process, CKGFuzzer extracts essential program elements such as data structures, function signatures, function implementations, and call relationships.\nUsing the knowledge graph, CKGFuzzer then identifies and queries meaningful API combinations, focusing on those that are either frequently invoked together or exhibit functional similarity. It generates candidate fuzz drivers for these combinations and attempts to compile them. Any compilation errors encountered during this phase are automatically repaired using heuristics and domain knowledge. A dynamically updated knowledge base, constructed from prior library usage patterns, guides both the generation and repair processes.\nOnce the drivers are successfully compiled, CKGFuzzer executes them while monitoring code coverage at the file level. It uses coverage feedback to iteratively mutate underperforming API combinations, refining them until new execution paths are discovered or a preset mutation budget is exhausted.\nFinally, any crashes triggered during fuzzing are subjected to a reasoning process based on chain-of-thought prompting. To help determine their severity and root cause, CKGFuzzer consults an LLM-generated knowledge base containing real-world examples of vulnerabilities mapped to known Common Weakness Enumeration (CWE) entries.\n\n\n3.1.8 PromptFuzz\nSAMPLE\nLyu et al. (2024) introduce PromptFuzz [19], a system for automatically generating fuzz drivers using LLMs, with a novel focus on prompt mutation to improve coverage. The system is implemented in Rust and targets C libraries, aiming to explore more of the API surface with each iteration.\nThe workflow begins with the random selection of API functions, extracted from header file declarations. These functions are used to construct initial prompts that instruct the LLM to generate a simple program utilizing the API. Each generated program is compiled, executed, and monitored for code coverage. Programs that fail to compile or violate runtime checks (e.g., sanitizers) are discarded.\nA key innovation in PromptFuzz is coverage-guided prompt mutation. Instead of mutating generated code directly, PromptFuzz mutates the LLM prompts—selecting new combinations of API functions to target unexplored code paths. This process is guided by a power scheduling strategy that prioritizes underused or promising API functions based on feedback from previous runs.\nOnce an effective program is produced, it is transformed into a fuzz driver by replacing constants and arguments with variables derived from the fuzzer input. Multiple such drivers are embedded into a single harness, where the input determines which program variant to execute, typically via a case-switch construct.\nOverall, PromptFuzz demonstrates that prompt-level mutation enables more effective exploration of complex APIs and achieves better coverage than direct code mutations, offering a compelling direction for LLM-based automated fuzzing systems.\n\n\n3.1.9 OSS-Fuzz\nOSS-Fuzz [15], [20] is a continuous, scalable and distributed cloud fuzzing solution for critical and prominent open-source projects. Developers of such software can submit their projects to OSS-Fuzz’s platform, where its harnesses are built and constantly executed. This results in multiple bug findings that are later disclosed to the primary developers and are later patched.\nOSS-Fuzz started operating in 2016, an initiative in response to the Heartbleed vulnerability [21], [22], [23]. Its hope is that through more extensive fuzzing such errors could be caught and corrected before having the chance to be exploited and thus disrupt the public digital infrastructure. So far, it has helped uncover over 10,000 security vulnerabilities and 36,000 bugs across more than 1,000 projects, significantly enhancing the quality and security of major software like Chrome, OpenSSL, and systemd.\nA project that’s part of OSS-Fuzz must have been configured as a ClusterFuzz [24] project. ClusterFuzz is the fuzzing infrastructure that OSS-Fuzz uses under the hood and depends on Google Cloud Platform services, although it can be hosted locally. Such an integration requires setting up a build pipeline, fuzzing jobs and expects a Google Developer account. Results are accessible through a web interface. ClusterFuzz, and by extension OSS-Fuzz, supports fuzzing through LibFuzzer, AFL++, Honggfuzz and FuzzTest—successor to Centipede— with the last two being Google projects [25], [26], [27], [28]. C, C++, Rust, Go, Python and Java/JVM projects are supported.\n\n\n3.1.10 OSS-Fuzz-Gen\nOSS-Fuzz-Gen (OFG) [6], [29] is Google’s current State-Of-The-Art (SOTA) project regarding automatic harness generation through LLMs. It’s purpose is to improve the fuzzing infrastructure of open-source projects that are already integrated into OSS-Fuzz. Given such a project, OSS-Fuzz-Gen uses its preexisting fuzzing harnesses and modifies them to produce new ones. Its architecture can be described as follows: 1. With an OSS-Fuzz project’s GitHub repository link, OSS-Fuzz-Gen iterates through a set of predefined build templates and generates potential build scripts for the project’s harnesses. 2. If any of them succeed they are once again compiled, this time through fuzz-introspector [30]. The latter constitutes a static analysis tool, with fuzzer developers specifically in mind. 3. Build results, old harness and fuzz-introspector report are included in a template-generated prompt, through which an LLM is called to generate a new harness. 4. The newly generated fuzz target is compiled and if it is done so successfully it begins execution inside OSS-Fuzz’s infrastructure.\nThis method proved meaningful, with code coverage in fuzz campaigns increasing thanks to the new generated fuzz drivers. In the case of [31], line coverage went from 38% to 69% without any manual interventions [29].\nIn 2024, OSS-Fuzz-Gen introduced an experimental feature for generating harnesses in previously unfuzzed projects [32]. The code for this feature resides in the experimental/from_scratch directory of the project’s GitHub repository [6], with the latest known working commit being 171aac2 and the latest overall commit being four months ago.\n\n\n3.1.11 AutoGen\nAutoGen [4] is a closed-source tool that generates new fuzzing harnesses, given only the library code and documentation. It works as following: The user specifies the function for which a harness is to be generated. AutoGen gathers information for this function—such as the function body, used header files, function calling examples—from the source code and documentation. Through specific prompt templates containing the above information, an LLM is tasked with generating a new fuzz driver, while another is tasked with generating a compilation command for said driver. If the compilation fails, both LLMs are called again to fix the problem, whether it was on the driver’s or command’s side. This loop iterates until a predefined maximum value or until a fuzz driver is successfully generated and compiled. If the latter is the case, it is then executed. If execution errors exist, the LLM responsible for the driver generation is used to correct them. If not, the pipeline has terminated and a new fuzz driver has been successfully generated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/03-related.html#sec-differences",
    "href": "chapters/03-related.html#sec-differences",
    "title": "3  Related work",
    "section": "3.2 Differences",
    "text": "3.2 Differences\nOverHAuL differs, in some way, with each of the aforementioned works. Firstly, although KLEE and IRIS [3], [13] tackle the problem of automated testing and both IRIS and OverHAuL can be considered neurosymbolic AI tools, the similarities end there. None of them utilize LLMs the same way we do—with KLEE not utilizing them by default, as it precedes them chronologically—and neither are automating any part of the fuzzing process.\nWhen it comes to FUDGE, FuzzGen and UTopia [8], [11], [12], all three depend on and demand existing client code and/or unit tests. On the other hand, OverHAuL requires only the bare minimum: the library code itself. Another point of difference is that in contrast with OverHAuL, these tools operate in a linear fashion. No feedback is produced or used in any step and any point failure results in the termination of the entire run.\nOverHAuL challenges a common principle of these tools, stated explicitly in FUDGE’s paper [12]: “Choosing a suitable fuzz target (still) requires a human”. OverHAuL chooses to let the LLM, instead of the user, explore the available functions and choose one to target in its fuzz driver.\nOSS-Fuzz-Gen [6] can be considered a close counterpart of OverHAuL, and in some ways it is. A lot of inspiration was gathered from it, like for example the inclusion of static analysis and its usage in informing the LLM. Yet, OSS-Fuzz-Gen has a number of disadvantages that make it in some cases an inferior option. For one, OFG is tightly coupled with the OSS-Fuzz platform [15], which even on its own creates a plethora of issues for the common developer. To integrate their project into OSS-Fuzz, they would need to: Transform it into a ClusterFuzz project [24] and take time to write harnesses for it. Even if these prerequisites are carried out, it probably would not be enough. Per OSS-Fuzz’s documentation [20]: “To be accepted to OSS-Fuzz, an open-source project must have a significant user base and/or be critical to the global IT infrastructure”. This means that OSS-Fuzz is a viable option only for a small minority of open-source developers and maintainers. One countermeasure of the above shortcoming would be for a developer to run OSS-Fuzz-Gen locally. This unfortunately proves to be an arduous task. As it is not meant to be used standalone, OFG is not packaged in the form of a self-contained application. This makes it hard to setup and difficult to use interactively. Like in the case of FUDGE, OFG’s actions are performed linearly. No feedback is utilized nor is there graceful error handling in the case of a step’s failure. Even in the case of the experimental feature for bootstrapping unfuzzed projects, OFG’s performance varies heavily. During experimentation, a lot of generated harnesses were still wrapped either in Markdown backticks or &lt;code&gt; tags, or were accompanied with explanations inside the generated .c source file. Even if code was formatted correctly, in many cases it missed necessary headers for compilation or used undeclared functions.\nLastly, the closest counterpart to OverHAuL is AutoGen [4]. Their similarity stands in the implementation of a feedback loop between LLM and generated harness. However, most other implementation decisions remain distinct. One difference regards the fuzzed function. While AutoGen requires a target function to be specified by the user in which it narrows during its whole run, OverHAuL delegates this to the LLM, letting it explore the codebase and decide by itself the best candidate. Another difference lies in the need—and the lack of—of documentation. While AutoGen requires it to gather information for the given function, OverHAuL leans into the role of a developer by reading the related code and comments and thus avoiding any mismatches between documentation and code. Finally, the LLMs’ input is built based on predefined prompt templates, a technique also present in OSS-Fuzz-Gen. OverHAuL operates one abstraction level higher, leveraging DSPy [33] for programming instead of prompting the LLMs used.\nIn conclusion, OverHAuL constitutes an open-source tool that offers new functionality by offering a straightforward installation process, packaged as a self-contained Python package with minimal external dependencies. It also introduces novel approaches compared to previous work by\n\nImplementing a feedback mechanism between harness generation, compilation, and evaluation phases,\nUsing autonomous ReAct agents capable of codebase exploration,\nLeveraging a vector store for code consumption and retrieval.\n\n\n\n\n\n[1] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[2] OpenAI, “ChatGPT,” 2025. Available: https://chatgpt.com\n\n\n[3] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[4] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272\n\n\n[5] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[6] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[7] H. Green and T. Avgerinos, “GraphFuzz: Library API fuzzing with lifetime-aware dataflow graphs,” in Proceedings of the 44th International Conference on Software Engineering, Pittsburgh Pennsylvania: ACM, May 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228. Available: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[8] B. Jeong et al., “UTopia: Automatic Generation of Fuzz Driver using Unit Tests,” in 2023 IEEE Symposium on Security and Privacy (SP), May 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394. Available: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[9] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[10] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[13] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[14] LLVM Project, “The LLVM Compiler Infrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[15] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[16] N. Sasirekha, A. Edwin Robert, and M. Hemalatha, “Program Slicing Techniques and its Applications,” IJSEA, vol. 2, no. 3, pp. 50–64, Jul. 2011, doi: 10.5121/ijsea.2011.2304. Available: http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf\n\n\n[17] M. Zhang, J. Liu, F. Ma, H. Zhang, and Y. Jiang, “IntelliGen: Automatic Driver Synthesis for FuzzTesting,” Mar. 01, 2021. doi: 10.48550/arXiv.2103.00862. Available: http://arxiv.org/abs/2103.00862\n\n\n[18] H. Xu et al., “CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph,” Dec. 20, 2024. doi: 10.48550/arXiv.2411.11532. Available: http://arxiv.org/abs/2411.11532\n\n\n[19] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[20] OSS-Fuzz, “OSS-Fuzz Documentation,” 2025. Available: https://google.github.io/oss-fuzz/\n\n\n[21] CVE Program, “CVE - CVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[22] D. Wheeler, “How to Prevent the next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[23] Blackduck, Inc., “Heartbleed Bug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[24] Google, “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[25] LLVM Project, “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[26] Google, “Google/fuzztest.” Google, Jul. 10, 2025. Available: https://github.com/google/fuzztest\n\n\n[27] Google, “Google/honggfuzz.” Google, Jul. 10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[28] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[29] D. Liu, J. Metzman, O. Chang, and G. O. S. S. Team, “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier,” Aug. 16, 2023. Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[30] Open Source Security Foundation (OpenSSF), “Ossf/fuzz-introspector.” Open Source Security Foundation (OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[31] L. Thomason, “Leethomason/Tinyxml2.” Jul. 10, 2025. Available: https://github.com/leethomason/tinyxml2\n\n\n[32] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[33] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html",
    "href": "chapters/04-overhaul.html",
    "title": "4  OverHAuL",
    "section": "",
    "text": "4.1 Architecture\nIn this thesis we present OverHAuL (Harness Automation with LLMs), a neurosymbolic AI tool that automatically generates fuzzing harnesses for C libraries through LLM agents. In its core, OverHAuL is comprised by three LLM ReAct agents [1]—each with its own responsibility and scope—and a vector store index reserving the given project’s analyzed codebase. An overview of OverHAuL’s process is presented in Figure 4.1. The objective of OverHAuL is to streamline the process of fuzz testing for C libraries. Given a link to a git repository [2] of a C library, OverHAuL automatically generates a new fuzzing harness specifically designed for the project. In addition to the harness, it produces a compilation script to facilitate building the harness, generates a representative input that can trigger crashes, and logs the output from the executed harness.\nAs commented in Section 3.2, OverHAuL does not expect and depend on the existence of client code or unit tests [3], [4], [5] nor does it require any preexisting fuzzing harnesses [6] or any documentation present [7]. Also importantly, OverHAuL is decoupled from other fuzzing projects, thus lowering the barrier to entry for new projects [6], [8]. Lastly, the user isn’t mandated to specify manually the function which the harness-to-be-generated must fuzz. Instead, OverHAuL’s agents examine and assess the provided codebase, choosing after evaluation the most optimal targeted function.\nOverHAuL utilizes autonomous ReAct agents [1] which inspect and analyze the project’s source code. The latter is stored and interacted with as a set of text embeddings [9], kept in a vector store. Both approaches are, to the best of our knowledge, novel in the field of automatic fuzzing harnesses generation. OverHAuL also implements an evaluation component that assesses in real-time all generated harnesses, making the results tenable, reproducible and well-founded. Ideally, this methodology provides a comprehensive and systematic framework for identifying previously unknown software vulnerabilities in projects that have not yet been fuzz tested.\nFinally, OverHAuL excels in its user-friendliness, as it constitutes a simple and easily-installable Python package with minimal external dependencies—only real dependency being Clang, a prevalent compiler available across all primary operating systems. This contrasts most other comparable systems, which are typically characterized by their limited documentation, lack of extensive testing, and a focus primarily on experimental functionality.1\nOverHAuL can be compartmentalized in three stages: First, the project analysis stage (Section 4.1.1), the harness creation stage (Section 4.1.2) and the harness evaluation stage (Section 4.1.3).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#sec-architecture",
    "href": "chapters/04-overhaul.html#sec-architecture",
    "title": "4  OverHAuL",
    "section": "",
    "text": "4.1.1 Project Analysis\nIn the project analysis stage (steps A.1–A.4), the project to be fuzzed is ran through a static analysis tool and is sliced into function-level chunks, which are stored in a vector store. The results of this stage are a static analysis report and a vector store containing embeddings of function-level code chunks, both of which are later available to the LLM agents.\nThe static analysis tool Flawfinder [10] is executed with the project directory as input and is responsible for the static analysis report. This report is considered a meaningful resource, since it provides the LLM agent with some starting points to explore, regarding the occurrences of potentially vulnerable functions and/or unsafe code practices.\nThe vector store is created in the following manner: The codebase is first chunked in function-level pieces by traversing the code’s Abstract Syntax Tree (AST) through Clang. Each chunk is represented by an object with the function’s signature, the corresponding filepath and the function’s body. Afterwards, each function body is turned into a vector embedding through an embedding model. Each embedding is stored in the vector store. This structure is created and used for easier and more semantically meaningful code retrieval, and to also combat context window limitations present in the LLMs.\n\n\n4.1.2 Harness Creation\nSecond is the harness creation stage (steps B.1–B.2). In this part, a “generator” ReAct LLM agent is tasked with creating a fuzzing harness for the project. The agent has access to a querying tool that acts as an interface between it and the vector store. When the agent makes queries like “functions containing strcpy()”, the querying tool turns the question into an embedding and through similarity search returns the top k=3 most similar results—in this case, functions of the project. With this approach, the agent is able to explore the codebase semantically and pinpoint potentially vulnerable usage patterns easily.\nThe harness generated by the agent is then compiled using Clang and linked with the AddressSanitizer, LeakSanitizer, and UndefinedBehaviorSanitizer. The compilation command used is generated programmatically, according to the rules described in Section 4.5.1. If the compilation fails for any reason, e.g. a missing header include, then the generated faulty harness and its compilation output are passed to a new “fixer” agent tasked with repairing any errors in the harness (step B.2.a). This results in a newly generated harness, presumably free from the previously shown flaws. This process is iterated until a compilable harness has been obtained. After success, a script is also exported in the project directory, containing the generated compilation command.\n\n\n4.1.3 Harness Evaluation\nThird comes the evaluation stage (steps C.1–C.3). During this step, the compiled harness is executed and its results evaluated. Namely, a generated harness passes the evaluation phase if and only if:\n\nThe harness has no memory leaks during its execution This is inferred by the existence of leak-&lt;hash&gt; files.\nA new testcase was created or the harness executed for at least MIN_EXECUTION_TIME (i.e. did not crash on its own) When a crash happens, and thus a testcase is created, it results in a crash-&lt;hash&gt; file.\nThe created testcase is not empty This is examined through xxd’s output given the crash-file.\n\nSimilarly to the second stage’s compilation phase (steps B.2–B.2.a), if a harness does not pass the evaluation for whatever reason it is sent to an “improver” agent. This agent is instructed to refine it based on its code and cause of failing the evaluation. This process is also iterative. If any of the improved harness versions fail to compile, the aforementioned “fixer” agent is utilized again (steps C.2–C.2.a). All produced crash files and the harness execution output are saved in the project’s directory.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#sec-techniques",
    "href": "chapters/04-overhaul.html#sec-techniques",
    "title": "4  OverHAuL",
    "section": "4.2 Main techniques",
    "text": "4.2 Main techniques\nThe fundamental techniques that distinguish OverHAuL in its approach and enhance its effectiveness in achieving its objectives are: The implementation of an iterative feedback loop between the LLM agents, the distribution of responsibility across a swarm of distinct agents and the employment of a “codebase oracle” for interacting with the given project’s source code.\n\n4.2.1 Feedback loop\nThe initial generated harness produced by OverHAuL is unlikely to be successful from the get-go. The iterative feedback loop implemented facilitates its enhancement, enabling the harness to be tested under real-world conditions and subsequently refined based on the results of these tests. This approach mirrors the typical workflow employed by developers in the process of creating and optimizing fuzz targets.\nIn this iterative framework, the development process continues until either an acceptable and functional harness is realized or the defined iteration budget is exhausted. The iteration budget N=10 is initialized at the onset of OverHAuL’s execution and is shared between both the compilation and evaluation phases of the harness development process. This means that the iteration budget is decremented each time a dashed arrow in the flowchart illustrated in Figure 4.1 is followed. Such an approach allows for targeted improvements while maintaining oversight of resource allocation throughout the harness development cycle.\n\n\n4.2.2 ReAct agents swarm\nAn integral design decision in our framework is the implementation of each agent as a distinct LLM instance, although all utilizing the same underlying model. This approach yields several advantages, particularly in the context of maintaining separate and independent contexts for each agent throughout each OverHAuL run.\nBy assigning individual contexts to the agents, we enable a broader exploration of possibilities during each run. For instance, the “improver” agent can investigate alternative pathways or strategies that the “generator” agent may have potentially overlooked or internally deemed inadequate inaccurately. This separation not only fosters a more diverse range of solutions but also enhances the overall robustness of the system by allowing for iterative refinement based on each agent’s unique insights.\nFurthermore, this design choice effectively addresses the limitations imposed by context window sizes. By distributing the “cognitive” load across multiple agents, we can manage and mitigate the risks associated with exceeding these constraints. As a result, this architecture promotes efficient utilization of available resources while maximizing the potential for innovative outcomes in multi-agent interactions. This layered approach ultimately contributes to a more dynamic and exploratory research environment, facilitating a comprehensive examination of the problem space.\n\n\n4.2.3 Codebase oracle\nThe third central technique employed is the creation and utilization of a codebase oracle, which is effectively realized through a vector store. This oracle is designed to contain the various functions within the project, enabling it to return the most semantically similar functions upon querying it. Such an approach serves to address the inherent challenges associated with code exploration difficulties faced by LLM agents, particularly in relation to their limited context window.\nBy structuring the codebase into chunks at the level of individual functions, LLM agents can engage with the code more effectively by focusing on its functional components. This methodology not only allows for a more nuanced understanding of the codebase but also ensures that the responses generated do not consume an excessive portion of the limited context window available to the agents. In contrast, if the codebase were organized and queried at the file level, the chunks of information would inevitably become larger, leading to an increase in noise and a dilution of meaningful content in each chunk [11]. Given the constant size of the embeddings used in processing, each progressively larger chunk would be less semantically significant, ultimately compromising the quality of the retrieval process.\nDefining the function as the primary unit of analysis represents the most proportionate balance between the size of the code segments and their semantic significance. It serves as the ideal “zoom-in” level for the exploration of code, allowing for greater clarity and precision in understanding the functionality of individual code segments. This same principle is widely recognized in the training of code-specific LLMs, where a function-level approach has been shown to enhance performance and comprehension [12]. By adopting this methodology, we aim to foster a more robust interaction between LLM agents and the underlying codebase, ultimately facilitating a more effective and efficient exploration process.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#high-level-algorithm",
    "href": "chapters/04-overhaul.html#high-level-algorithm",
    "title": "4  OverHAuL",
    "section": "4.3 High-Level Algorithm",
    "text": "4.3 High-Level Algorithm\nA pseudocode version of OverHAuL’s main function can be seen in Algorithm 1. It represents the workflow presented in Figure 4.1 and uses the techniques described in sections 4.1 and 4.2. It is important to emphasize that, within the context of this algorithm, the Harnesser() function serves as an interface that bridges the “generator”, “fixer” and “improver” LLM agents. The agent that is used upon each function call depends on the values of the function’s arguments. This results in the harness variable representing all generated, fixed or improved harnesses. This approach is adopted for making the abstract algorithm simpler and easier to understand.\n\n\n\\begin{algorithm} \\caption{OverHAuL}\\begin{algorithmic} \\Require $repository$ \\Ensure $harness, compilation\\_script, crash\\_input, execution\\_log$ \\State $path \\gets$ \\Call{RepoClone}{repository} \\State $report \\gets$ \\Call{StaticAnalysis}{$path$} \\State $vector\\_store \\gets$ \\Call{CreateOracle}{$path$} \\State $acceptable \\gets$ False \\State $compiled \\gets$ False \\State $error \\gets$ None \\State $violation \\gets$ None \\State $output \\gets$ None \\For{$i = 1$ to $MAX\\_ITERATIONS$} \\State $harness \\gets$ \\Call{Harnesser}{$path, report, vector\\_store, error, violation, output$} \\State $error, compiled \\gets$ \\Call{BuildHarness}{$harness$} \\If{$\\neg compiled$} \\State \\textbf{continue} \\Comment{Regenerate harness} \\EndIf \\State $output, accepted \\gets $\\Call{EvaluateHarness}{$harness$} \\If{$\\neg accepted$} \\State \\textbf{continue} \\Comment{Improve harness} \\Else \\State $acceptable \\gets$ True \\State \\textbf{break} \\EndIf \\EndFor \\State \\Return $compiled \\land acceptable$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#examples",
    "href": "chapters/04-overhaul.html#examples",
    "title": "4  OverHAuL",
    "section": "4.4 Examples",
    "text": "4.4 Examples\n\n\n\n\n\n\nFigure 4.2: A successful execution of OverHAuL, harnessing the dateparse library.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#scope",
    "href": "chapters/04-overhaul.html#scope",
    "title": "4  OverHAuL",
    "section": "4.5 Scope",
    "text": "4.5 Scope\n\nLimited to C libraries\nExpects relatively simple project structure\n\nCode either in root or in common-named subdirs (e.g. src/)\nAny file or directory with main, test or example substring is ignored\nNo main() function, or only exists in some file that is ignored by the above\n\nBuild systems not supported\n\nHarness is compiled with a predefined command\n\n\n\n4.5.1 Assumptions/Prerequisites\n\nProject structure\nfile/folder naming\nbuilding process",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#abandoned-techniques",
    "href": "chapters/04-overhaul.html#abandoned-techniques",
    "title": "4  OverHAuL",
    "section": "4.6 Abandoned techniques",
    "text": "4.6 Abandoned techniques\n\nZero-shot harness generation\nChainOfThought modules for LLM instances [13]\nNaive source code concatenation\nmanual {index, read}_tool usage for ReAct agents\n\n\n\n\n\n[1] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[2] L. Torvalds, “Git.” Apr. 07, 2005. Available: https://git-scm.com/\n\n\n[3] B. Jeong et al., “UTopia: Automatic Generation of Fuzz Driver using Unit Tests,” in 2023 IEEE Symposium on Security and Privacy (SP), May 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394. Available: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[4] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[5] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[6] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[7] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272\n\n\n[8] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[9] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient Estimation of Word Representations in Vector Space,” Sep. 06, 2013. doi: 10.48550/arXiv.1301.3781. Available: http://arxiv.org/abs/1301.3781\n\n\n[10] D. A. Wheeler, “Flawfinder Home Page.” Available: https://dwheeler.com/flawfinder/\n\n\n[11] S. Zhao, Y. Yang, Z. Wang, Z. He, L. K. Qiu, and L. Qiu, “Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely,” Sep. 23, 2024. doi: 10.48550/arXiv.2409.14924. Available: http://arxiv.org/abs/2409.14924\n\n\n[12] M. Chen et al., “Evaluating Large Language Models Trained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374. Available: http://arxiv.org/abs/2107.03374\n\n\n[13] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/05-evaluation.html",
    "href": "chapters/05-evaluation.html",
    "title": "5  Evaluation",
    "section": "",
    "text": "5.1 Research questions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-evaluation.html#benchmarks",
    "href": "chapters/05-evaluation.html#benchmarks",
    "title": "5  Evaluation",
    "section": "5.2 Benchmarks",
    "text": "5.2 Benchmarks\n10 open-source C/C++ projects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-evaluation.html#performance",
    "href": "chapters/05-evaluation.html#performance",
    "title": "5  Evaluation",
    "section": "5.3 Performance",
    "text": "5.3 Performance",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-evaluation.html#issues",
    "href": "chapters/05-evaluation.html#issues",
    "title": "5  Evaluation",
    "section": "5.4 Issues",
    "text": "5.4 Issues",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-evaluation.html#future-work",
    "href": "chapters/05-evaluation.html#future-work",
    "title": "5  Evaluation",
    "section": "5.5 Future Work",
    "text": "5.5 Future Work\n\n5.5.1 Technical Future Work\n\n\n5.5.2 Architectural Future Work/Extensions\n\nBuild system\nMore (static) analysis tolls integrations\nGeneral localization problem",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/06-results.html",
    "href": "chapters/06-results.html",
    "title": "6  Results",
    "section": "",
    "text": "Results from integration with 10/100 open-source C/C++ projects.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/07-implementation.html",
    "href": "chapters/07-implementation.html",
    "title": "7  Implementation",
    "section": "",
    "text": "7.1 Development environment\n–depth 1 output/\nembedder model openai Source code is processed and chunked using Clang [33]. The chunks are function-level units, found to be a sweet-spot between semantic significance and size [1], [2]. This results in a list of Python dicts, each containing a function’s body, signature and filepath. Each chunk’s function code is then turned into an embedding using OpenAI’s “text-embedding-3-small” model. faiss store and index A FAISS [36] vector store is created. Each function embedding is stored in it (with the same order, as to correspond with the previous list containing the metadata).\nsame order code chunks\nPrompting techniques used (callback to Section 2.2.2). Sample prompt\n[3]\nlibclang Python package\nuv, ruff, mypy, pytest, pdoc",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "chapters/07-implementation.html#equipment",
    "href": "chapters/07-implementation.html#equipment",
    "title": "7  Implementation",
    "section": "7.2 Equipment",
    "text": "7.2 Equipment\ndesktop pc cpu, memory",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "chapters/07-implementation.html#models-used",
    "href": "chapters/07-implementation.html#models-used",
    "title": "7  Implementation",
    "section": "7.3 models used",
    "text": "7.3 models used\ngpt-4.1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "chapters/07-implementation.html#reproducibility",
    "href": "chapters/07-implementation.html#reproducibility",
    "title": "7  Implementation",
    "section": "7.4 Reproducibility",
    "text": "7.4 Reproducibility\ngithub workflow actions, artifacts, summary\n\n\n\n\n[1] S. Zhao, Y. Yang, Z. Wang, Z. He, L. K. Qiu, and L. Qiu, “Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely,” Sep. 23, 2024. doi: 10.48550/arXiv.2409.14924. Available: http://arxiv.org/abs/2409.14924\n\n\n[2] M. Chen et al., “Evaluating Large Language Models Trained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374. Available: http://arxiv.org/abs/2107.03374\n\n\n[3] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[4] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[5] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Implementation</span>"
    ]
  },
  {
    "objectID": "chapters/08-future.html",
    "href": "chapters/08-future.html",
    "title": "8  Future Work",
    "section": "",
    "text": "8.1 Enhancements to Core Features\nThe prototype implementation of OverHAuL offers a compelling demonstration of its potential to automate the fuzzing process for open-source libraries, providing tangible benefits to developers and maintainers alike. This initial version successfully validates the core design principles underpinning OverHAuL, showcasing its ability to streamline and enhance the software testing workflow through automated generation of fuzz drivers using large language models. Nevertheless, while these foundational capabilities lay a solid groundwork, numerous avenues exist for further expansion, refinement, and rigorous evaluation to fully realize the tool’s potential and adapt to evolving challenges in software quality assurance.\nEnhancing OverHAuL’s core functionality represents a primary direction for future development. First, expanding support to encompass a wider array of build systems commonly employed in C and C++ projects—such as GNU Make, CMake, Meson, and Ninja [1], [2], [3], [4]—would significantly broaden the scope of libraries amenable to automated fuzzing using OverHAuL. This advancement would enable OverHAuL to scale effectively and be applied to larger, more complex codebases, thereby increasing its practical utility and impact.\nSecond, integrating additional fuzzing engines beyond LibFuzzer stands out as a strategic enhancement. Incorporation of widely adopted fuzzers like AFL++ [5] could diversify the fuzzing strategies available to OverHAuL, while exploring more experimental tools such as GraphFuzz [6] may pioneer specialized approaches for certain code patterns or architectures. Multi-engine support would also facilitate extending language coverage, for instance by incorporating fuzzers tailored to other programming ecosystems—for example, Google’s Atheris for Python projects [7]. Such versatility would position OverHAuL as a more universal fuzzing automation platform.\nThird, the evaluation component of OverHAuL presents an opportunity for refinement through more sophisticated analysis techniques. Beyond the current criteria, incorporating dynamic metrics such as differential code coverage tracking between generated fuzz harnesses would yield deeper insights into test quality and coverage completeness. This quantitative evaluation could guide iterative improvements in fuzz driver generation and overall testing effectiveness.\nFinally, OverHAuL’s methodology could be extended to leverage existing client codebases and unit tests in addition to the library source code itself, resources that for now OverHAuL leaves untapped. Inspired by approaches like those found in FUDGE and FuzzGen [8], [9], this enhancement would enable the tool to exploit programmer-written usage scenarios as seeds or contexts, potentially generating more meaningful and targeted fuzz inputs. Incorporating these richer information sources would likely improve the efficacy of fuzzing campaigns and uncover subtler bugs.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/08-future.html#experimentation-with-large-language-models-and-data-representation",
    "href": "chapters/08-future.html#experimentation-with-large-language-models-and-data-representation",
    "title": "8  Future Work",
    "section": "8.2 Experimentation with Large Language Models and Data Representation",
    "text": "8.2 Experimentation with Large Language Models and Data Representation\nOverHAuL’s reliance on large language models (LLMs) invites comprehensive experimentation with different providers and architectures to assess their comparative strengths and limitations. Conducting empirical evaluations across leading models—such as OpenAI’s o1 and o3 families and Anthropic’s Claude Opus 4—will provide valuable insights into their capabilities, cost-efficiency, and suitability for fuzz driver synthesis. Additionally, specialized code-focused LLMs, including generative and fill-in models like Codex-1 and CodeGen [10], [11], [12], merit exploration due to their targeted optimization for source code generation and understanding.\nAnother dimension worthy of investigation concerns the granularity of code chunking employed during the given project’s code processing stage. Whereas the current approach partitions code at the function level, experimenting with more nuanced segmentation strategies—such as splitting per step inside a function, as a finer-grained technique—could improve the semantic coherence of stored representations and enhance retrieval relevance during fuzz driver generation. This line of inquiry has the potential to optimize model input preparation and ultimately improve output quality.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/08-future.html#comprehensive-evaluation-and-benchmarking",
    "href": "chapters/08-future.html#comprehensive-evaluation-and-benchmarking",
    "title": "8  Future Work",
    "section": "8.3 Comprehensive Evaluation and Benchmarking",
    "text": "8.3 Comprehensive Evaluation and Benchmarking\nTo thoroughly establish OverHAuL’s effectiveness, extensive large-scale evaluation beyond the initial 10-project corpus is imperative. Applying the tool to repositories indexed in the clib package manager [13], which encompasses hundreds of C libraries, would test scalability and robustness across diverse real-world settings. Such a broad benchmark would also enable systematic comparisons against state-of-the-art automated fuzzing frameworks like OSS-Fuzz-Gen and AutoGen, elucidating OverHAuL’s relative strengths and identifying areas for improvement [14], [15].\nComplementing broad benchmarking, detailed ablation studies dissecting the contributions of individual pipeline components and algorithmic choices will yield critical insights into what drives OverHAuL’s performance. Understanding the impact of each module will guide targeted optimizations and support evidence-based design decisions.\nFurthermore, an economic analysis exploring resource consumption—such as API token usage and associated monetary costs—relative to fuzzing effectiveness would be valuable for assessing the practical viability of integrating LLM-based fuzz driver generation into continuous integration processes.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/08-future.html#practical-deployment-and-community-engagement",
    "href": "chapters/08-future.html#practical-deployment-and-community-engagement",
    "title": "8  Future Work",
    "section": "8.4 Practical Deployment and Community Engagement",
    "text": "8.4 Practical Deployment and Community Engagement\nFrom a usability perspective, embedding OverHAuL within a GitHub Actions workflow represents a practical and impactful enhancement, enabling seamless integration with developers’ existing toolchains and continuous integration pipelines. This would promote wider adoption by reducing barriers to entry and fostering real-time feedback during code development cycles.\nAdditionally, establishing a mechanism to generate and submit automated pull requests (PRs) to the maintainers of fuzzed libraries—highlighting detected bugs and proposing patches—would not only validate OverHAuL’s findings but also contribute tangible improvements to open-source software quality. This collaborative feedback loop epitomizes the symbiosis between automated testing tools and the open-source community. As an initial step, developing targeted PRs for the projects where bugs were discovered during OverHAuL’s development would help facilitate practical follow-up and improvements.\n\n\n\n\n[1] A. Cedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf, “CMake - Upgrade Your Software Build System.” 2000. Available: https://cmake.org/\n\n\n[2] S. I. Feldman, “Make — a program for maintaining computer programs,” Software: Practice and Experience, vol. 9, no. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[3] E. Martin, “Ninja-build/ninja.” ninja-build, Jul. 14, 2025. Available: https://github.com/ninja-build/ninja\n\n\n[4] J. Pakkanen, “Mesonbuild/meson.” The Meson Build System, Jul. 14, 2025. Available: https://github.com/mesonbuild/meson\n\n\n[5] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[6] H. Green and T. Avgerinos, “GraphFuzz: Library API fuzzing with lifetime-aware dataflow graphs,” in Proceedings of the 44th International Conference on Software Engineering, Pittsburgh Pennsylvania: ACM, May 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228. Available: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[7] Google, “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[8] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[9] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[10] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “CodeGen2: Lessons for training llms on programming and natural languages,” ICLR, 2023.\n\n\n[11] E. Nijkamp et al., “CodeGen: An open large language model for code with multi-turn program synthesis,” ICLR, 2023.\n\n\n[12] OpenAI, “Introducing Codex,” May 16, 2025. Available: https://openai.com/index/introducing-codex/\n\n\n[13] Clibs Project, “Clib Packages,” 2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[14] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[15] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/09-discussion.html",
    "href": "chapters/09-discussion.html",
    "title": "9  Discussion",
    "section": "",
    "text": "more powerful llms -&gt; better results\nopen source libraries might have been in the training data results for closed source libraries could be worse this could be mitigated with llm fine-tuning",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/10-conclusion.html",
    "href": "chapters/10-conclusion.html",
    "title": "10  Conclusion",
    "section": "",
    "text": "Recap\nPresented the algorithm and the implementation.\ngenerative AI disclaimer à la ACM?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/refs.html",
    "href": "chapters/refs.html",
    "title": "Bibliography",
    "section": "",
    "text": "[1] M.\nZalewski, “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[2] M.\nHeuse, H. Eißfeldt, A. Fioraldi, and D. Maier,\n“AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[3] Google, “Google/atheris.” Google,\nApr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[4] D.\nBahdanau, K. Cho, and Y. Bengio, “Neural Machine\nTranslation by Jointly Learning to\nAlign and Translate,” May 19, 2016.\ndoi: 10.48550/arXiv.1409.0473.\nAvailable: http://arxiv.org/abs/1409.0473\n\n\n[5] GNU\nProject, “Bash - GNU Project - Free Software\nFoundation.” Available: https://www.gnu.org/software/bash/\n\n\n[6] F.\nBellard, P. Maydell, and QEMU Team, “QEMU.”\nMay 29, 2025. Available: https://www.qemu.org/\n\n\n[7] T.\nB. Brown et al., “Language Models are\nFew-Shot Learners,” Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165.\nAvailable: http://arxiv.org/abs/2005.14165\n\n\n[8] A.\nCedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf,\n“CMake - Upgrade Your Software Build\nSystem.” 2000. Available: https://cmake.org/\n\n\n[9] J.\nWei et al., “Chain-of-Thought Prompting Elicits\nReasoning in Large Language Models,” Jan. 10,\n2023. doi: 10.48550/arXiv.2201.11903.\nAvailable: http://arxiv.org/abs/2201.11903\n\n\n[10] OpenAI, “ChatGPT,”\n2025. Available: https://chatgpt.com\n\n\n[11] M.\nChen et al., “Evaluating Large Language Models\nTrained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374.\nAvailable: http://arxiv.org/abs/2107.03374\n\n\n[12] Anthropic, “Claude,” 2025.\nAvailable: https://claude.ai/new\n\n\n[13] Clibs Project, “Clib\nPackages,” 2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[14] Google, “Google/clusterfuzz.”\nGoogle, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[15] Anysphere, “Cursor - The AI Code\nEditor,” 2025. Available: https://cursor.com/\n\n\n[16] DeepSeek-AI et al.,\n“DeepSeek-R1: Incentivizing Reasoning\nCapability in LLMs via Reinforcement\nLearning,” Jan. 22, 2025. doi: 10.48550/arXiv.2501.12948.\nAvailable: http://arxiv.org/abs/2501.12948\n\n\n[17] J.\nDevlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of Deep Bidirectional\nTransformers for Language Understanding,” May\n24, 2019. doi: 10.48550/arXiv.1810.04805.\nAvailable: http://arxiv.org/abs/1810.04805\n\n\n[18] O.\nKhattab et al., “DSPy: Compiling\nDeclarative Language Model Calls into Self-Improving\nPipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714.\nAvailable: http://arxiv.org/abs/2310.03714\n\n\n[19] S.\nI. Feldman, “Make — a program for maintaining computer\nprograms,” Software: Practice and Experience, vol. 9,\nno. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402.\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[20] D.\nA. Wheeler, “Flawfinder Home Page.” Available:\nhttps://dwheeler.com/flawfinder/\n\n\n[21] D.\nBabić et al., “FUDGE: Fuzz driver generation\nat scale,” in Proceedings of the 2019 27th ACM Joint\nMeeting on European Software Engineering Conference\nand Symposium on the Foundations of\nSoftware Engineering, Tallinn Estonia: ACM, Aug. 2019,\npp. 975–985. doi: 10.1145/3338906.3340456.\nAvailable: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[22] Open Source Security Foundation (OpenSSF),\n“Ossf/fuzz-introspector.” Open Source Security Foundation\n(OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[23] K.\nIspoglou, D. Austin, V. Mohan, and M. Payer,\n“FuzzGen: Automatic fuzzer\ngeneration,” in 29th USENIX Security Symposium\n(USENIX Security 20), 2020, pp. 2271–2287. Available:\nhttps://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[24] Y.\nDeng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,\n“Large Language Models are Edge-Case\nFuzzers: Testing Deep Learning Libraries via\nFuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014.\nAvailable: http://arxiv.org/abs/2304.02014\n\n\n[25] Google, “Google/fuzztest.” Google,\nJul. 10, 2025. Available: https://github.com/google/fuzztest\n\n\n[26] D.\nGanguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of\nThought : Neurosymbolic Program Synthesis\nallows Robust and Interpretable\nReasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270.\nAvailable: http://arxiv.org/abs/2409.17270\n\n\n[27] A.\nd’Avila Garcez and L. C. Lamb, “Neurosymbolic AI:\nThe 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876.\nAvailable: http://arxiv.org/abs/2012.05876\n\n\n[28] M.\nGaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI\nSystems: Consistency, Reliability,\nExplainability, and Safety,” Dec. 05,\n2023. doi: 10.48550/arXiv.2312.06798.\nAvailable: http://arxiv.org/abs/2312.06798\n\n\n[29] Google, “‎Google\nGemini,” 2025. Available: https://gemini.google.com\n\n\n[30] Microsoft, “GitHub Copilot ·\nYour AI pair programmer,” 2025. Available: https://github.com/features/copilot\n\n\n[31] A.\nGrattafiori et al., “The Llama 3\nHerd of Models,” Nov. 23, 2024. doi: 10.48550/arXiv.2407.21783.\nAvailable: http://arxiv.org/abs/2407.21783\n\n\n[32] H.\nGreen and T. Avgerinos, “GraphFuzz: Library\nAPI fuzzing with lifetime-aware dataflow graphs,” in\nProceedings of the 44th International Conference on\nSoftware Engineering, Pittsburgh Pennsylvania: ACM,\nMay 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228.\nAvailable: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[33] Blackduck, Inc., “Heartbleed\nBug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[34] CVE Program, “CVE -\nCVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[35] Google, “Google/honggfuzz.”\nGoogle, Jul. 10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[36] Z.\nLi, S. Dutta, and M. Naik, “IRIS: LLM-Assisted\nStatic Analysis for Detecting Security\nVulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238.\nAvailable: http://arxiv.org/abs/2405.17238\n\n\n[37] H.\nKautz, “The Third AI Summer,” presented at the\n34th Annual Meeting of the Association for the\nAdvancement of Artificial Intelligence, Feb.\n10, 2020. Available: https://www.youtube.com/watch?v=_cQITY0SPiw\n\n\n[38] C.\nCadar, D. Dunbar, and D. Engler, “KLEE:\nUnassisted and Automatic Generation of\nHigh-Coverage Tests for Complex Systems\nPrograms,” presented at the USENIX Symposium\non Operating Systems Design and\nImplementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[39] H.\nChase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[40] P.\nLewis et al., “Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP\nTasks,” Apr. 12, 2021. doi: 10.48550/arXiv.2005.11401.\nAvailable: http://arxiv.org/abs/2005.11401\n\n\n[41] H.\nLi, “Language models: Past, present, and future,”\nCommun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available:\nhttps://dl.acm.org/doi/10.1145/3490443\n\n\n[42] LLVM Project, “libFuzzer – a library for coverage-guided fuzz\ntesting. — LLVM 21.0.0git documentation,” 2025.\nAvailable: https://llvm.org/docs/LibFuzzer.html\n\n\n[43] D.\nLiu, J. Metzman, O. Chang, and G. O. S. S. Team, “AI-Powered\nFuzzing: Breaking the Bug Hunting\nBarrier,” Aug. 16, 2023. Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[44] J.\nLiu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234.\nAvailable: https://github.com/jerryjliu/llama_index\n\n\n[45] LLVM Project, “The LLVM Compiler\nInfrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[46] Y.\nLyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing\nfor Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677.\nAvailable: http://arxiv.org/abs/2312.17677\n\n\n[47] V.\nJ. M. Manes et al., “The Art,\nScience, and Engineering of\nFuzzing: A Survey,” Apr. 07, 2019. doi:\n10.48550/arXiv.1812.00140.\nAvailable: http://arxiv.org/abs/1812.00140\n\n\n[48] E.\nMartin, “Ninja-build/ninja.” ninja-build, Jul. 14, 2025.\nAvailable: https://github.com/ninja-build/ninja\n\n\n[49] T.\nMikolov, K. Chen, G. Corrado, and J. Dean, “Efficient\nEstimation of Word Representations in\nVector Space,” Sep. 06, 2013. doi: 10.48550/arXiv.1301.3781.\nAvailable: http://arxiv.org/abs/1301.3781\n\n\n[50] B.\nP. Miller, L. Fredriksen, and B. So, “An empirical study of the\nreliability of UNIX utilities,” Commun.\nACM, vol. 33, no. 12, pp. 32–44, Dec. 1990, doi: 10.1145/96267.96279.\nAvailable: https://dl.acm.org/doi/10.1145/96267.96279\n\n\n[51] E.\nNijkamp et al., “CodeGen: An\nopen large language model for code with multi-turn program\nsynthesis,” ICLR, 2023.\n\n\n[52] E.\nNijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\n“CodeGen2: Lessons for training llms on\nprogramming and natural languages,” ICLR, 2023.\n\n\n[53] OpenAI et al., “GPT-4\nTechnical Report,” Mar. 04, 2024. doi: 10.48550/arXiv.2303.08774.\nAvailable: http://arxiv.org/abs/2303.08774\n\n\n[54] OpenAI, “Introducing\nCodex,” May 16, 2025. Available: https://openai.com/index/introducing-codex/\n\n\n[55] A.\nArya, O. Chang, J. Metzman, K. Serebryany, and D. Liu,\n“OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[56] D.\nLiu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target\ngeneration.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[57] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed\nprojects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[58] OSS-Fuzz, “OSS-Fuzz\nDocumentation,” 2025. Available: https://google.github.io/oss-fuzz/\n\n\n[59] OWASP Foundation, “Fuzzing.”\nAvailable: https://owasp.org/www-community/Fuzzing\n\n\n[60] J.\nPakkanen, “Mesonbuild/meson.” The Meson Build System, Jul.\n14, 2025. Available: https://github.com/mesonbuild/meson\n\n\n[61] N.\nPerry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users\nWrite More Insecure Code with AI Assistants?”\nDec. 18, 2023. doi: 10.48550/arXiv.2211.03622.\nAvailable: http://arxiv.org/abs/2211.03622\n\n\n[62] D.\nWang, G. Zhou, L. Chen, D. Li, and Y. Miao,\n“ProphetFuzz: Fully Automated Prediction\nand Fuzzing of High-Risk Option Combinations\nwith Only Documentation via Large Language\nModel,” Sep. 01, 2024. doi: 10.1145/3658644.3690231.\nAvailable: http://arxiv.org/abs/2409.00922\n\n\n[63] A.\nRadford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” 2018,\nAvailable: https://www.mikecaptain.com/resources/pdf/GPT-1.pdf\n\n\n[64] A.\nRadford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,”\nOpenAI blog, vol. 1, no. 8, p. 9, 2019, Available: https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf\n\n\n[65] N.\nRathaus and G. Evron, Open source fuzzing tools. Burlington,\nMA: Syngress Pub, 2007.\n\n\n[66] S.\nYao et al., “ReAct: Synergizing\nReasoning and Acting in Language\nModels,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629.\nAvailable: http://arxiv.org/abs/2210.03629\n\n\n[67] A.\nRebert et al., “Optimizing seed selection for\nfuzzing,” in Proceedings of the 23rd USENIX\nconference on Security Symposium, in\nSEC’14. USA: USENIX Association, Aug. 2014, pp.\n861–875.\n\n\n[68] J.\nSaarinen, “Further flaws render Shellshock patch\nineffective,” Sep. 29, 2014. Available: https://www.itnews.com.au/news/further-flaws-render-shellshock-patch-ineffective-396256\n\n\n[69] A.\nSarkar and I. Drosos, “Vibe coding: Programming through\nconversation with artificial intelligence,” Jun. 29, 2025. doi:\n10.48550/arXiv.2506.23253.\nAvailable: http://arxiv.org/abs/2506.23253\n\n\n[70] M.\nK. Sarker, L. Zhou, A. Eberhart, and P. Hitzler, “Neuro-symbolic\nartificial intelligence: Current trends,”\nAIC, vol. 34, no. 3, pp. 197–209, Mar. 2022, doi: 10.3233/aic-210084.\nAvailable: https://journals.sagepub.com/doi/full/10.3233/AIC-210084\n\n\n[71] N.\nSasirekha, A. Edwin Robert, and M. Hemalatha, “Program\nSlicing Techniques and its\nApplications,” IJSEA, vol. 2, no. 3, pp.\n50–64, Jul. 2011, doi: 10.5121/ijsea.2011.2304.\nAvailable: http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf\n\n\n[72] K.\nSerebryany, D. Bruening, A. Potapenko, and D. Vyukov,\n“AddressSanitizer: A fast address sanity\nchecker,” in 2012 USENIX annual technical\nconference (USENIX ATC 12), 2012, pp. 309–318.\nAvailable: https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany\n\n\n[73] A.\nSheth, K. Roy, and M. Gaur, “Neurosymbolic AI –\nWhy, What, and How,” May\n01, 2023. doi: 10.48550/arXiv.2305.00813.\nAvailable: http://arxiv.org/abs/2305.00813\n\n\n[74] T.\nSimonite, “This Bot Hunts Software Bugs for the\nPentagon,” Wired, Jun. 01, 2020. Available:\nhttps://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[75] Y.\nSun, “Automated Generation and\nCompilation of Fuzz Driver Based on\nLarge Language Models,” in Proceedings of the\n2024 9th International Conference on Cyber\nSecurity and Information Engineering, in\nICCSIE ’24. New York, NY, USA: Association for Computing\nMachinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272.\nAvailable: https://doi.org/10.1145/3689236.3689272\n\n\n[76] M.\nSutton, A. Greene, and P. Amini, Fuzzing: Brute force vulnerabilty\ndiscovery. Upper Saddle River, NJ: Addison-Wesley, 2007.\n\n\n[77] A.\nTakanen, J. DeMott, C. Miller, and A. Kettunen, Fuzzing for software\nsecurity testing and quality assurance, Second edition. in\nInformation security and privacy library. Boston London Norwood, MA:\nArtech House, 2018.\n\n\n[78] The OpenSSL Project,\n“Openssl/openssl.” OpenSSL, Jul. 15, 2025. Available: https://github.com/openssl/openssl\n\n\n[79] L.\nThomason, “Leethomason/Tinyxml2.” Jul. 10, 2025. Available:\nhttps://github.com/leethomason/tinyxml2\n\n\n[80] D.\nTilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic\nAI approach to Attribution in Large\nLanguage Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726.\nAvailable: http://arxiv.org/abs/2410.03726\n\n\n[81] Y.\nDeng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large\nLanguage Models Are Zero-Shot Fuzzers: Fuzzing\nDeep-Learning Libraries via Large Language\nModels,” in Proceedings of the 32nd ACM SIGSOFT\nInternational Symposium on Software Testing and\nAnalysis, in ISSTA 2023. New York, NY,\nUSA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi:\n10.1145/3597926.3598067.\nAvailable: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[82] L.\nTorvalds, “Git.” Apr. 07, 2005. Available: https://git-scm.com/\n\n\n[83] Unicorn Engine,\n“Unicorn-engine/unicorn.” Unicorn Engine, Jul. 15, 2025.\nAvailable: https://github.com/unicorn-engine/unicorn\n\n\n[84] B.\nJeong et al., “UTopia: Automatic\nGeneration of Fuzz Driver using Unit\nTests,” in 2023 IEEE Symposium on\nSecurity and Privacy (SP),\nMay 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394.\nAvailable: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[85] A.\nVaswani et al., “Attention Is All You\nNeed,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762.\nAvailable: http://arxiv.org/abs/1706.03762\n\n\n[86] Z.\nWang, Z. Chu, T. V. Doan, S. Ni, M. Yang, and W. Zhang, “History,\ndevelopment, and principles of large language models: An introductory\nsurvey,” AI Ethics, vol. 5, no. 3, pp. 1955–1971, Jun.\n2025, doi: 10.1007/s43681-024-00583-7.\nAvailable: https://doi.org/10.1007/s43681-024-00583-7\n\n\n[87] D.\nWheeler, “How to Prevent the next\nHeartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[88] H.\nXu et al., “CKGFuzzer: LLM-Based Fuzz\nDriver Generation Enhanced By Code Knowledge Graph,” Dec.\n20, 2024. doi: 10.48550/arXiv.2411.11532.\nAvailable: http://arxiv.org/abs/2411.11532\n\n\n[89] S.\nYao et al., “Tree of Thoughts:\nDeliberate Problem Solving with Large Language\nModels,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601.\nAvailable: http://arxiv.org/abs/2305.10601\n\n\n[90] M.\nZhang, J. Liu, F. Ma, H. Zhang, and Y. Jiang,\n“IntelliGen: Automatic Driver Synthesis\nfor FuzzTesting,” Mar. 01, 2021. doi: 10.48550/arXiv.2103.00862.\nAvailable: http://arxiv.org/abs/2103.00862\n\n\n[91] S.\nZhao, Y. Yang, Z. Wang, Z. He, L. K. Qiu, and L. Qiu, “Retrieval\nAugmented Generation (RAG) and\nBeyond: A Comprehensive Survey on\nHow to Make your LLMs use\nExternal Data More Wisely,” Sep. 23, 2024. doi: 10.48550/arXiv.2409.14924.\nAvailable: http://arxiv.org/abs/2409.14924",
    "crumbs": [
      "Bibliography"
    ]
  }
]