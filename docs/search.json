[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OverHAuL",
    "section": "",
    "text": "Preface\nThis thesis was prepared in Athens, Greece, during the academic year 2024–2025, fulfilling a requirement for the Bachelor of Science degree at the Department of Informatics and Telecommunications of the National and Kapodistrian University of Athens. The research presented herein was carried out under the supervision of Prof. Thanassis Avgerinos and in accordance with the guidelines stipulated by the department. All processes and methodologies adopted during the research adhere to the academic and ethical standards of the university. The final version of this thesis is hosted online and is also archived in the department’s records, made publicly accessible through the university’s digital repository Pergamos.\n\n\n\n\nAcknowledgments\nI would like to express my gratitude to my supervisor, Prof. Thanassis Avgerinos, for his insightful guidance, patience, and unwavering encouragement throughout this journey. His openness and our shared passion for the subject greatly enhanced my enjoyment of the thesis process.\nI am also thankful to my fellow group members in Prof. Avgerinos’ weekly meetings, whose willingness to exchange ideas and offer support was invaluable. My appreciation extends to Jorgen and Phaedon, friends who provided thoughtful input and advice along the way.\nA special thank you goes to my parents Giannis and Gianna, Christina, and my friends for their constant support and understanding. Their patience and encouragement helped me persevere through this challenging period.\n\n\n\n\n\n\nCitation\n\n\n\nBibTeX citation:\n\n@thesis{chousos2025,\n  title = {{OverHAuL}: Harnessing Automation for C Libraries with Large Language Models},\n  shorttitle = {{OverHAuL}},\n  author = {Chousos, Konstantinos},\n  date = {2025-07-27},\n  institution = {{National and Kapodistrian University of Athens}},\n  location = {Athens, Greece},\n  url = {https://kchousos.github.io/BSc-Thesis/},\n  langid = {en}\n}\n\nFor attribution, please cite this work as:\n\n\nK. Chousos, “OverHAuL: Harnessing Automation for C Libraries with Large Language Models,” Bachelor Thesis, National and Kapodistrian University of Athens, Athens, Greece, 2025. [Online]. Available: https://kchousos.github.io/BSc-Thesis/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Thesis Structure\nModern society’s reliance on software systems continues to grow, particularly in mission-critical environments such as healthcare, aerospace, and industrial infrastructure. The reliability of these systems is crucial—failures or vulnerabilities can lead to severe financial losses and even endanger lives. A significant portion of this foundational software is still written in C, a language created by Dennis Ritchie in 1972 [1], [2]. Although C has been instrumental in the evolution of software, its lack of safeguards—especially around memory management—is notorious. Memory safety bugs remain a persistent vulnerability, and producing provably and verifiably safe code in C is exceptionally challenging—take for example the stringent guidelines required by organizations like NASA for safety-critical applications [3].\nTo address these challenges, programming languages with built-in memory safety features, such as Ada and Rust, have been introduced [4], [5]. Nevertheless, no language offers absolute immunity from such vulnerabilities. In addition, much of the global software infrastructure remains written in memory-unsafe languages, with C-based codebases unlikely to disappear in the near future. Ultimately, the potential for human error grows in tandem with increasing software complexity, meaning software is only as safe as its weakest link.\nThe advent of Large Language Models (LLMs) has profoundly influenced software development. Developers have began to regularly use LLMs for code generation, refactoring, and documentation assistance. These models at large demonstrate remarkable programming capabilities. Still, they can often introduce subtle errors that may go unnoticed by even experienced developers. Many researchers argue that the use of such technologies inherently contributes to the generation of insecure code [6], [7], [8]. As LLM-generated code becomes more pervasive, so does the likelihood of unnoticed software errors escaping traditional human review.\nWithin this landscape, the need to detect vulnerabilities and ensure software quality is more urgent than ever. Fuzzing, a technique that generates and executes a vast array of test cases to identify potential bugs, has emerged as a vital approach for detecting memory safety violations. However, the necessity of manually-written harnesses—programs designed to exercise the Application Programming Interface (API) of the software under examination—poses a significant barrier to its broader adoption. As a result, the field of fuzzing automation through LLMs has gained considerable traction in recent years. Despite extensive advances in automating fuzzing, significant hurdles remain. Most current automatic-fuzzing systems require pre-existing fuzz harnesses [9] or depend on sample client code to exercise the target program [10], [11], [12]. Often, these tools still rely on developers for integration or final evaluation, leaving parts of the process manual and incomplete. Consequently, the application of LLMs to harness generation and end-to-end fuzzing remains a developing field.\nThis thesis aims to push the boundaries of fuzzing automation by leveraging the code synthesis and most importantly reasoning strengths of modern LLMs. We introduce OverHAuL, a system that accepts a bare and previously unfuzzed C project, utilizes LLM agents to author a new fuzzing harness from scratch and evaluates its efficacy in a closed iterative feedback loop. In this loop, said feedback is constantly utilized to improve the generated harness. This end-to-end approach is designed to minimize manual effort and accelerate vulnerability detection in C codebases.\nThis thesis begins by establishing the fundamental concepts required to contextualize its contributions (Chapter 2). Subsequently, we introduce the OverHAuL system, providing a comprehensive description of its architecture, the innovative techniques employed and their respective roles in advancing the state of automated harness generation (Chapter 3). In the evaluation chapter (Chapter 4), we assemble a benchmark suite comprised of ten open-source C projects and systematically evaluate the effectiveness of OverHAuL by measuring the number of successfully generated harnesses. Additionally, we present an extensive survey of recent research in automated fuzzing (Chapter 5), highlighting that most fuzzing systems either rely on pre-existing harnesses or employ client code, thereby shifting the responsibility for validation and integration onto the user. Finally, we discuss avenues for future enhancements to OverHAuL and conclude with a summary of our findings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#summary-of-contributions",
    "href": "chapters/intro.html#summary-of-contributions",
    "title": "1  Introduction",
    "section": "1.2 Summary of Contributions",
    "text": "1.2 Summary of Contributions\nThis thesis presents the following key contributions:\n\nThe introduction of OverHAuL, a framework that enables fully automated end-to-end fuzzing harness generation using LLMs. It introduces novel techniques like an iterative feedback loop between LLM agents and the usage of a codebase oracle for code exploration.\nEmpirical validation through benchmarking experiments using ten real-world open source projects. We demonstrate that OverHAuL generates effective fuzzing harnesses with a success rate of 81.25%.\nFull open sourcing of all research artifacts, datasets, and code at https://github.com/kchousos/OverHAuL to encourage further research and ensure reproducibility.\n\nThis work aims to advance the use of LLMs in automated software testing, particularly for legacy codebases where building harnesses by hand is impractical or costly. By doing so, we strive to enhance software security and reliability in sectors where correctness is imperative.\n\n\n\n\n[1] B. W. Kernighan and D. M. Ritchie, The C programming language. in Prentice-Hall software series. Englewood Cliffs, N.J: Prentice-Hall, 1978.\n\n\n[2] D. M. Ritchie, S. C. Johnson, M. E. Lesk, and B. W. Kernighan, “The C programming language,” Bell Sys. Tech. J, vol. 57, no. 6, pp. 1991–2019, 1978, Available: https://www.academia.edu/download/67840358/1978.07_Bell_System_Technical_Journal.pdf#page=85\n\n\n[3] G. J. Holzmann, “The Power of 10: Rules for Developing Safety-Critical Code,” Jun. 2006, Available: https://web.eecs.umich.edu/~imarkov/10rules.pdf\n\n\n[4] Ada Developers, “Ada Reference Manual, 2022 Edition,” 2022. Available: https://www.adaic.org/resources/add_content/standards/22rm/html/RM-TTL.html\n\n\n[5] Rust Project Developers, “Rust Programming Language,” 2025. Available: https://www.rust-lang.org/\n\n\n[6] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[7] N. Kosmyna et al., “Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task,” Jun. 10, 2025. doi: 10.48550/arXiv.2506.08872. Available: http://arxiv.org/abs/2506.08872\n\n\n[8] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[9] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[10] B. Jeong et al., “UTopia: Automatic Generation of Fuzz Driver using Unit Tests,” in 2023 IEEE Symposium on Security and Privacy (SP), May 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394. Available: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/background.html",
    "href": "chapters/background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Fuzz Testing\nThis chapter provides the foundational and necessary background for this thesis, by exploring the core concepts and technological advances central to modern fuzzing and Large Language Models (LLMs). It begins with an in-depth definition and overview of fuzz testing—an automated technique for uncovering software bugs and vulnerabilities through randomized input generation—highlighting its methodology, tools, and impact. What follows is a discussion on LLMs and their transformative influence on natural language processing, programming, and code generation. Challenges and opportunities in applying LLMs to tasks such as fuzzing harness generation are examined, leading to a discussion of Neurosymbolic AI, an emerging approach that combines neural and symbolic reasoning to address the limitations of current AI systems. This multifaceted background establishes the context necessary for understanding the research and innovations presented in subsequent chapters.\nFuzzing is an automated software-testing technique in which a Program Under Test (PUT) is executed with (pseudo-)random inputs in the hope of exposing undefined behavior. When such behavior manifests as a crash, hang, or memory-safety violation, the corresponding input constitutes a test-case that reveals a bug and often a vulnerability [1]. In a certain sense, fuzzing is a form of adversarial, penetration-style testing carried out by the defender before the adversary has an opportunity to do so. Interest in the technique surged after the publication of three practitioner-oriented books in 2007–2008 [2], [3], [4].\nHistorically, the term was coined by Miller et al. in 1990, who used “fuzz” to describe a program that “generates a stream of random characters to be consumed by a target program” [5]. This informal usage captured the essence of what fuzzing aims to do: stress test software by bombarding it with unexpected inputs to reveal bugs. To formalize this concept, we adopt Manes et al.’s rigorous definitions [1]:\nThis means fuzzing involves running the target program on inputs that go beyond those it is typically designed to handle, aiming to uncover hidden issues. An individual instance of such execution—or a bounded sequence thereof—is called a fuzzing run. When these runs are conducted systematically and at scale with the specific goal of detecting violations of a security policy, the activity is known as fuzz testing (or simply fuzzing):\nThis distinction highlights that fuzz testing is fuzzing with an explicit focus on security properties and policy enforcement. Central to managing this process is the fuzzer engine, which orchestrates the execution of one or more fuzzing runs as part of a fuzz campaign. A fuzz campaign represents a concrete instance of fuzz testing tailored to a particular program and security policy:\nThroughout each execution within a campaign, a bug oracle plays a critical role in evaluating the program’s behavior to determine whether it violates the defined security policy:\nIn practice, bug oracles often rely on runtime instrumentation techniques, such as monitoring for fatal POSIX signals (e.g., SIGSEGV) or using sanitizers like AddressSanitizer (ASan) [6]. Tools like LibFuzzer [7] commonly incorporate such instrumentation to reliably identify crashes or memory errors during fuzzing.\nMost fuzz campaigns begin with a set of seeds—inputs that are well-formed and belong to the PUT’s expected input space—called a seed corpus. These seeds serve as starting points from which the fuzzer generates new test cases by applying transformations or mutations, thereby exploring a broader input space:\nThe process of selecting an effective initial corpus is crucial because it directly impacts how quickly and thoroughly the fuzzer can cover the target program’s code. This challenge—studied as the seed-selection problem—involves identifying seeds that enable rapid discovery of diverse execution paths and is non-trivial [8]. A well-chosen seed set often accelerates bug discovery and improves overall fuzzing efficiency.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#fuzz-testing",
    "href": "chapters/background.html#fuzz-testing",
    "title": "2  Background",
    "section": "",
    "text": "Definition 2.1 (Fuzzing) Fuzzing is the execution of a Program Under Test (PUT) using input(s) sampled from an input space (the fuzz input space) that protrudes the expected input space of the PUT [1].\n\n\n\nDefinition 2.2 (Fuzz Testing) Fuzz testing is the use of fuzzing to test whether a PUT violates a security policy [1].\n\n\n\nDefinition 2.3 (Fuzzer, Fuzzer Engine) A fuzzer is a program that performs fuzz testing on a PUT [1].\n\n\nDefinition 2.4 (Fuzz Campaign) A fuzz campaign is a specific execution of a fuzzer on a PUT with a specific security policy [1].\n\n\n\nDefinition 2.5 (Bug Oracle) A bug oracle is a component (often inside the fuzzer) that determines whether a given execution of the PUT violates a specific security policy [1].\n\n\n\n\nDefinition 2.6 (Seed) An input given to the PUT that is mutated by the fuzzer to produce new test cases. During a fuzz campaign (Definition 2.4) all seeds are stored in a seed pool or corpus [1].\n\n\n\n2.1.1 Motivation\n\nThe purpose of fuzzing relies on the assumption that there are bugs within every program, which are waiting to be discovered. Therefore, a systematic approach should find them sooner or later.\n— OWASP Foundation [9]\n\nFuzz testing provides several key advantages that contribute substantially to software quality and security. First, by uncovering vulnerabilities early in the development cycle, fuzzing reduces both the cost and risk associated with addressing security flaws after deployment. This proactive approach not only minimizes potential exposure but also streamlines the remediation process. Additionally, by subjecting software to the same randomized, adversarial inputs that malicious actors might use, fuzz testing puts defenders on equal footing with attackers, enhancing preparedness against emerging zero-day threats.\nBeyond security, fuzzing plays a crucial role in improving the robustness and correctness of software systems. It is particularly effective at identifying logical errors and stability issues in complex, high-throughput APIs—such as decompressors and parsers—especially when these systems are expected to handle only well-formed inputs. Moreover, the integration of fuzz testing into continuous integration pipelines provides an effective guard against regressions. By systematically re-executing a corpus of previously discovered crashing inputs, developers can ensure that resolved bugs do not resurface in subsequent releases, thereby maintaining a consistent level of software reliability over time.\n\n2.1.1.1 Success Stories\nHeartbleed (CVE-2014-0160) [10], [11] arose from a buffer over-read1 in the TLS implementation of the OpenSSL library [12], introduced on 1st of February 2012 and unnoticed until 1st of April 2014. Later analysis showed that a simple fuzz campaign exercising the TLS heartbeat extension would have revealed the defect almost immediately [13].\n\n\nhttps://xkcd.com/1354/ provides a concise illustration.↩︎\n\n\nLikewise, the Shellshock (or Bashdoor) family of bugs in GNU Bash [14] enabled arbitrary command execution on many UNIX systems. While the initial flaw was fixed promptly, subsequent bug variants were discovered by Google’s Michał Zalewski using his own fuzzer—the now ubiquitous AFL fuzzer [15]—in late 2014 [16].\nOn the defensive tooling side, the security tool named Mayhem [17], [18]—developed by the company of the same name, formerly known as ForAllSecure—has since been adopted by the US Air Force, the Pentagon, Cloudflare, and numerous open-source communities. It has found and facilitated the remediation of thousands of previously unknown vulnerabilities, from errors in Cloudflare’s infrastructure to bugs in open-source projects like OpenWRT [19].\nThese cases underscore the central thesis of fuzz testing: exhaustive manual review is infeasible, but scalable stochastic exploration reliably surfaces the critical few defects that matter most.\n\n\n\n2.1.2 Methodology\nAs previously discussed, fuzz testing of a PUT is typically conducted using a dedicated fuzzing engine (Definition 2.3). Among the most widely adopted fuzzers for C and C++ projects and libraries are AFL [15]—which has since evolved into AFL++ [20]—and LibFuzzer [7]. Within the OverHAuL framework, LibFuzzer is preferred due to its superior suitability for library fuzzing, whereas AFL++ predominantly targets executables and binary fuzzing.\n\n2.1.2.1 LibFuzzer\nLibFuzzer [7] is an in-process, coverage-guided evolutionary fuzzing engine primarily designed for testing libraries. It forms part of the LLVM ecosystem [21] and operates by linking directly with the library under evaluation. The fuzzer delivers mutated input data to the library through a designated fuzzing entry point, commonly referred to as the fuzz target or harness.\n\nDefinition 2.7 (Fuzz target) A function that accepts a byte array as input and exercises the application programming interface (API) under test using these inputs [7]. This construct is also known as a fuzz driver, fuzzer entry point, or fuzzing harness.\n\nFor the remainder of this thesis, the terms presented in Definition 2.7 will be used interchangeably.\nTo effectively validate an implementation or library, developers are required to author a fuzzing harness that invokes the target library’s API functions utilizing the fuzz-generated inputs. This harness serves as the principal interface for the fuzzer and is executed iteratively, each time with mutated input designed to maximize code coverage and uncover defects. To comply with LibFuzzer’s interface requirements, a harness must conform to the function signature shown in Listing 2.1. A more illustrative example of such a harness is provided in Listing 2.2.\n\n\n\nListing 2.1: This function receives the fuzzing input via a pointer to an array of bytes (Data) and its associated size (Size). Efficiency in fuzzing is achieved by invoking the API of interest within the body of this function, thereby allowing the fuzzer to explore a broad spectrum of behavior through systematic input mutation.\n\n\nint LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {\n  DoSomethingInterestingWithData(Data, Size);\n  return 0;\n}\n\n\n\n\n\n\nListing 2.2: This example demonstrates a minimal harness that triggers a controlled crash upon receiving HI! as input.\n\n\n// test_fuzzer.cpp\n#include &lt;stdint.h&gt;\n#include &lt;stddef.h&gt;\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n  if (size &gt; 0 && data[0] == 'H')\n    if (size &gt; 1 && data[1] == 'I')\n      if (size &gt; 2 && data[2] == '!')\n        __builtin_trap();\n  return 0;\n}\n\n\n\nTo compile and link such a harness with LibFuzzer, the Clang compiler—also part of the LLVM project [21]—must be used alongside appropriate compiler flags. For instance, compiling the harness in Listing 2.2 can be achieved as shown in Listing 2.3.\n\n\n\nListing 2.3: This example illustrates the compilation and execution workflow necessary for deploying a LibFuzzer-based fuzzing harness.\n\n\n# Compile test_fuzzer.cc with AddressSanitizer and link against LibFuzzer.\nclang++ -fsanitize=address,fuzzer test_fuzzer.cc\n# Execute the fuzzer without any pre-existing seed corpus.\n./a.out\n\n\n\n\n\n2.1.2.2 AFL and AFL++\nAmerican Fuzzy Lop (AFL) [15], developed by Michał Zalewski, is a seminal fuzzer targeting C and C++ applications. Its core methodology relies on instrumented binaries to provide edge coverage feedback, thereby guiding input mutation towards unexplored program paths. AFL supports several emulation backends including QEMU [22]—an open-source CPU emulator facilitating fuzzing on diverse architectures—and Unicorn [23], a lightweight multi-platform CPU emulator. While AFL established itself as a foundational tool within the fuzzing community, its successor AFL++ [20] incorporates numerous enhancements and additional features to improve fuzzing efficacy.\nAFL operates by ingesting seed inputs from a specified directory (seeds_dir), applying mutations, and then executing the target binary to discover novel execution paths. Execution can be initiated using the following command-line syntax:\n./afl-fuzz -i seeds_dir -o output_dir -- /path/to/tested/program\nAFL is capable of fuzzing both black-box and instrumented binaries, employing a fork-server mechanism to optimize performance. It additionally supports persistent mode execution as well as modes leveraging QEMU and Unicorn emulators, thereby providing extensive flexibility for different testing environments.\nAlthough AFL is traditionally utilized for fuzzing standalone programs or binaries, it is also capable of fuzzing libraries and other software components. In such scenarios, rather than implementing the LLVMFuzzerTestOneInput style harness, AFL can use the standard main() function as the fuzzing entry point. Nonetheless, AFL also accommodates integration with LLVMFuzzerTestOneInput-based harnesses, underscoring its adaptability across varied fuzzing use cases.\n\n\n\n2.1.3 Challenges in Adoption\nDespite its potential for uncovering software vulnerabilities, fuzzing remains a relatively underutilized testing technique compared to more established methodologies such as Test-Driven Development (TDD). This limited adoption can be attributed, in part, to the substantial initial investment required to design and implement appropriate test harnesses that enable effective fuzzing processes. Furthermore, the interpretation of fuzzing outcomes—particularly the identification, diagnostic analysis, and prioritization of program crashes—demands considerable resources and specialized expertise. These factors collectively pose significant barriers to the widespread integration of fuzzing within standard software development and testing practices. OverHAuL addresses this challenge by facilitating the seamless integration of fuzzing into developers’ workflows, minimizing initial barriers and reducing upfront costs to an almost negligible level.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#large-language-models",
    "href": "chapters/background.html#large-language-models",
    "title": "2  Background",
    "section": "2.2 Large Language Models",
    "text": "2.2 Large Language Models\nNatural Language Processing (NLP), a subfield of AI, has a rich and ongoing history that has evolved significantly since its beginning in the 1990s [24], [25]. Among the most notable—and recent—advancements in this domain are LLMs, which have transformed the landscape of NLP and AI in general.\nAt the core of many LLMs is the attention mechanism, which was introduced by Bahdanau et al. in 2014 [26]. This pivotal innovation enabled models to focus on relevant parts of the input sequence when making predictions, significantly improving language understanding and generation tasks. Building on this foundation, the Transformer architecture was proposed by Vaswani et al. in 2017 [27]. This architecture has become the backbone of most contemporary LLMs, as it efficiently processes sequences of data, capturing long-range dependencies without being hindered by sequential processing limitations.\nOne of the first major breakthroughs utilizing the Transformer architecture was BERT (Bidirectional Encoder Representations from Transformers), developed by Devlin et al. in 2019 [28]. BERT’s bi-directional understanding allowed it to capture the context of words from both directions, which improved the accuracy of various NLP tasks. Following this, the Generative Pre-trained Transformer (GPT) series, initiated by OpenAI with the original GPT model in 2018 [29], further pushed the boundaries. Subsequent iterations, including GPT-2 [30], GPT-3 [31], and the most current GPT-4 [32], have continued to enhance performance by scaling model size, data, and training techniques.\nIn addition to OpenAI’s contributions, other significant models have emerged, such as Claude, DeepSeek-R1 and the Llama series (1 through 3) [33], [34], [35]. The proliferation of LLMs has sparked an active discourse about their capabilities, applications, and implications in various fields.\n\n2.2.1 State-of-the-art GPTs\nUser-facing LLMs are generally categorized between closed-source and open-source models. Closed-source LLMs like ChatGPT, Claude, and Gemini [33], [36], [37] represent commercially developed systems often optimized for specific tasks without public access to their underlying weights. In contrast, open-source models2, including the Llama series [35] and Deepseek [34], provide researchers and practitioners with access to model weights, allowing for greater transparency and adaptability.\n\n\nThe term “open-source” models is somewhat misleading, since these are better termed as open-weights models. While their weights are publicly available, their training data and underlying code are often proprietary. This terminology reflects community usage but fails to capture the limitations of transparency and accessibility inherent in these models.↩︎\n\n\n\n\n2.2.2 Prompting\nInteraction with LLMs typically occurs through chat-like interfaces where the user gives queries and tasks for the LLM to answer and complete, a process commonly referred to as prompting. A critical aspect of effective engagement with LLMs is the usage of different prompting strategies, which can significantly influence the quality and relevance of the generated outputs. Various approaches to prompting have been developed and studied, including zero-shot and few-shot prompting. In zero-shot prompting, the model is expected to perform the given task without any provided examples, while in few-shot prompting, the user offers a limited number of examples to guide the model’s responses [31].\nTo enhance performance on more complex tasks, several advanced prompting techniques have emerged. One notable strategy is the Chain of Thought approach (COT) [38], which entails presenting the model with sample thought processes for solving a given task. This method encourages the model to generate more coherent and logical reasoning by mimicking human-like cognitive pathways. A more refined but complex variant of this approach is the Tree of Thoughts technique [39], which enables the LLM to explore multiple lines of reasoning concurrently, thereby facilitating the selection of the most promising train of thought for further exploration.\nIn addition to these cognitive strategies, Retrieval-Augmented Generation (RAG) [40] is another innovative technique that enhances the model’s capacity to provide accurate information by incorporating external knowledge not present in its training dataset. RAG operates by integrating the LLM with an external storage system—often a vector store containing relevant documents—that the model can query in real-time. This allows the LLM to pull up pertinent and/or proprietary information in response to user queries, resulting in more comprehensive and accurate answers.\nMoreover, the ReAct framework [41], which stands for Reasoning and Acting, empowers LLMs by granting access to external tools. This capability allows LLM instances to function as intelligent agents that can interact meaningfully with their environment through user-defined functions. For instance, a ReAct tool could be a function that returns a weather forecast based on the user’s current location. In this scenario, the LLM can provide accurate and truthful predictions, thereby mitigating risks associated with hallucinated responses.\n\n\n2.2.3 LLMs for Coding\nThe impact of LLMs in software development in recent years is apparent, with hundreds of LLM-assistance extensions and Integrated Development Environments (IDEs) being published. Notable instances include tools like GitHub Copilot and IDEs such as Cursor [42], [43], which leverage LLM capabilities to provide developers with coding suggestions, auto-completions, and even real-time debugging assistance. Such innovations have introduced a layer of interaction that enhances productivity and fosters a more intuitive coding experience. Additionally, more and more LLMs are now specifically trained for usage in code-generation tasks [44], [45], [46].\nOne exemplary product of this innovation is vibecoding and the no-code movement, which describe the development of software by only prompting and tasking an LLM, i.e. without any actual programming required by the user. This constitutes a showcase of how LLMs can be used to elevate the coding experience by supporting developers as they navigate complex programming tasks [47]. By analyzing the context of the code being written, these sophisticated models can provide contextualized insights and relevant snippets, effectively streamlining the development process. Developers can benefit from reduced cognitive load, as they receive suggestions that not only cater to immediate coding needs but also promote adherence to best practices and coding standards.\nDespite these advancements, it is crucial to recognize the inherent limitations of LLMs when applied to software development. While they can help in many aspects of coding, they are not immune to generating erroneous outputs—a phenomenon often referred to as “hallucination” [48]. Hallucinations occur when LLMs produce information that is unfounded or inaccurate, which can stem from several factors, including the limitations of their training data and the constrained context window within which they operate. As LLMs generate code suggestions based on the patterns learned from vast datasets, they may inadvertently propose solutions that do not align with the specific requirements of a task or that utilize outdated programming paradigms.\nMoreover, the challenge of limited context windows can lead to suboptimal suggestions [49]. LLMs generally process a fixed amount of text when generating responses, which can impact their ability to fully grasp the nuances of complex coding scenarios. This may result in outputs that lack the necessary depth and specificity required for successful implementation. As a consequence, developers must exercise caution and critically evaluate the suggestions offered by these models, as reliance on them without due diligence could lead to the introduction of bugs or other issues in the code.\n\n\n2.2.4 LLMs for Fuzzing\nIn the domain of fuzzing, recent research has explored the application of LLMs primarily along two axes: employing LLMs to generate seeds and inputs for the program under test [50], [51], [52], [53] and leveraging them to generate the fuzz driver itself (Chapter 5). This thesis focuses on the latter, recognizing that while using LLMs for seed generation offers certain advantages, the challenge of automating harness generation represents a deeper and more meaningful frontier. Significant limitations such as restricted context windows and the propensity for LLMs to hallucinate remain central concerns in this area [48], [49].\nThe process of constructing a fuzzing harness is inherently complex, demanding a profound understanding of the target library and the nuanced interactions among its components. Achieving this level of comprehension is often beyond the reach of LLMs when deployed in isolation. Empirical evidence by Jiang et al. [54] demonstrates that zero-shot harness generation with LLMs is both ineffective and prone to significant errors. Specifically, LLMs tend to rely heavily on patterns encountered during training, which leads to the erroneous invocation of APIs, particularly when their context window is pushed to its limits. This over-reliance on training data exacerbates the risk of hallucination, compounding the challenge of generating correct and robust fuzz drivers.\nCompounding this issue is the inherent risk introduced by error-prone code synthesized by LLMs. In the context of fuzzing, a fundamental requirement is the clear attribution of observed failures: developers must be confident that detected crashes stem from vulnerabilities in the tested software rather than flaws or bugs inadvertently introduced by the harness. This necessity imposes an additional verification burden, increasing developers’ cognitive load and diverting attention from the primary goal of meaningful software evaluation and improvement.\nEnhancing the reliability of LLM-generated harnesses thus necessitates systematic and programmatic evaluation and validation of generated artifacts [55]. Such validation involves implementing structured techniques to rigorously assess both the accuracy and robustness of the code, confirming that it interacts correctly with the relevant software components and behaves as intended. This approach aligns with the emerging framework of Neurosymbolic AI (Section 2.3), which integrates the statistical learning capabilities of neural networks with the rigor and precision of symbolic reasoning. By leveraging the strengths of both paradigms, neuroscience-inspired symbolic methods [56] may offer pathways toward more reliable and effective LLM-generated fuzzing harnesses, facilitating a smoother integration of automated testing practices into established software development pipelines [57], [58].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#sec-nesy",
    "href": "chapters/background.html#sec-nesy",
    "title": "2  Background",
    "section": "2.3 Neurosymbolic AI",
    "text": "2.3 Neurosymbolic AI\nNeurosymbolic AI represents a groundbreaking fusion of neural network methodologies with symbolic execution techniques and tools, providing a multi-faceted approach to overcoming the inherent limitations of traditional AI paradigms [59], [60]. This innovative synthesis seeks to combine the strengths of both neural networks, which excel in pattern recognition and data-driven learning, and symbolic systems, which offer structured reasoning and interpretability. By integrating these two approaches, neurosymbolic AI aims to create cognitive models that are not only more accurate but also more robust in problem-solving contexts.\nAt its core, Neurosymbolic AI facilitates the development of AI systems that are capable of understanding and interpreting feedback in real-world scenarios [61]. This characteristic is particularly significant in the current landscape of artificial intelligence, where LLMs are predominant. In this context, Neurosymbolic AI is increasingly viewed as a critical solution to pressing issues related to explainability, attribution, and reliability in AI systems [55], [62]. These challenges are essential for ensuring that AI systems can be trusted and effectively utilized in various applications, from business to healthcare.\nThe burgeoning field of neurosymbolic AI is still in its nascent stages, with ongoing research and development actively exploring its potential to enhance attribution methodologies within large language models. By addressing these critical challenges, Neurosymbolic AI can significantly contribute to the broader landscape of trustworthy AI systems, allowing for more transparent and accountable decision-making processes [55], [59], [62].\nMoreover, the application of neurosymbolic AI within the domain of fuzzing is gaining traction, paving the way for innovative explorations. This integration of LLMs with symbolic systems opens up new avenues for research. Currently, there are only a limited number of tools that support such hybrid approaches (Chapter 5). Among these, OverHAuL constitutes a Neuro[Symbolic] tool, as classified by Henry Kautz’s taxonomy [63], [64]. This means that the neural model—specifically the LLM—can leverage symbolic reasoning tools—in this case a source code explorer (Section 3.6)—to augment its reasoning capabilities. This symbiotic relationship enhances the overall efficacy and versatility of LLMs for fuzzing harnesses generation, demonstrating the profound potential held by the fusion of neural and symbolic methodologies.\n\n\n\n\n[1] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[2] A. Takanen, J. DeMott, C. Miller, and A. Kettunen, Fuzzing for software security testing and quality assurance, Second edition. in Information security and privacy library. Boston London Norwood, MA: Artech House, 2018.\n\n\n[3] M. Sutton, A. Greene, and P. Amini, Fuzzing: Brute force vulnerabilty discovery. Upper Saddle River, NJ: Addison-Wesley, 2007.\n\n\n[4] N. Rathaus and G. Evron, Open source fuzzing tools. Burlington, MA: Syngress Pub, 2007.\n\n\n[5] B. P. Miller, L. Fredriksen, and B. So, “An empirical study of the reliability of UNIX utilities,” Commun. ACM, vol. 33, no. 12, pp. 32–44, Dec. 1990, doi: 10.1145/96267.96279. Available: https://dl.acm.org/doi/10.1145/96267.96279\n\n\n[6] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “AddressSanitizer: A fast address sanity checker,” in 2012 USENIX annual technical conference (USENIX ATC 12), 2012, pp. 309–318. Available: https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany\n\n\n[7] LLVM Project, “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[8] A. Rebert et al., “Optimizing seed selection for fuzzing,” in Proceedings of the 23rd USENIX conference on Security Symposium, in SEC’14. USA: USENIX Association, Aug. 2014, pp. 861–875.\n\n\n[9] OWASP Foundation, “Fuzzing.” Available: https://owasp.org/www-community/Fuzzing\n\n\n[10] Blackduck, Inc., “Heartbleed Bug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[11] CVE Program, “CVE - CVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[12] The OpenSSL Project, “Openssl/openssl.” OpenSSL, Jul. 15, 2025. Available: https://github.com/openssl/openssl\n\n\n[13] D. Wheeler, “How to Prevent the next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[14] GNU Project, “Bash - GNU Project - Free Software Foundation.” Available: https://www.gnu.org/software/bash/\n\n\n[15] M. Zalewski, “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[16] J. Saarinen, “Further flaws render Shellshock patch ineffective,” Sep. 29, 2014. Available: https://www.itnews.com.au/news/further-flaws-render-shellshock-patch-ineffective-396256\n\n\n[17] T. Avgerinos et al., “The mayhem cyber reasoning system,” IEEE Security & Privacy, vol. 16, no. 2, pp. 52–60, 2018.\n\n\n[18] S. K. Cha, T. Avgerinos, A. Rebert, and D. Brumley, “Unleashing mayhem on binary code,” in 2012 IEEE symposium on security and privacy, IEEE, 2012, pp. 380–394.\n\n\n[19] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[20] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[21] LLVM Project, “The LLVM Compiler Infrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[22] F. Bellard, P. Maydell, and QEMU Team, “QEMU.” May 29, 2025. Available: https://www.qemu.org/\n\n\n[23] Unicorn Engine, “Unicorn-engine/unicorn.” Unicorn Engine, Jul. 15, 2025. Available: https://github.com/unicorn-engine/unicorn\n\n\n[24] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[25] Z. Wang, Z. Chu, T. V. Doan, S. Ni, M. Yang, and W. Zhang, “History, development, and principles of large language models: An introductory survey,” AI Ethics, vol. 5, no. 3, pp. 1955–1971, Jun. 2025, doi: 10.1007/s43681-024-00583-7. Available: https://doi.org/10.1007/s43681-024-00583-7\n\n\n[26] D. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate,” May 19, 2016. doi: 10.48550/arXiv.1409.0473. Available: http://arxiv.org/abs/1409.0473\n\n\n[27] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[28] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,” May 24, 2019. doi: 10.48550/arXiv.1810.04805. Available: http://arxiv.org/abs/1810.04805\n\n\n[29] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-training,” 2018, Available: https://www.mikecaptain.com/resources/pdf/GPT-1.pdf\n\n\n[30] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI blog, vol. 1, no. 8, p. 9, 2019, Available: https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf\n\n\n[31] T. B. Brown et al., “Language Models are Few-Shot Learners,” Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165. Available: http://arxiv.org/abs/2005.14165\n\n\n[32] OpenAI et al., “GPT-4 Technical Report,” Mar. 04, 2024. doi: 10.48550/arXiv.2303.08774. Available: http://arxiv.org/abs/2303.08774\n\n\n[33] Anthropic, “Claude,” 2025. Available: https://claude.ai/new\n\n\n[34] DeepSeek-AI et al., “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,” Jan. 22, 2025. doi: 10.48550/arXiv.2501.12948. Available: http://arxiv.org/abs/2501.12948\n\n\n[35] A. Grattafiori et al., “The Llama 3 Herd of Models,” Nov. 23, 2024. doi: 10.48550/arXiv.2407.21783. Available: http://arxiv.org/abs/2407.21783\n\n\n[36] OpenAI, “ChatGPT,” 2025. Available: https://chatgpt.com\n\n\n[37] Google, “‎Google Gemini,” 2025. Available: https://gemini.google.com\n\n\n[38] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[39] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[40] P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” Apr. 12, 2021. doi: 10.48550/arXiv.2005.11401. Available: http://arxiv.org/abs/2005.11401\n\n\n[41] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[42] Anysphere, “Cursor - The AI Code Editor,” 2025. Available: https://cursor.com/\n\n\n[43] Microsoft, “GitHub Copilot · Your AI pair programmer,” 2025. Available: https://github.com/features/copilot\n\n\n[44] E. Nijkamp et al., “CodeGen: An open large language model for code with multi-turn program synthesis,” ICLR, 2023.\n\n\n[45] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “CodeGen2: Lessons for training llms on programming and natural languages,” ICLR, 2023.\n\n\n[46] OpenAI, “Introducing GPT-4.1 in the API,” Apr. 14, 2025. Available: https://openai.com/index/gpt-4-1/\n\n\n[47] A. Sarkar and I. Drosos, “Vibe coding: Programming through conversation with artificial intelligence,” Jun. 29, 2025. doi: 10.48550/arXiv.2506.23253. Available: http://arxiv.org/abs/2506.23253\n\n\n[48] L. Huang et al., “A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions,” ACM Trans. Inf. Syst., vol. 43, no. 2, pp. 1–55, Mar. 2025, doi: 10.1145/3703155. Available: http://arxiv.org/abs/2311.05232\n\n\n[49] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy, “Challenges and Applications of Large Language Models,” Jul. 19, 2023. doi: 10.48550/arXiv.2307.10169. Available: http://arxiv.org/abs/2307.10169\n\n\n[50] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[51] G. Black, V. Mathew Vaidyan, and G. Comert, “Evaluating Large Language Models for Enhanced Fuzzing: An Analysis Framework for LLM-Driven Seed Generation,” IEEE Access, vol. 12, pp. 156065–156081, 2024, doi: 10.1109/ACCESS.2024.3484947. Available: https://ieeexplore.ieee.org/abstract/document/10731701\n\n\n[52] W. Shi, Y. Zhang, X. Xing, and J. Xu, “Harnessing Large Language Models for Seed Generation in Greybox Fuzzing,” Nov. 27, 2024. doi: 10.48550/arXiv.2411.18143. Available: http://arxiv.org/abs/2411.18143\n\n\n[53] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[54] Y. Jiang et al., “When Fuzzing Meets LLMs: Challenges and Opportunities,” in Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering, in ACM Conferences. 2024, pp. 492–496. doi: 10.1145/3663529.3663784. Available: https://dl.acm.org/doi/abs/10.1145/3663529.3663784\n\n\n[55] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[56] D. Kahneman, Thinking, fast and slow, 1st ed. New York: Farrar, Straus and Giroux, 2011.\n\n\n[57] A. Mastropaolo and D. Poshyvanyk, “A Path Less Traveled: Reimagining Software Engineering Automation via a Neurosymbolic Paradigm,” May 04, 2025. doi: 10.48550/arXiv.2505.02275. Available: http://arxiv.org/abs/2505.02275\n\n\n[58] A. Velasco, A. Garryyeva, D. N. Palacio, A. Mastropaolo, and D. Poshyvanyk, “Toward Neurosymbolic Program Comprehension,” Feb. 03, 2025. doi: 10.48550/arXiv.2502.01806. Available: http://arxiv.org/abs/2502.01806\n\n\n[59] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[60] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[61] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[62] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[63] M. K. Sarker, L. Zhou, A. Eberhart, and P. Hitzler, “Neuro-symbolic artificial intelligence: Current trends,” AIC, vol. 34, no. 3, pp. 197–209, Mar. 2022, doi: 10.3233/aic-210084. Available: https://journals.sagepub.com/doi/full/10.3233/AIC-210084\n\n\n[64] H. Kautz, “The Third AI Summer,” presented at the 34th Annual Meeting of the Association for the Advancement of Artificial Intelligence, Feb. 10, 2020. Available: https://www.youtube.com/watch?v=_cQITY0SPiw",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html",
    "href": "chapters/overhaul.html",
    "title": "3  OverHAuL’s Design",
    "section": "",
    "text": "3.1 Installation and Usage\nIn this thesis we present OverHAuL (Harness Automation with LLMs), a neurosymbolic AI tool that automatically generates fuzzing harnesses for C libraries through LLM agents. In its core, OverHAuL is comprised by a triplet of LLM ReAct agents [1]—each with its own responsibility and scope—and a codebase oracle reserving the given project’s analyzed source code. An overview of OverHAuL’s process is presented in Figure 3.1, detailed in Section 3.2. The objective of OverHAuL is to streamline the process of fuzz testing for unfuzzed C libraries. Given a link to a git repository [2] of a C library, OverHAuL automatically generates a new fuzzing harness specifically designed for the project. In addition to the harness, it produces a compilation script to facilitate building the harness, generates a representative input that can trigger crashes, and logs the output from the executed harness.\nOverHAuL utilizes autonomous ReAct agents which inspect and analyze the project’s source code. The latter is stored and interacted with as a set of text embeddings [3], kept in a vector store. Both approaches are, to the best of our knowledge, novel in the field of automatic fuzzing harnesses generation. OverHAuL also implements an evaluation component that assesses in real-time all generated harnesses, making the results tenable, reproducible and well-founded. Ideally, this methodology provides a comprehensive and systematic framework for identifying previously unknown software vulnerabilities in projects that have not yet been fuzz tested.\nAs detailed in Section 5.4, OverHAuL does not expect and depend on the existence of client code or unit tests [4], [5], [6] nor does it require any preexisting fuzzing harnesses [7] or any documentation present [8]. Also importantly, OverHAuL is decoupled from other fuzzing projects, thus lowering the barrier to entry for new projects [7], [9]. Lastly, the user isn’t mandated to manually specify the function which the harness-to-be-generated must fuzz. Instead, OverHAuL’s agents examine and assess the provided codebase, choosing after evaluation the most optimal target function.\nFinally, OverHAuL excels in its user-friendliness, as it constitutes a simple and easily-installable Python package with minimal external dependencies—only real dependency being Clang, a prevalent compiler available across all primary operating systems. This contrasts most other comparable systems, which are typically characterized by their limited documentation, lack of extensive testing, and a focus primarily on experimental functionality.1\nThe source code of OverHAuL is available in https://github.com/kchousos/OverHAuL. OverHAuL can be installed by cloning the git repository locally, creating and enabling a Python3.10 virtual environment [11] (optional, but recommended) and installing it inside the environment using Python’s PIP package installer [12], like in Listing 3.1.\nTo use OverHAuL, you need to provide a secret key for using OpenAI’s API service. This key can be either stored in a .env file in the root directory or exported in the shell environment:\nOnce these preliminary steps are completed, OverHAuL can be executed. The primary argument required by OverHAuL is the repository link of the library that is to be fuzzed. Additionally, users have the option to specify certain command-line flags, which allow them to control the checked-out commit of the cloned project, select the OpenAI LLM model from a predefined list, define specific file patterns for OverHAuL to search for, and determine the directory in which the project will be cloned. For a concrete example, we will use OverHAuL to create a new fuzzing harness for dvhar’s dateparsing C library and specify the LLM model to OpenAI’s gpt-4.1 model. The resulting command and its output is presented in Figure 3.2.\nIn this example, the dateparse repository is cloned into the ./output/dateparse directory, which is relative to the root directory of OverHAuL. Following a successful execution, the project’s directory will contain a new folder named harnesses, which will house all the generated harnesses formatted as harness_n.c—where n ranges from 1 to N-1, with N representing the total number of harnesses produced. The most recent and verifiably correct harness will be designated simply as harness.c. Additionally, the dateparse folder will include an executable script named overhaul.sh, which contains the compilation command necessary for the harness. A log file titled harness.out will also be present, documenting the output from the latest harness execution. Lastly and most importantly, there will be at least one non-empty crash file included, serving as a witness to the harness’s correctness. In the following sections, the intermediary steps between invocation and completion are disected and analyzed. The dateparse project is used as a running example.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html#sec-install",
    "href": "chapters/overhaul.html#sec-install",
    "title": "3  OverHAuL’s Design",
    "section": "",
    "text": "Listing 3.1: OverHAuL’s straightforward installation process.\n\n\n$ git clone https://github.com/kchousos/overhaul; cd overhaul\n  ...\n$ python3.10 -m venv .venv\n$ source ./.venv/bin/activate\n$ pip install .\n  ...\n$ overhaul --help\nusage: overhaul [-h] [-c COMMIT] [-m MODEL] [-f FILES [FILES ...]] \n[-o OUTPUT_DIR] repo\n\nGenerate fuzzing harnesses for C/C++ projects\n\npositional arguments:\n  repo                  Link of a project's git repo, for which to generate \n                        a harness.\n\noptions:\n  -h, --help            show this help message and exit\n  -c COMMIT, --commit COMMIT\n                        A specific commit of the project to check out\n  -m MODEL, --model MODEL\n                        LLM model to be used. Available: o3-mini, o3, gpt-4o,\n                        gpt-4o-mini, gpt-4.1, gpt-4.1-mini, gpt-3.5-turbo, gpt-4\n  -f FILES [FILES ...], --files FILES [FILES ...]\n                        File patterns to include in analysis (e.g. *.c *.h)\n  -o OUTPUT_DIR, --output-dir OUTPUT_DIR\n                        Directory to clone the project into. Defaults to \"output\"\n$\n\n\n\n\n$ echo \"OPENAI_API_KEY=&lt;API-key-here&gt;\" &gt;&gt; .env\n# OR\n$ export OPENAI_API_KEY=&lt;API-key-here&gt;\n\n\n\n\n\n\n\nFigure 3.2: A successful execution of OverHAuL, harnessing the “dateparse” library using OpenAI’s gpt-4.1 model. Debug statements are printed to showcase the queries of the LLM agents to the codebase oracle (Section 3.3.3).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html#sec-architecture",
    "href": "chapters/overhaul.html#sec-architecture",
    "title": "3  OverHAuL’s Design",
    "section": "3.2 Architecture",
    "text": "3.2 Architecture\nOverHAuL can be compartmentalized in three stages: First, the project analysis stage (Section 3.2.1), the harness creation stage (Section 3.2.2) and the harness evaluation stage (Section 3.2.3).\n\n3.2.1 Project Analysis\nIn the project analysis stage (steps A.1–A.4), dateparse is ran through a static analysis tool named Flawfinder [13] and is sliced into function-level chunks, which are stored in a vector store. The results of this stage are a static analysis report and a codebase oracle, i.e. a vector store containing embeddings of function-level code chunks. Both resources are later available to the LLM agents. Flawfinder is executed with the dateparse directory as input and is responsible for the static analysis report. This report is considered a meaningful resource, since it provides the LLM agent responsible with the harness creation with some starting points to explore, regarding the occurrences of potentially vulnerable functions and/or unsafe code practices. Part of dateparse’s static analysis report is shown in Listing 3.2.\n\n\n\nListing 3.2: Static analysis report (Flawfinder output) of dateparse.\n\n\nFlawfinder version 2.0.19, (C) 2001-2019 David A. Wheeler.\nNumber of rules (primarily dangerous function names) in C/C++ ruleset: 222\nExamining ./dateparse.c\nExamining ./dateparse.h\nExamining ./test.c\n\nFINAL RESULTS:\n\n./dateparse.c:405:  [4] (buffer) strcpy:\n  Does not check for buffer overflows when copying to destination [MS-banned]\n  (CWE-120). Consider using snprintf, strcpy_s, or strlcpy (warning: strncpy\n  easily misused).\n\n......\n\n./dateparse.c:2192:  [1] (buffer) strlen:\n  Does not handle strings that are not \\0-terminated; if given one it may\n  perform an over-read (it could cause a crash if unprotected) (CWE-126).\n\nANALYSIS SUMMARY:\n\nHits = 64\nLines analyzed = 2719 in approximately 0.04 seconds (61234 lines/second)\nPhysical Source Lines of Code (SLOC) = 1966\nHits@level = [0]  15 [1]  28 [2]  31 [3]   0 [4]   5 [5]   0\nHits@level+ = [0+]  79 [1+]  64 [2+]  36 [3+]   5 [4+]   5 [5+]   0\nHits/KSLOC@level+ = [0+] 40.1831 [1+] 32.5534 [2+] 18.3113 [3+] 2.54323\n    [4+] 2.54323 [5+]   0\nDot directories skipped = 1 (--followdotdir overrides)\nMinimum risk level = 1\n\nNot every hit is necessarily a security vulnerability.\nYou can inhibit a report by adding a comment in this form:\n// flawfinder: ignore\nMake *sure* it's a false positive!\nYou can use the option --neverignore to show these.\n\nThere may be other security vulnerabilities; review your code!\nSee 'Secure Programming HOWTO'\n(https://dwheeler.com/secure-programs) for more information.\n\n\n\nThe codebase oracle is created in the following manner: The source code is first chunked in function-level pieces by traversing the code’s Abstract Syntax Tree (AST) through Clang. Each chunk is represented by an object with the function’s signature, the corresponding filepath and the function’s body (see Listing 3.3). Afterwards, each function body is turned into a vector embedding through an embedding model. Each embedding is stored in the vector store. This structure is created and used for easier and more semantically meaningful code retrieval, and to also combat context window limitations present in LLMs.\n\n\n\nListing 3.3: Sample chunks contained in dateparse’s codebase oracle.\n\n\nFile: dateparse/dateparse.c\nSignature: void (struct parser *, int, int)\nCode:\nstatic void setOffset(struct parser* p, int i, int len){\n        strncpy(p-&gt;offsetbuf, p-&gt;datestr+i, len);\n        p-&gt;offsetbuf[len] = 0;\n}\n\n\nFile: dateparse/dateparse.c\nSignature: void (struct parser *, char *)\nCode:\nstatic void setFullMonth(struct parser* p, char* month){\n        strcpy(p-&gt;mobuf, month);\n}\n\n\nFile: dateparse/dateparse.c\nSignature: int (const char *, long long *, int *, int)\nCode:\nint dateparse(const char* datestr, date_t* t, int *offset, int stringlen){\n        struct parser p;\n        *t = 0;\n        if (!stringlen)\n                stringlen = strlen(datestr);\n        if (parseTime(datestr, &p, stringlen))\n                return -1;\n        return parse(&p, t, offset);\n}\n\n\n\n\n\n3.2.2 Harness Creation\nSecond is the harness creation stage (steps B.1–B.2). In this part, a “generator” ReAct LLM agent is tasked with creating a fuzzing harness for the project. The agent has access to a querying tool that acts as an interface between it and the codebase oracle. When the agent makes queries like “functions containing strcpy()”, the querying tool turns the question into an embedding and through similarity search returns the top k=5 most similar results—in this case, functions of the project. With this approach, the agent is able to explore the codebase semantically and pinpoint potentially vulnerable usage patterns easily.\nThe harness generated by the agent is then compiled using Clang and linked with the AddressSanitizer, LeakSanitizer, and UndefinedBehaviorSanitizer. The compilation command used is generated programmatically, according to the rules described in Section 3.5. If the compilation fails for any reason, e.g. a missing header include, then the generated faulty harness and its compilation output are passed to a new “fixer” agent tasked with repairing any errors in the harness (step B.2.a). This results in a newly generated harness, presumably free from the previously shown flaws. This process is iterated until a compilable harness has been obtained. After success, a script is also exported in the project directory, containing the generated compilation command. Dateparse’s compilation command is shown in Listing 3.4.\n\n\n\nListing 3.4: OverHAuL’s generated compilation command for dateparse.\n\n\n# cat ./overhaul.sh\nclang -g -fsanitize=fuzzer,address,undefined harnesses/harness.c -I . \n dateparse.c -o harness\n\n\n\n\n\n3.2.3 Harness Evaluation\nThird comes the evaluation stage (steps C.1–C.3). During this step, the compiled harness is executed and its results evaluated. Namely, a generated harness passes the evaluation phase if and only if:\n\nThe harness has no memory leaks during its execution\nThis is inferred by the existence of leak-&lt;hash&gt; files.\nA new testcase was created or the harness executed for at least MIN_EXECUTION_TIME (i.e. did not crash on its own)\nWhen a crash happens, and thus a testcase is created, it results in a crash-&lt;hash&gt; file.\nThe created testcase is not empty\nThis is examined through xxd’s output given the crash-file.\n\nSimilarly to the second stage’s compilation phase (steps B.2–B.2.a), if a harness does not pass the evaluation for whatever reason it is sent to an “improver” agent. This agent is instructed to refine it based on its code and cause of failing the evaluation. This process is also iterative. If any of the improved harness versions fail to compile, the aforementioned “fixer” agent is utilized again (steps C.2–C.2.a). All produced crash files and the harness execution output are saved in the project’s directory. An evaluation-passing harness generated for the dateparse project is presented in Listing 3.5, along with the associated crash input and execution output displayed in Listing 3.6 and Listing 3.7, respectively.\n\n\n\nListing 3.5: A crash-finding harness for dateparse, generated through OverHAuL (some comments were removed).\n\n\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdint.h&gt;\n#include \"dateparse.h\"\n\n// ...\nstruct parser {\n    char mobuf[16];\n};\n\n// ...\nstatic void setFullMonth(struct parser* p, char* month){\n    strcpy(p-&gt;mobuf, month);\n}\n\nint LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    // Allocate a parser instance on the heap.\n    struct parser *p = (struct parser*)malloc(sizeof(struct parser));\n    if (!p) {\n        return 0;\n    }\n    // Initialize parser with zeros.\n    memset(p, 0, sizeof(struct parser));\n    // Prepare month string input: ensure null-terminated string for strcpy.\n    char *month = (char*)malloc(size + 1);\n    if (!month) {\n        free(p);\n        return 0;\n    }\n    memcpy(month, data, size);\n    month[size] = '\\0';  // Null terminate to avoid overread in strcpy.\n    // Call the vulnerable function with fuzzed month string.\n    setFullMonth(p, month);\n    // Cleanup\n    free(month);\n    free(p);\n    return 0;\n}\n\n\n\n\n\n\nListing 3.6: An input string that crashes the harness in Listing 3.5. What is shown is its xxd output.\n\n\n00000000: 315e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e 5e5e  1^^^^^^^^^^^^^^^\n00000010: 0a                                       .\n\n\n\n\n\n\nListing 3.7: The output of the harness in Listing 3.5 when executed with Listing 3.6 as input.\n\n\nINFO: Running with entropic power schedule (0xFF, 100).\nINFO: Seed: 2365219758\nINFO: Loaded 1 modules   (3723 inline 8-bit counters): 3723 [0x67ccc0,\n      0x67db4b),\nINFO: Loaded 1 PC tables (3723 PCs): 3723 [0x618d40,0x6275f0),\n./harness: Running 1 inputs 1 time(s) each.\nRunning: crash-7fd6f4dd5d39420d6f7887ff995b4e855ae90c16\n=================================================================\n==10973==ERROR: AddressSanitizer: heap-buffer-overflow on address\n0x7bcece9e00a0 at pc 0x000000526c0e bp 0x7fff3dc0aa20 sp 0x7fff3dc0a1d8\nWRITE of size 18 at 0x7bcece9e00a0 thread T0\n    #0 0x000000526c0d in strcpy\n       (/home/kchou/Bin/Repos/kchousos/OverHAuL/output/dateparse/harness\n       +0x526c0d) (BuildId: d658684b8726dc7e8e768089710d13c96cfc81f0)\n    #1 0x000000585555 in setFullMonth\n       /home/kchou/Bin/Repos/kchousos/OverHAuL/output/dateparse/harnesses\n       /harness.c:18:2\n    #2 0x0000005853fd in LLVMFuzzerTestOneInput\n       /home/kchou/Bin/Repos/kchousos/OverHAuL/output/dateparse/harnesses\n       /harness.c:41:2\n...\nSUMMARY: AddressSanitizer: heap-buffer-overflow\n         (/home/kchou/Bin/Repos/kchousos/OverHAuL/output/dateparse/harness\n         +0x526c0d) (BuildId: d658684b8726dc7e8e768089710d13c96cfc81f0) in\n         strcpy\nShadow bytes around the buggy address:\n  0x7bcece9dfe00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x7bcece9dfe80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x7bcece9dff00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x7bcece9dff80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n  0x7bcece9e0000: fa fa 00 00 fa fa 00 fa fa fa 00 fa fa fa 00 fa\n=&gt;0x7bcece9e0080: fa fa 00 00[fa]fa fa fa fa fa fa fa fa fa fa fa\n  0x7bcece9e0100: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x7bcece9e0180: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x7bcece9e0200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x7bcece9e0280: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\n  0x7bcece9e0300: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\nShadow byte legend (one shadow byte represents 8 application bytes):",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html#sec-techniques",
    "href": "chapters/overhaul.html#sec-techniques",
    "title": "3  OverHAuL’s Design",
    "section": "3.3 OverHAuL Techniques",
    "text": "3.3 OverHAuL Techniques\nThe fundamental techniques that distinguish OverHAuL in its approach and enhance its effectiveness in achieving its objectives are: The implementation of an iterative feedback loop between the LLM agents, the distribution of responsibility across a triplet of distinct agents and the employment of a “codebase oracle” for interacting with the given project’s source code.\n\n3.3.1 Feedback Loop\nThe initial generated harness produced by OverHAuL is unlikely to be successful from the get-go. The iterative feedback loop implemented facilitates its enhancement, enabling the harness to be tested under real-world conditions and subsequently refined based on the results of these tests. This approach mirrors the typical workflow employed by developers in the process of creating and optimizing fuzz targets.\nIn this iterative framework, the development process continues until either an acceptable and functional harness is realized or the defined iteration budget is exhausted. The iteration budget N=10 is initialized at the onset of OverHAuL’s execution and is shared between both the compilation and evaluation phases of the harness development process. This means that the iteration budget is decremented each time a dashed arrow in the flowchart illustrated in Figure 3.1 is followed. Such an approach allows for targeted improvements while maintaining oversight of resource allocation throughout the harness development cycle.\n\n\n3.3.2 React Agents Triplet\nAn integral design decision in our framework is the implementation of each agent as a distinct LLM instance, although all utilizing the same underlying model. This approach yields several advantages, particularly in the context of maintaining separate and independent contexts for each agent throughout each OverHAuL run.\nBy assigning individual contexts to the agents, we enable a broader exploration of possibilities during each run. For instance, the “improver” agent can investigate alternative pathways or strategies that the “generator” agent may have potentially overlooked or internally deemed inadequate inaccurately. This separation not only fosters a more diverse range of solutions but also enhances the overall robustness of the system by allowing for iterative refinement based on each agent’s unique insights.\nFurthermore, this design choice effectively addresses the limitations imposed by context window sizes. By distributing the “cognitive” load across multiple agents, we can manage and mitigate the risks associated with exceeding these constraints. As a result, this architecture promotes efficient utilization of available resources while maximizing the potential for innovative outcomes in multi-agent interactions. This layered approach ultimately contributes to a more dynamic and exploratory research environment, facilitating a comprehensive examination of the problem space.\n\n\n3.3.3 Codebase Oracle\nThe third central technique employed is the creation and utilization of a codebase oracle, which is effectively realized through a vector store. This oracle is designed to contain the various functions within the project, enabling it to return the most semantically similar functions upon querying it. Such an approach serves to address the inherent challenges associated with code exploration difficulties faced by LLM agents, particularly in relation to their limited context window.\nBy structuring the codebase into chunks at the level of individual functions, LLM agents can engage with the code more effectively by focusing on its functional components. This methodology not only allows for a more nuanced understanding of the codebase but also ensures that the responses generated do not consume an excessive portion of the limited context window available to the agents. In contrast, if the codebase were organized and queried at the file level, the chunks of information would inevitably become larger, leading to an increase in noise and a dilution of meaningful content in each chunk [14]. Given the constant size of the embeddings used in processing, each progressively larger chunk would be less semantically significant, ultimately compromising the quality of the retrieval process.\nDefining the function as the primary unit of analysis represents the most proportionate balance between the size of the code segments and their semantic significance. It serves as the ideal “zoom-in” level for the exploration of code, allowing for greater clarity and precision in understanding the functionality of individual code segments. This same principle is widely recognized in the training of code-specific LLMs, where a function-level approach has been shown to enhance performance and comprehension [15]. By adopting this methodology, we aim to foster a more robust interaction between LLM agents and the underlying codebase, ultimately facilitating a more effective and efficient exploration process.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html#high-level-algorithm",
    "href": "chapters/overhaul.html#high-level-algorithm",
    "title": "3  OverHAuL’s Design",
    "section": "3.4 High-Level Algorithm",
    "text": "3.4 High-Level Algorithm\nA pseudocode version of OverHAuL’s main function is shown in Algorithm 1, illustrating the workflow depicted in Figure 3.1 and incorporating the methods explained in Sections 3.2 and 3.3. Notably, within this algorithm, the HarnessAgents() function acts as an interface that connects the “generator”, “fixer”, and “improver” LLM agents. The specific agent utilized during each invocation of HarnessAgents() depends on the function’s arguments. As a result, the harness variable encapsulates all generated, fixed, or improved harnesses. Since both the “fixer” and “generator” agents are accessed through the HarnessAgents() function, the related continue statements correspond to the next iterations of fixing or improving a harness. This design choice streamlines the overall algorithm, making it more abstract and easier to comprehend.\n\n\n\\begin{algorithm} \\caption{OverHAuL}\\begin{algorithmic} \\Require $repository$ \\Ensure $harness, compilation\\_script, crash\\_input, execution\\_log$ \\State $path \\gets$ \\Call{RepoClone}{$repository$} \\State $report \\gets$ \\Call{StaticAnalysis}{$path$} \\State $vector\\_store \\gets$ \\Call{CreateOracle}{$path$} \\State $acceptable \\gets$ False \\State $compiled \\gets$ False \\State $error \\gets$ None \\State $violation \\gets$ None \\State $output \\gets$ None \\For{$i = 1$ to $MAX\\_ITERATIONS$} \\State $harness \\gets$ \\Call{HarnessAgents}{$path, report, vector\\_store, error, violation, output$} \\State $error, compiled \\gets$ \\Call{BuildHarness}{$path, harness$} \\If{$\\neg compiled$} \\State \\textbf{continue} \\Comment{Fix harness} \\EndIf \\State $output, accepted \\gets $\\Call{EvaluateHarness}{$path, harness$} \\If{$\\neg accepted$} \\State \\textbf{continue} \\Comment{Improve harness} \\Else \\State $acceptable \\gets$ True \\State \\textbf{break} \\EndIf \\EndFor \\State \\Return $compiled \\land acceptable$ \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html#sec-scope",
    "href": "chapters/overhaul.html#sec-scope",
    "title": "3  OverHAuL’s Design",
    "section": "3.5 Scope",
    "text": "3.5 Scope\nCurrently, OverHAuL is designed to generate new harnesses specifically for medium-sized C libraries. Given the inherent complexity of dealing with C++ projects, this is not a feature yet supported within the system.\nThe compilation command utilized by OverHAuL is created programmatically. It incorporates the root directory along with all subdirectories that conform to a predefined set of common naming conventions. Additionally, the compilation process uses all C source files identified within these directories. Crucially, it is important that no main() function is present in any of the files to ensure successful compilation. For this reason any files or directories that include “test”, “main”, “example”, “demo”, or “benchmark” in their paths are systematically excluded from the compilation process. This exclusion also decreases the “noise” in the oracle, as these files do not constitute part of the core library and would therefore not contain any functions meaningful to the LLM agents.\nLastly, No support for build systems such as Make or CMake [16], [17] is yet implemented. Such functionality would exponentially increase the complexity of the build step and is beyond the scope of this thesis.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/overhaul.html#sec-implementation",
    "href": "chapters/overhaul.html#sec-implementation",
    "title": "3  OverHAuL’s Design",
    "section": "3.6 Implementation",
    "text": "3.6 Implementation\nIn creating the codebase oracle, we employ the “libclang” Python package [18] to slice functions based on the AST capability provided by Clang. As detailed in Section 3.3.3, the intermediate output consists of a list of Python dictionaries, with each dictionary storing a function’s body, signature, and corresponding file path. Each chunk of function code is then converted into an embedding using OpenAI’s “text-embedding-3-small” model [19] and stored in a FAISS vector store index [20]. This index is mapped to a metadata structure that contains the aforementioned function data—specifically the actual function body, signature, and file path. When a search is conducted on the index, the results returned are the embeddings. The responses that the LLM agent receives are derived from the corresponding metadata entries of each embedding.\nAll LLM agents and components are developed using the DSPy library, a declarative Python framework for LLM programming created by Stanford’s NLP research team [21]. DSPy offers built-in modules and abstractions that facilitate the composition of LLMs and prompting techniques, such as Chain of Thought and ReAct (Listing 3.8). Each agent within OverHAuL is an instance of DSPy’s ReAct module [22], accompanied by a custom Signature [23]—displayed in Appendix C. DSPy was selected over other contemporary LLM libraries, such as LangChain and Llamaindex [24], [25], because of its user-friendliness, logical abstractions, and efficient development process—qualities that are often lacking in these alternative libraries [26], [27], [28].\n\n\n\nListing 3.8: Sample DSPy program.\n\n\nimport dspy\nlm = dspy.LM('openai/gpt-4o-mini', api_key='YOUR_OPENAI_API_KEY')\ndspy.configure(lm=lm)\n\nmath = dspy.ChainOfThought(\"question -&gt; answer: float\")\nmath(question=(\n    \"Two dice are tossed. What is the probability that the sum equals two?\"\n))\n\n\n\nRepository cloning is executed using the --depth 1 flag to minimize disk storage usage and reduce the size of artifacts.\nThe current implementation of OverHAuL sits at 1,254 source lines of Python code.\n\n3.6.1 Development Tools\nThe development of OverHAuL incorporates a variety of tools aimed at enhancing functionality and efficiency. Notably, “uv” is a Python package and project manager written in Rust that serves as a replacement for Poetry. Additionally, “Ruff,” a code linter and formatter also developed in Rust, contributes to code quality by enforcing consistent formatting standards. The project also employs “MyPy,” the widely-used static type checker for Python, to ensure type correctness. Testing is facilitated through “PyTest,” a robust Python testing framework. Lastly, “pdoc” is utilized as a Static Site Generator (SSG) to automate the creation of API documentation2 [29], [30], [31], [32], [33].\n\n\nAvailable at https://kchousos.github.io/OverHAuL/.↩︎\n\n\n\n\n3.6.2 Reproducibility\nOverHAuL’s source code is available at https://github.com/kchousos/OverHAuL. Each benchmark run was conducted within the framework of a GitHub Actions workflow, resulting in a detailed summary accompanied by an artifact containing all cloned repositories. These artifacts are the compressed result directories described in Section 4.1.1 and provide the essential components necessary for the reproducibility each project’s results, as described in Section 3.1. All benchmark runs can be conveniently accessed at https://github.com/kchousos/OverHAuL/actions/workflows/benchmarks.yml.\n\n\n\n\n[1] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[2] L. Torvalds, “Git.” Apr. 07, 2005. Available: https://git-scm.com/\n\n\n[3] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient Estimation of Word Representations in Vector Space,” Sep. 06, 2013. doi: 10.48550/arXiv.1301.3781. Available: http://arxiv.org/abs/1301.3781\n\n\n[4] B. Jeong et al., “UTopia: Automatic Generation of Fuzz Driver using Unit Tests,” in 2023 IEEE Symposium on Security and Privacy (SP), May 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394. Available: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[5] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[6] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[7] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[8] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272\n\n\n[9] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[10] Open Source Security Foundation (OpenSSF), “Ossf/fuzz-introspector.” Open Source Security Foundation (OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[11] Python Software Foundation, “Venv — Creation of virtual environments,” Jul. 17, 2025. Available: https://docs.python.org/3/library/venv.html\n\n\n[12] pip developers, “Pip documentation V25.1.1,” 2025. Available: https://pip.pypa.io/en/stable/\n\n\n[13] D. A. Wheeler, “Flawfinder Home Page.” Available: https://dwheeler.com/flawfinder/\n\n\n[14] S. Zhao, Y. Yang, Z. Wang, Z. He, L. K. Qiu, and L. Qiu, “Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely,” Sep. 23, 2024. doi: 10.48550/arXiv.2409.14924. Available: http://arxiv.org/abs/2409.14924\n\n\n[15] M. Chen et al., “Evaluating Large Language Models Trained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374. Available: http://arxiv.org/abs/2107.03374\n\n\n[16] A. Cedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf, “CMake - Upgrade Your Software Build System.” 2000. Available: https://cmake.org/\n\n\n[17] S. I. Feldman, “Make — a program for maintaining computer programs,” Software: Practice and Experience, vol. 9, no. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[18] T. He, “Sighingnow/libclang.” Jul. 03, 2025. Available: https://github.com/sighingnow/libclang\n\n\n[19] OpenAI Docs, “Text-embedding-3-small - OpenAI API,” 2025. Available: https://platform.openai.com\n\n\n[20] M. Douze et al., “The Faiss library,” Feb. 11, 2025. doi: 10.48550/arXiv.2401.08281. Available: http://arxiv.org/abs/2401.08281\n\n\n[21] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[22] Stanford NLP Team, “Signatures - DSPy Documentation,” 2025. Available: https://dspy.ai/learn/programming/signatures/\n\n\n[23] Stanford NLP Team, “ReAct - DSPy Documentation,” 2025. Available: https://dspy.ai/api/modules/ReAct/\n\n\n[24] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[25] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[26] F. Both, “Why we no longer use LangChain for building our AI agents,” 2024. Available: https://octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents\n\n\n[27] M. Woolf, “The Problem With LangChain,” Jul. 14, 2023. Available: https://minimaxir.com/2023/07/langchain-problem/\n\n\n[28] Woyera, “6 Reasons why Langchain Sucks,” Sep. 08, 2023. Available: https://medium.com/@woyera/6-reasons-why-langchain-sucks-b6c99c98efbe\n\n\n[29] Astral, “Astral-sh/uv.” Astral, Jul. 18, 2025. Available: https://github.com/astral-sh/uv\n\n\n[30] Astral, “Astral-sh/ruff.” Astral, Jul. 18, 2025. Available: https://github.com/astral-sh/ruff\n\n\n[31] A. Cortesi, M. Hils, and T. Kriechbaumer, “Mitmproxy/pdoc.” mitmproxy, Jul. 18, 2025. Available: https://github.com/mitmproxy/pdoc\n\n\n[32] PyTest Dev Team, “Pytest-dev/pytest.” pytest-dev, Jul. 18, 2025. Available: https://github.com/pytest-dev/pytest\n\n\n[33] Python Software Foundation, “Python/mypy.” Python, Jul. 18, 2025. Available: https://github.com/python/mypy",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>OverHAuL's Design</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation.html",
    "href": "chapters/evaluation.html",
    "title": "4  Evaluation",
    "section": "",
    "text": "4.1 Experimental Benchmark\nTo thoroughly assess the performance and effectiveness of OverHAuL, we established four research questions to direct our investigative efforts. These questions are designed to provide a structured framework for our inquiry and to ensure that our research remains focused on the key aspects of OverHAuL’s functionality and impact within its intended domain. By addressing these questions, we aim to uncover valuable insights that will contribute to a deeper understanding of OverHAuL’s capabilities and its position in contemporary automatic fuzzing applications:\nTo evaluate OverHAuL, a benchmarking script was implemented1 and a corpus of ten open-source C libraries was assembled. This collection comprises firstly of user dhvar’s “dateparse” library, which is also used as a running example in OSS-Fuzz-Gen’s [1] experimental from-scratch harnessing feature (Chapter 5). Secondly, nine other libraries chosen randomly2 from the package catalog of Clib, a “package manager for the C programming language” [2], [3]. All libraries can be seen Table 4.1, along with their descriptions.\nOverHAuL was evaluated through the experimental benchmark from 6th of June, 2025 to 18th of July, 2025, using OpenAI’s gpt-4.1-mini model [4]. For these runs, each OverHAuL execution was configured with a 5 minute harness execution timeout and an iteration budget of 10. Each benchmark run was executed as a GitHub Actions workflow on Linux virtual machines with 4-vCPUs and 16GiB of memory hosted on Microsoft Azure [5], [6]. The result directory (as described in Section 4.1.1) for each is available as a downloadable artifact in the corresponding GitHub Actions entry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation.html#sec-benchmark",
    "href": "chapters/evaluation.html#sec-benchmark",
    "title": "4  Evaluation",
    "section": "",
    "text": "Available at https://github.com/kchousos/OverHAuL/blob/master/benchmarks/benchmark.sh.↩︎\nFrom the subset of libraries that do not have exotic external dependencies, like the X11 development toolchain.↩︎\n\n\n\n\n\n\nTable 4.1: The benchmark project corpus. Each project name links to its corresponding GitHub repository. Each is followed by a short description, its GitHub stars count and its Significant Lines of Code (SLOC), as of July 18th, 2025.\n\n\n\n\n\n\n\n\n\n\n\nProject\nDescription\nStars\nSLOC\n\n\n\n\ndvhar/dateparse\nA library that allows parsing dates without knowing the format in advance.\n2\n2272\n\n\nclibs/buffer\nA string manipulation library.\n204\n354\n\n\njwerle/libbeaufort\nA library implementation of the Beaufort cipher [7].\n13\n321\n\n\njwerle/libbacon\nA library implementation of the Baconian cipher [8].\n8\n191\n\n\njwerle/chfreq.c\nA library for computing the character frequency in a string.\n5\n55\n\n\njwerle/progress.c\nA library for displaying progress bars in the terminal.\n76\n357\n\n\nwillemt/cbuffer\nA circular buffer implementation.\n261\n170\n\n\nwillemt/torrent-reader\nA torrent-file reader library.\n6\n294\n\n\norangeduck/mpc\nA type-generic parser combinator library.\n2,753\n3632\n\n\nh2non/semver.c\nA semantic version v2.0 parsing and rendering library [9].\n190\n608\n\n\n\n\n\n\n\n4.1.1 Local Benchmarking\nTo run the benchmark locally, one would need to follow the installation instructions in Section 3.1 and then execute the benchmarking script, like so:\n$ ./benchmarks/benchmark.sh\nThe cloned repositories with their corresponding harnesses will then be located in a subdirectory of benchmark_results, which will have the name format of mini__&lt;timestamp&gt;__ReAct__&lt;llm-model&gt;__&lt;max-exec-time&gt;__&lt;iter-budget&gt;. “Mini” corresponds to the benchmark project corpus described above, since a 30-project corpus was initially created and is now coined as “full” benchmark. Both the mini and full benchmarks are located in benchmarks/repos.txt and benchmarks/repos-mini.txt respectively. To execute the benchmark for the “full” corpus, users can add the -b full flag in the script’s invocation. Also, the LLM model used can be defined with the -m command-line flag.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation.html#sec-results",
    "href": "chapters/evaluation.html#sec-results",
    "title": "4  Evaluation",
    "section": "4.2 Results",
    "text": "4.2 Results\nThe outcomes of the benchmark experiments are shown in Figure 4.1. To ensure the reliability of these results, each reported crash was manually validated to confirm that it stemmed from genuine defects within the target library, rather than issues of the generated harness. An iteration heatmap was also generated for the verifiably fuzzed projects, displayed in Figure 4.2. With these validated findings, we are now positioned to address the initial research questions posed in this chapter.\n\n\n\n\n\n\nFigure 4.1: The benchmark results for OverHAuL are illustrated with the y-axis depicting the ten-project corpus outlined in Section 4.1. The x-axis represents the various benchmark runs. Each label constitutes a unique hash identifier corresponding to a specific GitHub Actions workflow run, which can be accessed at https://github.com/kchousos/OverHAuL/actions/runs/HASH. An overview of all benchmark runs is available at https://github.com/kchousos/OverHAuL/actions/workflows/benchmarks.yml. In this matrix, a green/1 block indicates that OverHAuL successfully generated a new harness for the project and was able to find a crash input. On the other hand, a yellow/0 block indicates that while a compilable harness was produced, no crash input was found within the five-minute execution period. Finally, an orange/-2 block means that the crash that was found derives from errors in the harness itself. AImportantly, there are no red/-1 blocks, which would indicate cases where a compilable harness could not be generated.\n\n\n\n\n\n\n\n\n\nFigure 4.2: This heatmap illustrates the number of iterations required for each project to be successfully harnessed, as determined by the benchmark results. Higher color intensity corresponds to a greater number of iterations needed for successful harnessing. Cells left blank signify instances where no valid harness was generated.\n\n\n\n\n4.2.1 RQ 1: Can OverHAuL generate working harnesses for unfuzzed C projects?\nOverHAuL demonstrates a strong capability in generating working harnesses for previously unfuzzed C projects. In benchmark evaluations, it achieved a success rate of 81.25% in producing fuzzing harnesses that were effective at uncovering crash-inducing inputs in target programs. Notably, all harnesses generated by OverHAuL were valid C programs—an improvement over prior methods such as OSS-Fuzz-Gen [1], which occasionally outputs the LLM’s markdown answers instead. The harnesses consistently utilized existing functions obtained from the codebase oracle and interacted appropriately with the Library Under Test’s API, with only minimal instances of irrelevant or hallucinated code observed. While the potential exists for non-compilable harnesses to be generated, the benchmark results included no such cases, underscoring the significance and effectiveness of compilation feedback and the integrated “fixer” agent in OverHAuL’s workflow. These findings collectively indicate that OverHAuL is effective at generating robust, valid, and meaningful harnesses for C projects lacking previous fuzzing infrastructure.\n\n\n4.2.2 RQ2: What characteristics do these harnesses have? Are they similar to man-made harnesses?\nIn examining the characteristics of the generated harnesses, we observe several notable patterns. The harnesses are typically well-commented, a result of explicit instructions given to the language models. They are designed to target various levels of the library’s functionality. In some cases, they focus on higher-level entry point functions (Section B.3), while in other instances, they concentrate on more narrowly scoped internal functions (Listing 3.5). Usually the generated fuzz targets are clear and closely resemble the kind of harnesses a skilled software engineer might write. These harnesses make appropriate and sensible use of the target API, as illustrated in examples such as Listing 3.5. However, some harnesses do exhibit the use of unexplained constants or idiosyncratic control flow constructs, which can hinder comprehensibility and may introduce errors. Additionally, we find that the characteristics of generated harnesses can vary substantially across different projects and even between runs, with differences evident in both their size and complexity (see Appendix B). Overall, while the generated harnesses often echo the structure and intent of man-made harnesses, inconsistencies and occasional inexplicable design choices distinguish them from their manually written counterparts.\n\n\n4.2.3 RQ3: How do LLM usage patterns influence the generated harnesses?\nThe effectiveness of LLM-driven fuzzing harness generation in OverHAuL is heavily influenced by two primary factors: model selection and prompting strategies. The experimental evaluation presents compelling evidence regarding the substantial impact of both dimensions.\nAll benchmark experiments on GitHub’s infrastructure were conducted using OpenAI’s gpt-4.1-mini. Preliminary local testing included a spectrum of models—gpt-4.1, gpt-4o, gpt-4, and gpt-3.5-turbo. Notably, both gpt-4.1 and gpt-4.1-mini achieved comparable performance, consistently generating robust fuzzing harnesses. In contrast, gpt-4o yielded somewhat average results, while gpt-4 and gpt-3.5-turbo exhibited significantly inferior performance, averaging only 2 out of 10 projects successfully harnessed per benchmark run. Models with suboptimal performance were excluded in subsequent development phases. These findings underscore the necessity of selecting advanced LLM architectures to realize OverHAuL’s potential; in particular, gpt-4o represents a recent baseline for acceptable performance. Because LLM model capabilities are evolving rapidly, it is reasonable to anticipate ongoing improvements in OverHAuL’s harness-generation efficacy as newer LLMs become available.\nPrompting methodology is equally crucial. The adoption of ReAct prompting has proven most effective in the current implementation of OverHAuL [10]. Alternative prompting paradigms—including zero-shot and Chain-of-Thought (COT) approaches [11]—were empirically evaluated, as detailed in Appendix A, but failed to deliver satisfactory outcomes. A central challenge in automated harness generation involves ensuring that the resulting harness is both compilable and operationally effective. This alignment with real-world constraints necessitates continuous interaction between the LLM and the target environment, best achieved through agentic workflows [12]. The superior performance of ReAct prompting likely stems from its structured approach to iterative code exploration and refinement, facilitating a cycle of observation, planning, and action that is particularly well-suited to harness synthesis.\nA central element of OverHAuL’s architecture is its triplet of ReAct agents, each contributing a distinct role in the collaborative generation of fuzzing harnesses. Local benchmarking demonstrates an almost linear increase in success rates with the number of iteration cycles, underscoring the efficacy of agentic collaboration and iterative refinement in enhancing harness quality. As illustrated in Figure 4.2, projects such as “dateparse” and “semver.c” exhibit marked improvements when afforded larger iteration budgets. This trend highlights the pivotal roles of the “fixer” and “improver” agents, whose interventions enable the system to surmount challenges present in initial harness generations, ultimately advancing the caliber of the final outputs.\nAdditionally, the inclusion of a codebase oracle is instrumental in scaling code exploration efficiently. Unlike previously tested methods (see Appendix A), the codebase oracle enables comprehensive traversal and understanding of project code, overcoming the token and context window limitations typically associated with LLMs.\nIn summary, the findings for RQ3 indicate that continuous advancements in LLM technology and prompting architectures will further enhance the ability of systems like OverHAuL to automate efficient fuzzing harness generation. Integrating agentic modules that can dynamically assess their environment and incorporate runtime feedback will likely outperform more static LLM applications, particularly within the domain of automated fuzzing.\n\n\n4.2.4 RQ4: How do different symbolic techniques affect the generated harnesses?\nThroughout the development of OverHAuL and its various iterations, numerous programming techniques were assessed in pursuit of answering RQ4 (Appendix A). Simple source code concatenation and its subsequent injection into LLM prompts revealed significant limitations, primarily due to the constraints of context windows. Conversely, the usage of tools capable of retrieving file contents marked a meaningful advancement. Nonetheless, this approach still encountered challenges, such as inaccessible code blocks and exploration that lacked semantic relevance. In response to these difficulties, the implementation of a function-level vector store functioning as a codebase oracle is proposed as a highly scalable solution. This strategy not only enhances the organization of larger files but also accommodates expanding project sizes, facilitating more semantically meaningful code examination.\nThe significance of the iterative feedback loop is clearly demonstrated by the results presented in Figure 4.2. Analysis of the heatmap reveals that earlier versions of OverHAuL, which employed a one-shot approach to harness generation, achieved a success rate of only 28.75%. In contrast, the current implementation shows that 42 out of 65 projects successfully fuzzed (64.62%) did not produce a successful harness in the initial attempt and therefore benefited from the iterative feedback process. Notably, two projects (3.07%) required the full allocation of eight iterations, underscoring the necessity of maintaining a generous iteration budget to maximize effectiveness.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/evaluation.html#discussion",
    "href": "chapters/evaluation.html#discussion",
    "title": "4  Evaluation",
    "section": "4.3 Discussion",
    "text": "4.3 Discussion\nAs discussed in Section Section 4.2, the capabilities and effectiveness of OverHAuL are closely tied to the choice of the underlying large language model. OverHAuL’s modular architecture ensures that advances in LLM research will directly enhance its performance. Each release of a new, more capable model can be readily integrated, thereby amplifying OverHAuL’s effectiveness without the need for substantial redesign.\nA noteworthy consideration in our benchmarking setup is the possibility that some of the open-source libraries evaluated may have been included in the LLM’s training data. This introduces a risk of overestimating OverHAuL’s performance on code that is unseen or proprietary. Results for closed-source or less widely available libraries could therefore be weaker. Nonetheless, this potential limitation can theoretically be addressed through targeted fine-tuning of the LLM [13], [14].\n\n4.3.1 Threats to Validity\nOur evaluation of OverHAuL was conducted on ten relatively obscure open-source C libraries representing a range of application domains and functionalities. While this selection reduces the likelihood that these projects were used in LLM training and thus minimizes potential bias, it remains uncertain how transferable our results are to larger, more complex, or structurally different codebases. Factors such as varying design paradigms, architectural patterns, or real-world deployment contexts may pose new challenges for OverHAuL’s scalability and effectiveness.\nAdditionally, the risk of LLM hallucination constitutes an internal threat to validity. Such hallucinations may require multiple attempts or occasional manual adjustments to produce valid and useful fuzz drivers. However, because LLMs—and thus OverHAuL—operate in a non-deterministic manner, it is possible to rerun the process and obtain alternative results. The inherent stochasticity of the underlying LLMs thus allows users to recover from initial failures, ensuring that the impact of hallucinations remains limited to efficiency rather than undermining the core applicability of the approach.\nIn summary, while our findings demonstrate the potential of OverHAuL, they also highlight important limitations and directions for future work, especially in improving robustness and evaluating performance across a broader spectrum of software projects.\n\n\n\n\n[1] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[2] Clibs Project, “Clib Packages,” 2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[3] Clibs Project, “Clibs/clib.” clibs, Jul. 01, 2025. Available: https://github.com/clibs/clib\n\n\n[4] OpenAI Docs, “GPT-4.1 mini - Open AI API,” 2025. Available: https://platform.openai.com\n\n\n[5] GitHub Docs, “About GitHub-hosted runners,” 2025. Available: https://docs-internal.github.com/en/actions/concepts/runners/about-github-hosted-runners\n\n\n[6] GitHub Docs, “Choosing the runner for a job,” 2025. Available: https://docs-internal.github.com/en/actions/how-tos/writing-workflows/choosing-where-your-workflow-runs/choosing-the-runner-for-a-job\n\n\n[7] O. I. Franksen, “Babbage and cryptography. Or, the mystery of Admiral Beaufort’s cipher,” Mathematics and Computers in Simulation, vol. 35, no. 4, pp. 327–367, 1993, Available: https://www.sciencedirect.com/science/article/pii/037847549390063Z\n\n\n[8] F. Bacon, Of the Proficience and Advancement of Learning... Edited by the Rev. GW Kitchin. Bell & Daldy, 1861.\n\n\n[9] T. Preston-Werner, “Semantic Versioning 2.0.0.” Available: https://semver.org/\n\n\n[10] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[11] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[12] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[13] OpenAI Docs, “Model optimization - OpenAI API,” 2025. Available: https://platform.openai.com\n\n\n[14] S. Kim and S. Lee, “Performance Comparison of Prompt Engineering and Fine-Tuning Approaches for Fuzz Driver Generation Using Large Language Models,” in Innovative Mobile and Internet Services in Ubiquitous Computing, L. Barolli, H.-C. Chen, and K. Yim, Eds., Cham: Springer Nature Switzerland, 2025, pp. 111–120. doi: 10.1007/978-3-031-96093-2_12",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/related.html",
    "href": "chapters/related.html",
    "title": "5  Related work",
    "section": "",
    "text": "5.1 Static and Dynamic Analysis-Powered Fuzzing\nAutomated testing, automated fuzzing and automated harness creation have a long research history. Still, a lot of ground remains to be covered until true automation of these tasks is achieved. Until the introduction of transformers [1] and the 2020’s boom of commercial GPTs [2], automation regarding testing and fuzzing was mainly attempted through static and dynamic program analysis methods. These approaches are still utilized, but the fuzzing community has shifted almost entirely to researching the incorporation and employment of LLMs in the last half decade [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]. The most significant and recent works in this field can be categorized according to their primary methodologies—whether they employ program analysis techniques or LLMs—and by the extent to which they depend on external resources beyond the source code. It is important to note that these categories are not mutually exclusive.\nThese tools employ both dynamic and static analyses of source code, as well as LLMs to enhance the automated generation of effective fuzz drivers.\nKLEE [13] is a seminal and widely cited symbolic execution engine introduced in 2008 by Cadar et al. It was designed to automatically generate high-coverage test cases for programs written in C, using symbolic execution to systematically explore the control flow of a program. KLEE operates on the LLVM [14] bytecode representation of programs. Instead of executing a program on concrete inputs, KLEE performs symbolic execution—that is, it runs the program on symbolic inputs, which represent all possible values simultaneously. At each conditional branch, KLEE explores both paths by forking the execution and accumulating path constraints (i.e., logical conditions on input variables) along each path. This enables it to traverse many feasible execution paths in the program, including corner cases that may be difficult to reach through random testing or manual test creation. When an execution path reaches a terminal state (e.g., a program exit, an assertion failure, or a segmentation fault) KLEE uses a constraint solver to compute concrete input values that satisfy the accumulated constraints for that path. These values form a test case that will deterministically drive the program down that specific path when executed concretely.\nIRIS [3] is a 2025 open-source neurosymbolic system for static vulnerability analysis. Given a codebase and a list of user-specified Common Weakness Enumerations (CWEs), it analyzes source code to identify paths that may correspond to known vulnerability classes. IRIS combines symbolic analysis—such as control- and data-flow reasoning—with neural models trained to generalize over code patterns. It outputs candidate vulnerable paths along with explanations and CWE references. The system operates on full repositories and supports extensible CWE definitions.\nIntelliGen [15] is a system for automatically synthesizing fuzz drivers by statically identifying potentially vulnerable entry-point functions within C projects. Implemented using LLVM [14], IntelliGen focuses on Improving fuzzing efficiency by targeting code more likely to contain memory safety issues, rather than exhaustively fuzzing all available functions. The system comprises of two main components: the Entry Function Locator and the Fuzz Driver Synthesizer. The Entry Function Locator analyzes the project’s AST and classifies functions based on heuristics that indicate vulnerability. These include pointer dereferencing, calls to memory-related functions (e.g., memcpy, memset), and invocation of other internal functions. Functions that score highly on these metrics are prioritized for fuzz driver generation. The guiding insight is that entry points with fewer argument checks and more direct memory operations expose more useful program logic for fuzz testing. The Fuzz Driver Synthesizer then generates harnesses for these entry points. For each target function, it synthesizes an LLVMFuzzerTestOneInput function that invokes the target with arguments derived from the fuzz input. This process involves inferring argument types from the source code and ensuring that runtime behavior does not violate memory safety—thus avoiding invalid inputs that would cause crashes unrelated to genuine bugs.\nCKGFuzzer [16] is a fuzzing framework designed to automate the generation of effective fuzz drivers for C/C++ libraries by leveraging static analysis and LLMs. Its workflow begins by parsing the target project along with any associated library APIs to construct a code knowledge graph. This involves two primary steps: first, parsing the AST, and second, performing inter-procedural program analysis. Through this process, CKGFuzzer extracts essential program elements such as function signatures and implementations, and call relationships. Using the knowledge graph, CKGFuzzer then identifies and queries meaningful API combinations, focusing on those that are either frequently invoked together or exhibit functional similarity. It generates candidate fuzz drivers for these combinations and attempts to compile them. Any compilation errors encountered are automatically repaired using heuristics and domain knowledge. A dynamically updated knowledge base, constructed from prior library usage patterns, guides both the generation and repair processes. Once the drivers are successfully compiled, CKGFuzzer executes them while monitoring code coverage. It uses coverage feedback to iteratively mutate underperforming API combinations, refining them until new execution paths are discovered or a preset mutation budget is exhausted. Finally, any crashes triggered during fuzzing are subjected to a reasoning process based on chain-of-thought prompting [17] (Section 2.2.2). To help determine their severity and root cause, CKGFuzzer consults an LLM-generated knowledge base containing real-world examples of vulnerabilities mapped to known CWE entries.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/related.html#extra-resources-required",
    "href": "chapters/related.html#extra-resources-required",
    "title": "5  Related work",
    "section": "5.2 Extra Resources Required",
    "text": "5.2 Extra Resources Required\nThe following works necessitate the presence of client code and/or unit tests that interact with the program’s API. These works utilize and modify such existing code to create enhanced fuzzing harnesses.\nFUDGE [12] is a closed-source tool, made by Google, for automatic harness generation of C and C++ projects based on existing client code. It was used in conjunction with and in the improvement of Google’s OSS-Fuzz [18]. Being deployed inside Google’s infrastructure, FUDGE continuously examines Google’s internal code repository, searching for code that uses external libraries in a meaningful and “fuzzable” way (i.e. predominantly for parsing). If found, such code is sliced [19] based on its Abstract Syntax Tree (AST) using LLVM’s Clang tool [14]. The above process results in a set of abstracted mostly-self-contained code snippets that make use of a library’s calls and/or API. These snippets are later synthesized into the body of a fuzz driver, with variables being replaced and the fuzz input being utilized. Each is then injected in an LLVMFuzzerTestOneInput function and finalized as a fuzzing harness. A building and evaluation phase follows for each harness, where they are executed and examined. Every passing harness along with its evaluation results is stored in FUDGE’s database, reachable to the user through a custom web-based UI.\nUTopia [8] (stylized UTopia) is an open-source automatic harness generation framework. Aside from the library code, It operates solely on user-provided unit tests since, according to Jeong et al. [8], they are a resource of complete and correct API usage examples containing working library set-ups and tear-downs. Additionally, each of them are already close to a fuzz target, in the sense that they already examine a single and self-contained API usage pattern. Each generated harness follows the same data flow of the originating unit test. Static analysis is employed to figure out what fuzz input placement would yield the most results. It is also utilized in abstracting the tests away from the syntactical differences between testing frameworks, along with slicing and AST traversing using Clang.\nAnother project of Google is FuzzGen [11], this time open-source. Like FUDGE, it leverages existing client code of the target library to create fuzz targets for it. FuzzGen uses whole-system analysis, through which it creates an Abstract API Dependence Graph (A2DG). It uses the latter to automatically generate LibFuzzer-compatible harnesses. For FuzzGen to work, the user needs to provide both client code and/or tests for the API and the API library’s source code as well. FuzzGen uses the client code to infer the correct usage of the API and not its general structure, in contrast to FUDGE. FuzzGen’s workflow can be divided into three phases: 1. API usage inference. By analyzing client code and tests, FuzzGen recognizes which functions belong to the library and learns its correct API usage patterns. This process is done with the help of Clang. To test if a function is actually a part of the library, a sample program is created and compiled. If the program compiles successfully, then the function is indeed a valid API call. 2. A2DG construction mechanism. For all the existing API calls, FuzzGen builds an A2DG to record the API usages and infers its intended structure. After completion, this directed graph contains all the valid API call sequences found in the client code corpus. It is built in a two-step process: First, many smaller A2DGs are created, one for each root function per client code snippet. Once such graphs have been created for all the available client code instances, they are combined to formulate the master A2DG. 3. Fuzzer generator. Through the A2DG, a fuzzing harness is created. Contrary to FUDGE, FuzzGen does not create multiple “simple” harnesses but a single complex one with the goal of covering the whole A2DG.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/related.html#only-source-code-required",
    "href": "chapters/related.html#only-source-code-required",
    "title": "5  Related work",
    "section": "5.3 Only Source Code Required",
    "text": "5.3 Only Source Code Required\nThe approaches described in this section enable the creation of new fuzzing harnesses using exclusively the source code of the target library.\nOSS-Fuzz [18], [20] is a continuous, scalable and distributed cloud fuzzing solution for critical and prominent open-source projects. Developers of such software can submit their projects to OSS-Fuzz’s platform, where its harnesses are built and constantly executed. This results in multiple bug findings that are later disclosed to the primary developers and are later patched. OSS-Fuzz started operating in 2016, an initiative in response to the Heartbleed vulnerability [21], [22], [23]. Its hope is that through more extensive fuzzing such errors could be caught and corrected before having the chance to be exploited and thus disrupt the public digital infrastructure. So far, it has helped uncover over 10,000 security vulnerabilities and 36,000 bugs across more than 1,000 projects, significantly enhancing the quality and security of major software like Chrome, OpenSSL, and Systemd. A project that’s part of OSS-Fuzz must have been configured as a ClusterFuzz [24] project. ClusterFuzz is the fuzzing infrastructure that OSS-Fuzz uses under the hood and depends on Google Cloud Platform services, although it is possible to host it locally. Such an integration requires setting up a build pipeline, fuzzing jobs and expects a Google Developer account. Results are accessible through a web interface. ClusterFuzz, and by extension OSS-Fuzz, supports fuzzing through LibFuzzer, AFL++, Honggfuzz and FuzzTest—successor to Centipede— with the last two being Google projects [25], [26], [27], [28]. C, C++, Rust, Go, Python and Java/JVM projects are supported.\nOSS-Fuzz-Gen (OFG) [6], [29] is Google’s current state-of-the-art project regarding automatic harness generation through LLMs. It’s purpose is to improve the fuzzing infrastructure of open-source projects that are already integrated into OSS-Fuzz. Given such a project, OSS-Fuzz-Gen uses its preexisting fuzzing harnesses and modifies them to produce new ones. Its architecture can be described as follows: 1. With an OSS-Fuzz project’s GitHub repository link, OSS-Fuzz-Gen iterates through a set of predefined build templates and generates potential build scripts for the project’s harnesses. 2. If any of them succeed they are once again compiled, this time through fuzz-introspector [30]. The latter constitutes a static analysis tool, with fuzzer developers specifically in mind. 3. Build results, old harness and fuzz-introspector report are included in a template-generated prompt, through which an LLM is called to generate a new harness. 4. The newly generated fuzz target is compiled and if it is done so successfully it begins execution inside OSS-Fuzz’s infrastructure. This method proves to be meaningful, with code coverage in fuzz campaigns increasing thanks to the new generated fuzz drivers. In the case of the tinyxml2 project [31], line coverage went from 38% to 69% without any manual interventions [29]. In 2024, OSS-Fuzz-Gen introduced an experimental feature for generating harnesses in previously unfuzzed projects, meaning preexisting harnesses are no longer required [32]. Although this would be a step forwrard, this feature seems to have been abandonded. The code for this feature resides in the experimental/from_scratch directory of the project’s GitHub repository [6], with the latest known working commit being 171aac2 and the latest overall commit being four months ago, as of this writing.\nAutoGen [4] is a closed-source tool that generates new fuzzing harnesses, given only the library code and documentation. The user specifies the function for which a harness is to be generated. AutoGen gathers information for this function—such as the function body, used header files, function calling examples—from the source code and documentation1. Through specific prompt templates containing the above information, an LLM is tasked with generating a new fuzz driver, while another is tasked with generating a compilation command for said driver. If the compilation fails, both LLMs are called again to fix the problem, whether it was on the driver’s or command’s side. This loop iterates until a predefined maximum value or until a fuzz driver is successfully generated and compiled. If the latter is the case, it is then executed. If execution errors exist, the LLM responsible for the driver generation is used to correct them. If not, the pipeline has terminated and a new fuzz driver has been successfully generated.\n\n\nTherefore, while no pre-existing client code is necessary, available documentation remains essential.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/related.html#sec-differences",
    "href": "chapters/related.html#sec-differences",
    "title": "5  Related work",
    "section": "5.4 Differences With OverHAuL",
    "text": "5.4 Differences With OverHAuL\nOverHAuL differs, in some way, with each of the aforementioned works. Firstly, although KLEE and IRIS [3], [13] tackle the problem of automated testing and both IRIS and OverHAuL can be considered neurosymbolic AI tools, the similarities end there. None of them utilize LLMs the same way we do—with KLEE not utilizing them at all, as it precedes them chronologically—and neither are automating any part of the fuzzing process.\nWhen it comes to FUDGE, FuzzGen and UTopia [8], [11], [12], all three depend on and demand existing client code and/or unit tests. On the other hand, OverHAuL requires only the bare minimum: the library code itself. Another point of difference is that in contrast with OverHAuL, these tools operate in a linear fashion. No feedback is produced or used in any step and any point failure results in the termination of the entire run.\nOverHAuL challenges a common principle of these tools, stated explicitly in FUDGE’s paper [12]: “Choosing a suitable fuzz target (still) requires a human”. OverHAuL chooses to let the LLM, instead of the user, explore the available functions and choose one to target in its fuzz driver.\nBoth IntelliGen and CKGFuzzer [15], [16] depend primarily on programmatic analysis of the target projects—like type inference and knowledge graph construction, respectively. In contrast, OverHAuL delegates a greater portion of this analytical workload to LLM agents, leveraging their reasoning capabilities to achieve more accurate and reliable outcomes.\nOSS-Fuzz-Gen [6] can be considered a close counterpart of OverHAuL, and in some ways it is. A lot of inspiration was gathered from it, like for example the inclusion of static analysis and its usage in informing the LLM. Yet, OSS-Fuzz-Gen has a number of disadvantages that make it in some cases an inferior option. For one, OFG is tightly coupled with the OSS-Fuzz platform [18], which even on its own creates a plethora of issues for the common developer. To integrate their project into OSS-Fuzz, they would need to: Transform it into a ClusterFuzz project [24] and take time to write harnesses for it. Even if these prerequisites are carried out, it probably would not be enough. Per OSS-Fuzz’s documentation [20]: “To be accepted to OSS-Fuzz, an open-source project must have a significant user base and/or be critical to the global IT infrastructure”. This means that OSS-Fuzz is a viable option only for a small minority of open-source developers and maintainers. One countermeasure of the above shortcoming would be for a developer to run OSS-Fuzz-Gen locally. This unfortunately proves to be an arduous task. As it is not meant to be used standalone, OFG is not packaged in the form of a self-contained application. This makes it hard to setup and difficult to use interactively. Like in the case of FUDGE, OFG’s actions are performed linearly. No feedback is utilized nor is there graceful error handling in the case of a step’s failure. Even in the case of the experimental feature for bootstrapping unfuzzed projects, OFG’s performance varies heavily. During experimentation, a lot of generated harnesses were still wrapped either in Markdown backticks or &lt;code&gt; tags, or were accompanied with explanations inside the generated .c source file. Even if code was formatted correctly, in many cases it missed necessary headers for compilation or used undeclared functions.\nLastly, the closest counterpart to OverHAuL is AutoGen [4]. Their similarity stands in the implementation of a feedback loop between LLM and generated harness. However, most other implementation decisions remain distinct. One difference regards the fuzzed function. While AutoGen requires a target function to be specified by the user in which it narrows during its whole run, OverHAuL delegates this to the LLM, letting it explore the codebase and decide by itself the best candidate. Another difference lies in the need—and the lack of—of documentation. While AutoGen requires it to gather information for the given function, OverHAuL leans into the role of a developer by reading the related code and comments and thus avoiding any mismatches between documentation and code. Finally, the LLMs’ input is built based on predefined prompt templates, a technique also present in OSS-Fuzz-Gen. OverHAuL operates one abstraction level higher, leveraging DSPy [33] for programming instead of prompting the LLMs used.\nIn conclusion, OverHAuL constitutes an open-source tool that offers new functionality by offering a straightforward installation process, packaged as a self-contained Python package with minimal external dependencies. It also introduces novel approaches compared to previous work by\n\nImplementing a feedback mechanism between harness generation, compilation, and evaluation phases,\nUsing autonomous ReAct agents capable of codebase exploration,\nLeveraging a vector store for code consumption and retrieval.\n\n\n\n\n\n[1] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[2] OpenAI, “ChatGPT,” 2025. Available: https://chatgpt.com\n\n\n[3] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[4] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272\n\n\n[5] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[6] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[7] H. Green and T. Avgerinos, “GraphFuzz: Library API fuzzing with lifetime-aware dataflow graphs,” in Proceedings of the 44th International Conference on Software Engineering, Pittsburgh Pennsylvania: ACM, May 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228. Available: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[8] B. Jeong et al., “UTopia: Automatic Generation of Fuzz Driver using Unit Tests,” in 2023 IEEE Symposium on Security and Privacy (SP), May 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394. Available: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[9] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[10] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[13] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[14] LLVM Project, “The LLVM Compiler Infrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[15] M. Zhang, J. Liu, F. Ma, H. Zhang, and Y. Jiang, “IntelliGen: Automatic Driver Synthesis for FuzzTesting,” Mar. 01, 2021. doi: 10.48550/arXiv.2103.00862. Available: http://arxiv.org/abs/2103.00862\n\n\n[16] H. Xu et al., “CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph,” Dec. 20, 2024. doi: 10.48550/arXiv.2411.11532. Available: http://arxiv.org/abs/2411.11532\n\n\n[17] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[18] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[19] N. Sasirekha, A. Edwin Robert, and M. Hemalatha, “Program Slicing Techniques and its Applications,” IJSEA, vol. 2, no. 3, pp. 50–64, Jul. 2011, doi: 10.5121/ijsea.2011.2304. Available: http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf\n\n\n[20] OSS-Fuzz, “OSS-Fuzz Documentation,” 2025. Available: https://google.github.io/oss-fuzz/\n\n\n[21] CVE Program, “CVE - CVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[22] D. Wheeler, “How to Prevent the next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[23] Blackduck, Inc., “Heartbleed Bug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[24] Google, “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[25] LLVM Project, “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[26] Google, “Google/fuzztest.” Google, Jul. 10, 2025. Available: https://github.com/google/fuzztest\n\n\n[27] Google, “Google/honggfuzz.” Google, Jul. 10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[28] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[29] D. Liu, J. Metzman, O. Chang, and G. O. S. S. Team, “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier,” Aug. 16, 2023. Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[30] Open Source Security Foundation (OpenSSF), “Ossf/fuzz-introspector.” Open Source Security Foundation (OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[31] L. Thomason, “Leethomason/Tinyxml2.” Jul. 10, 2025. Available: https://github.com/leethomason/tinyxml2\n\n\n[32] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[33] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/future.html",
    "href": "chapters/future.html",
    "title": "6  Future Work",
    "section": "",
    "text": "6.1 Enhancements to Core Features\nThe prototype implementation of OverHAuL offers a compelling demonstration of its potential to automate the fuzzing process for open-source libraries, providing tangible benefits to developers and maintainers alike. This initial version successfully validates the core design principles underpinning OverHAuL, showcasing its ability to streamline and enhance the software testing workflow through automated generation of fuzz drivers using large language models. Nevertheless, while these foundational capabilities lay a solid groundwork, numerous avenues exist for further expansion, refinement, and rigorous evaluation to fully realize the tool’s potential and adapt to evolving challenges in software quality assurance.\nEnhancing OverHAuL’s core functionality represents a primary direction for future development. First, expanding support to encompass a wider array of build systems commonly employed in C and C++ projects—such as GNU Make, CMake, Meson, and Ninja [1], [2], [3], [4]—would significantly broaden the scope of libraries amenable to automated fuzzing using OverHAuL. This advancement would enable OverHAuL to scale effectively and be applied to larger, more complex codebases, thereby increasing its practical utility and impact.\nSecond, integrating additional fuzzing engines beyond LibFuzzer stands out as a strategic enhancement. Incorporation of widely adopted fuzzers like AFL++ [5] could diversify the fuzzing strategies available to OverHAuL, while exploring more experimental tools such as GraphFuzz [6] may pioneer specialized approaches for certain code patterns or architectures. Multi-engine support would also facilitate extending language coverage, for instance by incorporating fuzzers tailored to other programming ecosystems—for example, Google’s Atheris for Python projects [7]. Such versatility would position OverHAuL as a more universal fuzzing automation platform.\nThird, the evaluation component of OverHAuL presents an opportunity for refinement through more sophisticated analysis techniques. Beyond the current criteria, incorporating dynamic metrics such as differential code coverage tracking between generated fuzz harnesses would yield deeper insights into test quality and coverage completeness. This quantitative evaluation could guide iterative improvements in fuzz driver generation and overall testing effectiveness.\nFinally, OverHAuL’s methodology could be extended to leverage existing client codebases and unit tests in addition to the library source code itself, resources that for now OverHAuL leaves untapped. Inspired by approaches like those found in FUDGE and FuzzGen [8], [9], this enhancement would enable the tool to exploit programmer-written usage scenarios as seeds or contexts, potentially generating more meaningful and targeted fuzz inputs. Incorporating these richer information sources would likely improve the efficacy of fuzzing campaigns and uncover subtler bugs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/future.html#experimentation-with-large-language-models-and-data-representation",
    "href": "chapters/future.html#experimentation-with-large-language-models-and-data-representation",
    "title": "6  Future Work",
    "section": "6.2 Experimentation with Large Language Models and Data Representation",
    "text": "6.2 Experimentation with Large Language Models and Data Representation\nOverHAuL’s reliance on large language models (LLMs) invites comprehensive experimentation with different providers and architectures to assess their comparative strengths and limitations. Conducting empirical evaluations across leading models—such as OpenAI’s o1 and o3 families and Anthropic’s Claude Opus 4—will provide valuable insights into their capabilities, cost-efficiency, and suitability for fuzz driver synthesis. Additionally, specialized code-focused LLMs, including generative and fill-in models like Codex-1 and CodeGen [10], [11], [12], merit exploration due to their targeted optimization for source code generation and understanding.\nAnother dimension worthy of investigation concerns the granularity of code chunking employed during the given project’s code processing stage. Whereas the current approach partitions code at the function level, experimenting with more nuanced segmentation strategies—such as splitting per step inside a function, as a finer-grained technique—could improve the semantic coherence of stored representations and enhance retrieval relevance during fuzz driver generation. This line of inquiry has the potential to optimize model input preparation and ultimately improve output quality.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/future.html#comprehensive-evaluation-and-benchmarking",
    "href": "chapters/future.html#comprehensive-evaluation-and-benchmarking",
    "title": "6  Future Work",
    "section": "6.3 Comprehensive Evaluation and Benchmarking",
    "text": "6.3 Comprehensive Evaluation and Benchmarking\nTo thoroughly establish OverHAuL’s effectiveness, extensive large-scale evaluation beyond the initial 10-project corpus is imperative. Applying the tool to repositories indexed in the clib package manager [13], which encompasses hundreds of C libraries, would test scalability and robustness across diverse real-world settings. Such a broad benchmark would also enable systematic comparisons against state-of-the-art automated fuzzing frameworks like OSS-Fuzz-Gen and AutoGen, elucidating OverHAuL’s relative strengths and identifying areas for improvement [14], [15].\nComplementing broad benchmarking, detailed ablation and matrix studies dissecting the contributions of individual pipeline components and algorithmic choices will yield critical insights into what drives OverHAuL’s performance. Understanding the impact of each module will guide targeted optimizations and support evidence-based design decisions.\nFurthermore, an economic analysis exploring resource consumption—such as API token usage and associated monetary costs—relative to fuzzing effectiveness would be valuable for assessing the practical viability of integrating LLM-based fuzz driver generation into continuous integration processes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/future.html#practical-deployment-and-community-engagement",
    "href": "chapters/future.html#practical-deployment-and-community-engagement",
    "title": "6  Future Work",
    "section": "6.4 Practical Deployment and Community Engagement",
    "text": "6.4 Practical Deployment and Community Engagement\nFrom a usability perspective, embedding OverHAuL within a GitHub Actions workflow represents a practical and impactful enhancement, enabling seamless integration with developers’ existing toolchains and continuous integration pipelines. This would promote wider adoption by reducing barriers to entry and fostering real-time feedback during code development cycles.\nAdditionally, establishing a mechanism to generate and submit automated pull requests (PRs) to the maintainers of fuzzed libraries—highlighting detected bugs and proposing patches—would not only validate OverHAuL’s findings but also contribute tangible improvements to open-source software quality. This collaborative feedback loop epitomizes the symbiosis between automated testing tools and the open-source community. As an initial step, developing targeted PRs for the projects where bugs were discovered during OverHAuL’s development would help facilitate practical follow-up and improvements.\n\n\n\n\n[1] A. Cedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf, “CMake - Upgrade Your Software Build System.” 2000. Available: https://cmake.org/\n\n\n[2] S. I. Feldman, “Make — a program for maintaining computer programs,” Software: Practice and Experience, vol. 9, no. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[3] E. Martin, “Ninja-build/ninja.” ninja-build, Jul. 14, 2025. Available: https://github.com/ninja-build/ninja\n\n\n[4] J. Pakkanen, “Mesonbuild/meson.” The Meson Build System, Jul. 14, 2025. Available: https://github.com/mesonbuild/meson\n\n\n[5] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[6] H. Green and T. Avgerinos, “GraphFuzz: Library API fuzzing with lifetime-aware dataflow graphs,” in Proceedings of the 44th International Conference on Software Engineering, Pittsburgh Pennsylvania: ACM, May 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228. Available: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[7] Google, “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[8] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[9] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[10] E. Nijkamp et al., “CodeGen: An open large language model for code with multi-turn program synthesis,” ICLR, 2023.\n\n\n[11] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “CodeGen2: Lessons for training llms on programming and natural languages,” ICLR, 2023.\n\n\n[12] OpenAI, “Introducing GPT-4.1 in the API,” Apr. 14, 2025. Available: https://openai.com/index/gpt-4-1/\n\n\n[13] Clibs Project, “Clib Packages,” 2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[14] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[15] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "7  Conclusion",
    "section": "",
    "text": "This thesis set out to address a pressing challenge in software testing for legacy and under-tested C codebases: the significant manual effort required to develop fuzzing harnesses, especially in the absence of pre-existing test infrastructure. In response, we present OverHAuL, a neurosymbolic AI system capable of autonomously generating effective fuzzing harnesses directly from source code. OverHAuL leverages the strengths of advanced large language model (LLM) agents, enabling it to overcome the traditional dependencies on manual effort, client code, or existing test harnesses that characterize previous tools.\nCentral to OverHAuL’s methodology is the integration of a triplet of ReAct LLM agents working within a feedback-oriented, iterative loop, capable of investigating the given project’s source code through a codebase oracle. This architecture allows the system to intelligently explore otherwise opaque codebases, systematically identifying candidate entry points for fuzzing and synthesizing robust harnesses. The end-to-end automation pipeline incorporates a compilation and evaluation phase, during which the generated harnesses are systematically compiled and rigorously assessed for correctness and effectiveness.\nTo rigorously assess OverHAuL’s efficacy and reliability, we designed a comprehensive evaluation using a benchmark suite of ten open-source C libraries. Our experiments demonstrate that OverHAuL successfully produced valid and usable fuzzing harnesses in 81.25% of the cases. This high success rate offers strong evidence supporting OverHAuL’s correctness and practical applicability, substantiating the central hypothesis of this thesis.\nThrough a comprehensive literature review of prominent related projects and a detailed comparative analysis between them and OverHAuL, we demonstrate that OverHAuL distinguishes itself in several critical aspects. Our system’s high degree of automation and limited dependence on external artifacts constitute significant advantages over previous methods, particularly regarding its applicability to legacy or inadequately documented C codebases. OverHAuL’s novel methodology underscores its distinctive role within the rapidly evolving landscape of automated fuzzing solutions, especially when contrasted against other state-of-the-art approaches.\nLooking ahead, this body of work invites several promising directions for future exploration. Expanding OverHAuL’s applicability to additional programming languages and improving compatibility with established build ecosystems would significantly widen its practical impact. Ongoing refinements to its AI-driven algorithms, especially in areas of program slicing and harness evaluation, have the potential to further enhance the robustness and effectiveness of the system. Lastly, conducting more comprehensive evaluations and large-scale comparisons with state-of-the-art tools would provide stronger evidence for the effectiveness of OverHAuL, further demonstrating its superiority over existing solutions.\nIn summary, this thesis advances the field of automated software testing by demonstrating the feasibility and utility of autonomously generated fuzzing harnesses for C projects. OverHAuL establishes a compelling foundation for future research, representing a substantial step towards fully automated, scalable, and intelligent fuzzing infrastructure in the face of increasingly complex software systems.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/refs.html",
    "href": "chapters/refs.html",
    "title": "Bibliography",
    "section": "",
    "text": "[1] Ada\nDevelopers, “Ada Reference Manual, 2022\nEdition,” 2022. Available: https://www.adaic.org/resources/add_content/standards/22rm/html/RM-TTL.html\n\n\n[2] M.\nZalewski, “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[3] M.\nHeuse, H. Eißfeldt, A. Fioraldi, and D. Maier,\n“AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[4] Astral, “Astral-sh/uv.” Astral,\nJul. 18, 2025. Available: https://github.com/astral-sh/uv\n\n\n[5] Astral, “Astral-sh/ruff.” Astral,\nJul. 18, 2025. Available: https://github.com/astral-sh/ruff\n\n\n[6] Google, “Google/atheris.” Google,\nApr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[7] T.\nAvgerinos et al., “The mayhem cyber reasoning\nsystem,” IEEE Security & Privacy, vol. 16, no. 2,\npp. 52–60, 2018.\n\n\n[8] F.\nBacon, Of the Proficience and Advancement\nof Learning... Edited by the Rev.\nGW Kitchin. Bell & Daldy, 1861.\n\n\n[9] D.\nBahdanau, K. Cho, and Y. Bengio, “Neural Machine\nTranslation by Jointly Learning to\nAlign and Translate,” May 19, 2016.\ndoi: 10.48550/arXiv.1409.0473.\nAvailable: http://arxiv.org/abs/1409.0473\n\n\n[10] GNU Project, “Bash - GNU\nProject - Free Software Foundation.”\nAvailable: https://www.gnu.org/software/bash/\n\n\n[11] F.\nBellard, P. Maydell, and QEMU Team, “QEMU.”\nMay 29, 2025. Available: https://www.qemu.org/\n\n\n[12] G.\nBlack, V. Mathew Vaidyan, and G. Comert, “Evaluating Large\nLanguage Models for Enhanced Fuzzing: An\nAnalysis Framework for LLM-Driven Seed\nGeneration,” IEEE Access, vol. 12, pp.\n156065–156081, 2024, doi: 10.1109/ACCESS.2024.3484947.\nAvailable: https://ieeexplore.ieee.org/abstract/document/10731701\n\n\n[13] F.\nBoth, “Why we no longer use LangChain for building\nour AI agents,” 2024. Available: https://octomind.dev/blog/why-we-no-longer-use-langchain-for-building-our-ai-agents\n\n\n[14] T.\nB. Brown et al., “Language Models are\nFew-Shot Learners,” Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165.\nAvailable: http://arxiv.org/abs/2005.14165\n\n\n[15] A.\nCedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf,\n“CMake - Upgrade Your Software Build\nSystem.” 2000. Available: https://cmake.org/\n\n\n[16] S.\nK. Cha, T. Avgerinos, A. Rebert, and D. Brumley, “Unleashing\nmayhem on binary code,” in 2012 IEEE symposium\non security and privacy, IEEE, 2012, pp. 380–394.\n\n\n[17] J.\nWei et al., “Chain-of-Thought Prompting Elicits\nReasoning in Large Language Models,” Jan. 10,\n2023. doi: 10.48550/arXiv.2201.11903.\nAvailable: http://arxiv.org/abs/2201.11903\n\n\n[18] OpenAI, “ChatGPT,”\n2025. Available: https://chatgpt.com\n\n\n[19] M.\nChen et al., “Evaluating Large Language Models\nTrained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374.\nAvailable: http://arxiv.org/abs/2107.03374\n\n\n[20] Anthropic, “Claude,” 2025.\nAvailable: https://claude.ai/new\n\n\n[21] Clibs Project, “Clibs/clib.”\nclibs, Jul. 01, 2025. Available: https://github.com/clibs/clib\n\n\n[22] Clibs Project, “Clib\nPackages,” 2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[23] Google, “Google/clusterfuzz.”\nGoogle, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[24] A.\nCortesi, M. Hils, and T. Kriechbaumer, “Mitmproxy/pdoc.”\nmitmproxy, Jul. 18, 2025. Available: https://github.com/mitmproxy/pdoc\n\n\n[25] Anysphere, “Cursor - The AI Code\nEditor,” 2025. Available: https://cursor.com/\n\n\n[26] DeepSeek-AI et al.,\n“DeepSeek-R1: Incentivizing Reasoning\nCapability in LLMs via Reinforcement\nLearning,” Jan. 22, 2025. doi: 10.48550/arXiv.2501.12948.\nAvailable: http://arxiv.org/abs/2501.12948\n\n\n[27] J.\nDevlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of Deep Bidirectional\nTransformers for Language Understanding,” May\n24, 2019. doi: 10.48550/arXiv.1810.04805.\nAvailable: http://arxiv.org/abs/1810.04805\n\n\n[28] O.\nKhattab et al., “DSPy: Compiling\nDeclarative Language Model Calls into Self-Improving\nPipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714.\nAvailable: http://arxiv.org/abs/2310.03714\n\n\n[29] M.\nDouze et al., “The Faiss library,”\nFeb. 11, 2025. doi: 10.48550/arXiv.2401.08281.\nAvailable: http://arxiv.org/abs/2401.08281\n\n\n[30] S.\nI. Feldman, “Make — a program for maintaining computer\nprograms,” Software: Practice and Experience, vol. 9,\nno. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402.\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[31] D.\nA. Wheeler, “Flawfinder Home Page.” Available:\nhttps://dwheeler.com/flawfinder/\n\n\n[32] O.\nI. Franksen, “Babbage and cryptography. Or, the\nmystery of Admiral Beaufort’s cipher,”\nMathematics and Computers in Simulation, vol. 35, no. 4, pp.\n327–367, 1993, Available: https://www.sciencedirect.com/science/article/pii/037847549390063Z\n\n\n[33] D.\nBabić et al., “FUDGE: Fuzz driver generation\nat scale,” in Proceedings of the 2019 27th ACM Joint\nMeeting on European Software Engineering Conference\nand Symposium on the Foundations of\nSoftware Engineering, Tallinn Estonia: ACM, Aug. 2019,\npp. 975–985. doi: 10.1145/3338906.3340456.\nAvailable: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[34] Open Source Security Foundation (OpenSSF),\n“Ossf/fuzz-introspector.” Open Source Security Foundation\n(OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[35] K.\nIspoglou, D. Austin, V. Mohan, and M. Payer,\n“FuzzGen: Automatic fuzzer\ngeneration,” in 29th USENIX Security Symposium\n(USENIX Security 20), 2020, pp. 2271–2287. Available:\nhttps://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[36] Y.\nDeng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,\n“Large Language Models are Edge-Case\nFuzzers: Testing Deep Learning Libraries via\nFuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014.\nAvailable: http://arxiv.org/abs/2304.02014\n\n\n[37] Google, “Google/fuzztest.” Google,\nJul. 10, 2025. Available: https://github.com/google/fuzztest\n\n\n[38] D.\nGanguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of\nThought : Neurosymbolic Program Synthesis\nallows Robust and Interpretable\nReasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270.\nAvailable: http://arxiv.org/abs/2409.17270\n\n\n[39] A.\nd’Avila Garcez and L. C. Lamb, “Neurosymbolic AI:\nThe 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876.\nAvailable: http://arxiv.org/abs/2012.05876\n\n\n[40] M.\nGaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI\nSystems: Consistency, Reliability,\nExplainability, and Safety,” Dec. 05,\n2023. doi: 10.48550/arXiv.2312.06798.\nAvailable: http://arxiv.org/abs/2312.06798\n\n\n[41] Google, “‎Google\nGemini,” 2025. Available: https://gemini.google.com\n\n\n[42] Microsoft, “GitHub Copilot ·\nYour AI pair programmer,” 2025. Available: https://github.com/features/copilot\n\n\n[43] D.\nGiannone, “Demystifying AI Agents: ReAct-Style\nAgents vs Agentic Workflows,” Feb. 09, 2025.\nAvailable: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[44] GitHub Docs, “Choosing the runner for a\njob,” 2025. Available: https://docs-internal.github.com/en/actions/how-tos/writing-workflows/choosing-where-your-workflow-runs/choosing-the-runner-for-a-job\n\n\n[45] GitHub Docs, “About GitHub-hosted runners,” 2025. Available: https://docs-internal.github.com/en/actions/concepts/runners/about-github-hosted-runners\n\n\n[46] A.\nGrattafiori et al., “The Llama 3\nHerd of Models,” Nov. 23, 2024. doi: 10.48550/arXiv.2407.21783.\nAvailable: http://arxiv.org/abs/2407.21783\n\n\n[47] H.\nGreen and T. Avgerinos, “GraphFuzz: Library\nAPI fuzzing with lifetime-aware dataflow graphs,” in\nProceedings of the 44th International Conference on\nSoftware Engineering, Pittsburgh Pennsylvania: ACM,\nMay 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228.\nAvailable: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[48] T.\nHe, “Sighingnow/libclang.” Jul. 03, 2025. Available: https://github.com/sighingnow/libclang\n\n\n[49] Blackduck, Inc., “Heartbleed\nBug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[50] CVE Program, “CVE -\nCVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[51] G.\nJ. Holzmann, “The Power of 10: Rules for\nDeveloping Safety-Critical Code,” Jun. 2006,\nAvailable: https://web.eecs.umich.edu/~imarkov/10rules.pdf\n\n\n[52] Google, “Google/honggfuzz.”\nGoogle, Jul. 10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[53] L.\nHuang et al., “A Survey on\nHallucination in Large Language Models:\nPrinciples, Taxonomy, Challenges,\nand Open Questions,” ACM Trans. Inf. Syst.,\nvol. 43, no. 2, pp. 1–55, Mar. 2025, doi: 10.1145/3703155. Available:\nhttp://arxiv.org/abs/2311.05232\n\n\n[54] Z.\nLi, S. Dutta, and M. Naik, “IRIS: LLM-Assisted\nStatic Analysis for Detecting Security\nVulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238.\nAvailable: http://arxiv.org/abs/2405.17238\n\n\n[55] Y.\nJiang et al., “When Fuzzing Meets LLMs:\nChallenges and Opportunities,” in\nCompanion Proceedings of the 32nd ACM\nInternational Conference on the Foundations of\nSoftware Engineering, in ACM Conferences.\n2024, pp. 492–496. doi: 10.1145/3663529.3663784.\nAvailable: https://dl.acm.org/doi/abs/10.1145/3663529.3663784\n\n\n[56] J.\nKaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy,\n“Challenges and Applications of Large Language\nModels,” Jul. 19, 2023. doi: 10.48550/arXiv.2307.10169.\nAvailable: http://arxiv.org/abs/2307.10169\n\n\n[57] D.\nKahneman, Thinking, fast and slow, 1st ed. New York:\nFarrar, Straus and Giroux, 2011.\n\n\n[58] H.\nKautz, “The Third AI Summer,” presented at the\n34th Annual Meeting of the Association for the\nAdvancement of Artificial Intelligence, Feb.\n10, 2020. Available: https://www.youtube.com/watch?v=_cQITY0SPiw\n\n\n[59] B.\nW. Kernighan and D. M. Ritchie, The C programming\nlanguage. in Prentice-Hall software series. Englewood\nCliffs, N.J: Prentice-Hall, 1978.\n\n\n[60] S.\nKim and S. Lee, “Performance Comparison of\nPrompt Engineering and Fine-Tuning Approaches\nfor Fuzz Driver Generation Using Large Language\nModels,” in Innovative Mobile and\nInternet Services in Ubiquitous\nComputing, L. Barolli, H.-C. Chen, and K. Yim, Eds., Cham:\nSpringer Nature Switzerland, 2025, pp. 111–120. doi: 10.1007/978-3-031-96093-2_12\n\n\n[61] C.\nCadar, D. Dunbar, and D. Engler, “KLEE:\nUnassisted and Automatic Generation of\nHigh-Coverage Tests for Complex Systems\nPrograms,” presented at the USENIX Symposium\non Operating Systems Design and\nImplementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[62] N.\nKosmyna et al., “Your Brain on\nChatGPT: Accumulation of Cognitive\nDebt when Using an AI Assistant for\nEssay Writing Task,” Jun. 10, 2025. doi: 10.48550/arXiv.2506.08872.\nAvailable: http://arxiv.org/abs/2506.08872\n\n\n[63] H.\nChase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[64] H.-P. H. Lee et al., “The\nImpact of Generative AI on Critical\nThinking: Self-Reported Reductions in\nCognitive Effort and Confidence Effects From a\nSurvey of Knowledge Workers,” 2025,\nAvailable: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[65] P.\nLewis et al., “Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP\nTasks,” Apr. 12, 2021. doi: 10.48550/arXiv.2005.11401.\nAvailable: http://arxiv.org/abs/2005.11401\n\n\n[66] H.\nLi, “Language models: Past, present, and future,”\nCommun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available:\nhttps://dl.acm.org/doi/10.1145/3490443\n\n\n[67] LLVM Project, “libFuzzer – a library for coverage-guided fuzz\ntesting. — LLVM 21.0.0git documentation,” 2025.\nAvailable: https://llvm.org/docs/LibFuzzer.html\n\n\n[68] D.\nLiu, J. Metzman, O. Chang, and G. O. S. S. Team, “AI-Powered\nFuzzing: Breaking the Bug Hunting\nBarrier,” Aug. 16, 2023. Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[69] J.\nLiu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234.\nAvailable: https://github.com/jerryjliu/llama_index\n\n\n[70] LLVM Project, “The LLVM Compiler\nInfrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[71] V.\nJ. M. Manes et al., “The Art,\nScience, and Engineering of\nFuzzing: A Survey,” Apr. 07, 2019. doi:\n10.48550/arXiv.1812.00140.\nAvailable: http://arxiv.org/abs/1812.00140\n\n\n[72] E.\nMartin, “Ninja-build/ninja.” ninja-build, Jul. 14, 2025.\nAvailable: https://github.com/ninja-build/ninja\n\n\n[73] A.\nMastropaolo and D. Poshyvanyk, “A Path Less Traveled:\nReimagining Software Engineering Automation via a\nNeurosymbolic Paradigm,” May 04, 2025. doi: 10.48550/arXiv.2505.02275.\nAvailable: http://arxiv.org/abs/2505.02275\n\n\n[74] T.\nMikolov, K. Chen, G. Corrado, and J. Dean, “Efficient\nEstimation of Word Representations in\nVector Space,” Sep. 06, 2013. doi: 10.48550/arXiv.1301.3781.\nAvailable: http://arxiv.org/abs/1301.3781\n\n\n[75] B.\nP. Miller, L. Fredriksen, and B. So, “An empirical study of the\nreliability of UNIX utilities,” Commun.\nACM, vol. 33, no. 12, pp. 32–44, Dec. 1990, doi: 10.1145/96267.96279.\nAvailable: https://dl.acm.org/doi/10.1145/96267.96279\n\n\n[76] E.\nNijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\n“CodeGen2: Lessons for training llms on\nprogramming and natural languages,” ICLR, 2023.\n\n\n[77] E.\nNijkamp et al., “CodeGen: An\nopen large language model for code with multi-turn program\nsynthesis,” ICLR, 2023.\n\n\n[78] OpenAI et al., “GPT-4\nTechnical Report,” Mar. 04, 2024. doi: 10.48550/arXiv.2303.08774.\nAvailable: http://arxiv.org/abs/2303.08774\n\n\n[79] OpenAI, “Introducing GPT-4.1\nin the API,” Apr. 14, 2025. Available: https://openai.com/index/gpt-4-1/\n\n\n[80] OpenAI Docs, “GPT-4.1 mini -\nOpen AI API,” 2025. Available: https://platform.openai.com\n\n\n[81] OpenAI Docs, “Text-embedding-3-small -\nOpenAI API,” 2025. Available: https://platform.openai.com\n\n\n[82] OpenAI Docs, “Model optimization -\nOpenAI API,” 2025. Available: https://platform.openai.com\n\n\n[83] A.\nArya, O. Chang, J. Metzman, K. Serebryany, and D. Liu,\n“OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[84] D.\nLiu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target\ngeneration.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[85] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed\nprojects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[86] OSS-Fuzz, “OSS-Fuzz\nDocumentation,” 2025. Available: https://google.github.io/oss-fuzz/\n\n\n[87] OWASP Foundation, “Fuzzing.”\nAvailable: https://owasp.org/www-community/Fuzzing\n\n\n[88] J.\nPakkanen, “Mesonbuild/meson.” The Meson Build System, Jul.\n14, 2025. Available: https://github.com/mesonbuild/meson\n\n\n[89] N.\nPerry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users\nWrite More Insecure Code with AI Assistants?”\nDec. 18, 2023. doi: 10.48550/arXiv.2211.03622.\nAvailable: http://arxiv.org/abs/2211.03622\n\n\n[90] pip developers, “Pip documentation\nV25.1.1,” 2025. Available: https://pip.pypa.io/en/stable/\n\n\n[91] D.\nWang, G. Zhou, L. Chen, D. Li, and Y. Miao,\n“ProphetFuzz: Fully Automated Prediction\nand Fuzzing of High-Risk Option Combinations\nwith Only Documentation via Large Language\nModel,” Sep. 01, 2024. doi: 10.1145/3658644.3690231.\nAvailable: http://arxiv.org/abs/2409.00922\n\n\n[92] PyTest Dev Team,\n“Pytest-dev/pytest.” pytest-dev, Jul. 18, 2025. Available:\nhttps://github.com/pytest-dev/pytest\n\n\n[93] Python Software Foundation,\n“Python/mypy.” Python, Jul. 18, 2025. Available: https://github.com/python/mypy\n\n\n[94] A.\nRadford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” 2018,\nAvailable: https://www.mikecaptain.com/resources/pdf/GPT-1.pdf\n\n\n[95] A.\nRadford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,”\nOpenAI blog, vol. 1, no. 8, p. 9, 2019, Available: https://storage.prod.researchhub.com/uploads/papers/2020/06/01/language-models.pdf\n\n\n[96] N.\nRathaus and G. Evron, Open source fuzzing tools. Burlington,\nMA: Syngress Pub, 2007.\n\n\n[97] S.\nYao et al., “ReAct: Synergizing\nReasoning and Acting in Language\nModels,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629.\nAvailable: http://arxiv.org/abs/2210.03629\n\n\n[98] A.\nRebert et al., “Optimizing seed selection for\nfuzzing,” in Proceedings of the 23rd USENIX\nconference on Security Symposium, in\nSEC’14. USA: USENIX Association, Aug. 2014, pp.\n861–875.\n\n\n[99] D.\nM. Ritchie, S. C. Johnson, M. E. Lesk, and B. W. Kernighan, “The\nC programming language,” Bell Sys. Tech. J,\nvol. 57, no. 6, pp. 1991–2019, 1978, Available: https://www.academia.edu/download/67840358/1978.07_Bell_System_Technical_Journal.pdf#page=85\n\n\n[100] Rust Project Developers, “Rust\nProgramming Language,” 2025. Available: https://www.rust-lang.org/\n\n\n[101] J. Saarinen, “Further flaws render\nShellshock patch ineffective,” Sep. 29, 2014.\nAvailable: https://www.itnews.com.au/news/further-flaws-render-shellshock-patch-ineffective-396256\n\n\n[102] A. Sarkar and I. Drosos, “Vibe coding:\nProgramming through conversation with artificial intelligence,”\nJun. 29, 2025. doi: 10.48550/arXiv.2506.23253.\nAvailable: http://arxiv.org/abs/2506.23253\n\n\n[103] M. K. Sarker, L. Zhou, A. Eberhart, and P.\nHitzler, “Neuro-symbolic artificial intelligence:\nCurrent trends,” AIC, vol. 34, no. 3, pp.\n197–209, Mar. 2022, doi: 10.3233/aic-210084.\nAvailable: https://journals.sagepub.com/doi/full/10.3233/AIC-210084\n\n\n[104] N. Sasirekha, A. Edwin Robert, and M.\nHemalatha, “Program Slicing Techniques and its\nApplications,” IJSEA, vol. 2, no. 3, pp.\n50–64, Jul. 2011, doi: 10.5121/ijsea.2011.2304.\nAvailable: http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf\n\n\n[105] T. Preston-Werner, “Semantic\nVersioning 2.0.0.” Available: https://semver.org/\n\n\n[106] K. Serebryany, D. Bruening, A. Potapenko, and\nD. Vyukov, “AddressSanitizer: A fast\naddress sanity checker,” in 2012 USENIX annual\ntechnical conference (USENIX ATC 12), 2012, pp.\n309–318. Available: https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany\n\n\n[107] A. Sheth, K. Roy, and M. Gaur,\n“Neurosymbolic AI – Why,\nWhat, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813.\nAvailable: http://arxiv.org/abs/2305.00813\n\n\n[108] W. Shi, Y. Zhang, X. Xing, and J. Xu,\n“Harnessing Large Language Models for Seed\nGeneration in Greybox Fuzzing,” Nov. 27,\n2024. doi: 10.48550/arXiv.2411.18143.\nAvailable: http://arxiv.org/abs/2411.18143\n\n\n[109] T. Simonite, “This Bot Hunts\nSoftware Bugs for the Pentagon,”\nWired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[110] Stanford NLP Team, “Signatures -\nDSPy Documentation,” 2025. Available: https://dspy.ai/learn/programming/signatures/\n\n\n[111] Stanford NLP Team, “ReAct -\nDSPy Documentation,” 2025. Available: https://dspy.ai/api/modules/ReAct/\n\n\n[112] Y. Sun, “Automated\nGeneration and Compilation of Fuzz\nDriver Based on Large Language Models,” in\nProceedings of the 2024 9th International Conference on\nCyber Security and Information\nEngineering, in ICCSIE ’24. New York, NY, USA:\nAssociation for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272.\nAvailable: https://doi.org/10.1145/3689236.3689272\n\n\n[113] M. Sutton, A. Greene, and P. Amini,\nFuzzing: Brute force vulnerabilty discovery. Upper Saddle\nRiver, NJ: Addison-Wesley, 2007.\n\n\n[114] A. Takanen, J. DeMott, C. Miller, and A.\nKettunen, Fuzzing for software security testing and quality\nassurance, Second edition. in Information security and privacy\nlibrary. Boston London Norwood, MA: Artech House, 2018.\n\n\n[115] The OpenSSL Project,\n“Openssl/openssl.” OpenSSL, Jul. 15, 2025. Available: https://github.com/openssl/openssl\n\n\n[116] L. Thomason,\n“Leethomason/Tinyxml2.” Jul. 10, 2025. Available: https://github.com/leethomason/tinyxml2\n\n\n[117] D. Tilwani, R. Venkataramanan, and A. P. Sheth,\n“Neurosymbolic AI approach to\nAttribution in Large Language Models,”\nSep. 30, 2024. doi: 10.48550/arXiv.2410.03726.\nAvailable: http://arxiv.org/abs/2410.03726\n\n\n[118] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L.\nZhang, “Large Language Models Are Zero-Shot Fuzzers:\nFuzzing Deep-Learning Libraries via Large Language\nModels,” in Proceedings of the 32nd ACM SIGSOFT\nInternational Symposium on Software Testing and\nAnalysis, in ISSTA 2023. New York, NY,\nUSA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi:\n10.1145/3597926.3598067.\nAvailable: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[119] L. Torvalds, “Git.” Apr. 07, 2005.\nAvailable: https://git-scm.com/\n\n\n[120] Unicorn Engine,\n“Unicorn-engine/unicorn.” Unicorn Engine, Jul. 15, 2025.\nAvailable: https://github.com/unicorn-engine/unicorn\n\n\n[121] B. Jeong et al.,\n“UTopia: Automatic Generation of\nFuzz Driver using Unit Tests,” in\n2023 IEEE Symposium on Security and\nPrivacy (SP), May 2023, pp. 2676–2692.\ndoi: 10.1109/SP46215.2023.10179394.\nAvailable: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[122] A. Vaswani et al., “Attention\nIs All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762.\nAvailable: http://arxiv.org/abs/1706.03762\n\n\n[123] A. Velasco, A. Garryyeva, D. N. Palacio, A.\nMastropaolo, and D. Poshyvanyk, “Toward Neurosymbolic\nProgram Comprehension,” Feb. 03, 2025. doi: 10.48550/arXiv.2502.01806.\nAvailable: http://arxiv.org/abs/2502.01806\n\n\n[124] Python Software Foundation, “Venv —\nCreation of virtual environments,” Jul. 17, 2025.\nAvailable: https://docs.python.org/3/library/venv.html\n\n\n[125] Z. Wang, Z. Chu, T. V. Doan, S. Ni, M. Yang,\nand W. Zhang, “History, development, and principles of large\nlanguage models: An introductory survey,” AI Ethics,\nvol. 5, no. 3, pp. 1955–1971, Jun. 2025, doi: 10.1007/s43681-024-00583-7.\nAvailable: https://doi.org/10.1007/s43681-024-00583-7\n\n\n[126] D. Wheeler, “How to Prevent\nthe next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[127] M. Woolf, “The Problem With\nLangChain,” Jul. 14, 2023. Available: https://minimaxir.com/2023/07/langchain-problem/\n\n\n[128] Woyera, “6 Reasons why\nLangchain Sucks,” Sep. 08, 2023. Available: https://medium.com/@woyera/6-reasons-why-langchain-sucks-b6c99c98efbe\n\n\n[129] H. Xu et al.,\n“CKGFuzzer: LLM-Based Fuzz Driver Generation\nEnhanced By Code Knowledge Graph,” Dec. 20, 2024. doi: 10.48550/arXiv.2411.11532.\nAvailable: http://arxiv.org/abs/2411.11532\n\n\n[130] S. Yao et al., “Tree of\nThoughts: Deliberate Problem Solving with\nLarge Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601.\nAvailable: http://arxiv.org/abs/2305.10601\n\n\n[131] M. Zhang, J. Liu, F. Ma, H. Zhang, and Y.\nJiang, “IntelliGen: Automatic Driver\nSynthesis for FuzzTesting,” Mar. 01, 2021.\ndoi: 10.48550/arXiv.2103.00862.\nAvailable: http://arxiv.org/abs/2103.00862\n\n\n[132] S. Zhao, Y. Yang, Z. Wang, Z. He, L. K. Qiu,\nand L. Qiu, “Retrieval Augmented Generation\n(RAG) and Beyond: A Comprehensive\nSurvey on How to Make your\nLLMs use External Data More Wisely,”\nSep. 23, 2024. doi: 10.48550/arXiv.2409.14924.\nAvailable: http://arxiv.org/abs/2409.14924",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "chapters/abandoned.html",
    "href": "chapters/abandoned.html",
    "title": "Appendix A — Abandoned Techniques",
    "section": "",
    "text": "During its development, OverHAuL went through several iterations. A number of approaches were implemented and evaluated, with some being replaced for better alternatives. These are:\n\nOne-shot harness generation\nBefore the iterative feedback loop (Section 3.3.1) was implemented, OverHAuL attempted to operate in a straightforward pipeline, with just a “generator” agent being tasked to generate the harness. This meant that at either the compilation step or evaluation step, any failure resulted in the execution being terminated. This approach put too much responsibility in the response of a single LLM query, with results more often than not being unsatisfactory.\nChain-of-Thought LLM instances\nThe current implementation of ReAct agents has effectively supplanted the less effective Chain of Thought (COT) LLM modules [1]. This shift underscores a critical realization in the harness generation process: the primary challenge lies not in the creation of the harness itself, but rather in the necessity for real-time feedback during execution. This is the reason why first employing COT prompting offered limited observed improvements.\nCOT techniques are particularly advantageous when the task assigned to the LLM demands a more reflective, in-depth analysis. However, when it comes to tasks such as knowledge extraction from a codebase oracle and taking live feedback from the environment into consideration, ReAct agents demonstrate greater efficiency and effectiveness.\nSource code concatenation\nInitially, there was no implementation of a codebase oracle. Instead, the LLM agents operated with a Python string that contained a concatenation of all the collected source code. While this method proved effective for smaller and simpler projects, it encountered significant limitations when applied to more complex codebases. The primary challenge was the excessive consumption of the LLM’s context window, which hindered its ability to process and analyze larger codebases effectively. As a result, this approach became increasingly unsustainable as project complexity grew, underscoring the need for a more robust solution.\n{index, read}_tool usage for ReAct agents\nThe predecessor of the oracle comprised a dual-system approach for code exploration, integrating the index_tool and the read_tool. The index_tool offered the LLM agent a structured JSON object that delineated the project’s architecture, including all relevant file paths. On the other hand, the read_tool required a file path as input and returned the file’s content, albeit truncated to a maximum of 4000 characters. While this methodology presented an improvement in scalability over earlier systems, several limitations persisted.\nFirstly, the LLM was constrained to searching through the codebase strictly in file-specific terms, which limited its efficacy in understanding the broader context of code relationships. Furthermore, the imposed character limit on the read_tool meant that certain portions of the codebase remained inaccessible, impeding the agent’s analytical capabilities. Even if this character limit were to be lifted, the resultant output would still occupy a significant portion of the context window, particularly in larger and more intricate projects. As such, while this approach offered advancements in code exploration, it still fell short.\n\n\n\n\n\n[1] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Abandoned Techniques</span>"
    ]
  },
  {
    "objectID": "chapters/sample-harnesses.html",
    "href": "chapters/sample-harnesses.html",
    "title": "Appendix B — Sample Generated Harnesses",
    "section": "",
    "text": "B.1 clibs/buffer\nThis appendix presents a selection of harnesses generated by OverHAuL, originating from the benchmark run 16351915455. The examples included herein demonstrate a notable diversity, ranging from concise and straightforward harnesses to those that are more elaborate and complex. The crash inputs are provided in the form of their corresponding xxd hexadecimal representations.\nGenerated harness:\nBody of function that the harness targets:\nCrash input which triggers an exception in buffer_appendf()&gt;vsnprintf():",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sample Generated Harnesses</span>"
    ]
  },
  {
    "objectID": "chapters/sample-harnesses.html#clibsbuffer",
    "href": "chapters/sample-harnesses.html#clibsbuffer",
    "title": "Appendix B — Sample Generated Harnesses",
    "section": "",
    "text": "#include \"buffer.h\"\n#include &lt;stdint.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\n/*\n * LLVMFuzzerTestOneInput harness to fuzz the function buffer_appendf.\n *\n * The fuzzing input data is treated as a format string argument to\n * buffer_appendf. This targets potential format string vulnerabilities and\n * buffer resizing issues within buffer_appendf as flagged by static analysis.\n *\n * We create a fresh buffer_t instance with buffer_new(), then call\n * buffer_appendf() with the fuzz input as the format string. We do not supply\n * additional variadic arguments as the input is a single buffer.\n *\n * After the call, we free the buffer_t instance to avoid memory leaks.\n */\nint\nLLVMFuzzerTestOneInput (const uint8_t *data, size_t size)\n{\n  if (size == 0)\n    {\n      // No data to test; skip\n      return 0;\n    }\n\n  // Allocate a writable null-terminated string for the format input\n  // We add one byte for the null terminator\n  char *fmt = (char *)malloc (size + 1);\n  if (!fmt)\n    return 0; // Allocation failed, skip\n\n  // Copy input and null terminate\n  memcpy (fmt, data, size);\n  fmt[size] = '\\0';\n\n  // Create a new buffer instance\n  buffer_t *buf = buffer_new ();\n  if (!buf)\n    {\n      free (fmt);\n      return 0;\n    }\n\n  // Call buffer_appendf with user-controlled format string\n  // No additional arguments are supplied\n  (void)buffer_appendf (buf, fmt);\n\n  // Cleanup\n  buffer_free (buf);\n  free (fmt);\n\n  return 0;\n}\n\nint buffer_appendf(buffer_t *self, const char *format, ...) {\n  va_list ap;\n  va_list tmpa;\n  char *dst = NULL;\n  int length = 0;\n  int required = 0;\n  int bytes = 0;\n\n  va_start(ap, format);\n\n  length = buffer_length(self);\n\n  // First, we compute how many bytes are needed\n  // for the formatted string and allocate that\n  // much more space in the buffer.\n  va_copy(tmpa, ap);\n  required = vsnprintf(NULL, 0, format, tmpa);\n  va_end(tmpa);\n  if (-1 == buffer_resize(self, length + required)) {\n    va_end(ap);\n    return -1;\n  }\n\n  // Next format the string into the space that we\n  // have made room for.\n  dst = self-&gt;data + length;\n  bytes = vsnprintf(dst, 1 + required, format, ap);\n  va_end(ap);\n\n  return bytes &lt; 0\n    ? -1\n    : 0;\n}\n\n00000000: 1b2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  .---------------\n00000010: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d  ----------------\n00000020: 2d2d 2d2d 2d2d 2d2d 2d2d 2d25 2d2d 2d2d  -----------%----\n00000030: 2d2d 2d2d 2d2d 2d2d 2d2d 2d2d 2d         -------------",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sample Generated Harnesses</span>"
    ]
  },
  {
    "objectID": "chapters/sample-harnesses.html#willemtcbuffer",
    "href": "chapters/sample-harnesses.html#willemtcbuffer",
    "title": "Appendix B — Sample Generated Harnesses",
    "section": "B.2 willemt/cbuffer",
    "text": "B.2 willemt/cbuffer\nGenerated harness:\n#include \"cbuffer.h\"\n#include &lt;stddef.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\n// New improved fuzzer harness for cbuf_offer with multiple writes and boundary\n// tests\nint\nLLVMFuzzerTestOneInput (const uint8_t *data, size_t size)\n{\n  if (size == 0)\n    {\n      // allow zero-size input to test zero-length write\n      cbuf_t *cbuf = cbuf_new (12);\n      if (cbuf == NULL)\n        return 0;\n      // calling offer with zero length should be tested\n      (void)cbuf_offer (cbuf, NULL, 0);\n      cbuf_free (cbuf);\n      return 0;\n    }\n\n  cbuf_t *cbuf = cbuf_new (12);\n  if (cbuf == NULL)\n    return 0;\n\n  // Split input roughly into two parts; first fills the buffer partially,\n  // second part is used for fuzzing cbuf_offer calls\n  size_t split = size / 2;\n  const uint8_t *prefix_data = data;\n  size_t prefix_size = split;\n  const uint8_t *main_data = data + split;\n  size_t main_size = size - split;\n\n  // Initially fill the buffer partially with prefix_data to simulate used\n  // space\n  if (prefix_size &gt; 0)\n    {\n      int space = cbuf_unusedspace (cbuf);\n      int to_write\n          = prefix_size &lt; (size_t)space ? (int)prefix_size : space - 1;\n      if (to_write &gt; 0)\n        {\n          (void)cbuf_offer (cbuf, prefix_data, to_write);\n        }\n    }\n\n  // Now fuzz cbuf_offer with main_data\n  // Derive write size from first byte of main_data if available, else zero.\n  int write_size = 0;\n  if (main_size &gt; 0)\n    {\n      write_size = main_data[0];\n      // Allow write size to be zero (edge case) and up to larger than buffer\n      // size to test rejection path Normalize write_size to a range: 0 to 2 *\n      // cbuf-&gt;size to test boundary and overflow cases clearly\n      int max_test_size = (int)(cbuf-&gt;size * 2);\n      write_size\n          = (write_size\n             % (max_test_size + 1)); // allows 0 to max_test_size inclusive\n    }\n\n  // Pointer to data for writing is after first byte in main_data if exists\n  const uint8_t *write_data = main_data + 1;\n  size_t write_data_len = (main_size &gt; 0) ? main_size - 1 : 0;\n\n  // Clamp write_size to write_data_len but allow write_size &gt; write_data_len\n  // to simulate out of bounds sizes by assigning write_data_len as is -- note\n  // cbuf_offer will only read up to write_size bytes anyway but fuzzing with\n  // invalid sizes tests boundary conditions.\n\n  if ((size_t)write_size &gt; write_data_len)\n    {\n      // We keep write_size as is for boundary fuzzing (even if it overflows\n      // input), but avoid buffer overflow by passing smaller data pointer as\n      // NULL if no data available\n      if (write_data_len == 0)\n        {\n          write_data = NULL;\n        }\n    }\n\n  // Call cbuf_offer with fuzz parameters (including zero and oversized writes)\n  (void)cbuf_offer (cbuf, write_data, write_size);\n\n  // Additionally try a zero length call explicitly to test that edge case\n  (void)cbuf_offer (cbuf, NULL, 0);\n\n  // Optionally perform a few repeated calls with no data to simulate further\n  // state changes\n  (void)cbuf_offer (cbuf, NULL, 0);\n  (void)cbuf_offer (cbuf, NULL, 0);\n\n  cbuf_free (cbuf);\n  return 0;\n}\nBody of function that the harness targets:\nint cbuf_offer(cbuf_t *me, const unsigned char *data, const int size)\n{\n    /* prevent buffer from getting completely full or over commited */\n    if (cbuf_unusedspace(me) &lt;= size)\n        return 0;\n\n    int written = cbuf_unusedspace(me);\n    written = size &lt; written ? size : written;\n    memcpy(me-&gt;data + me-&gt;tail, data, written);\n    me-&gt;tail += written;\n    if (me-&gt;size &lt; me-&gt;tail)\n        me-&gt;tail %= me-&gt;size;\n    return written;\n}\nCrash input which triggers an exception in cbuf_offer()&gt;memcpy():\n00000000: 0a                                       .",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sample Generated Harnesses</span>"
    ]
  },
  {
    "objectID": "chapters/sample-harnesses.html#sec-dateparse",
    "href": "chapters/sample-harnesses.html#sec-dateparse",
    "title": "Appendix B — Sample Generated Harnesses",
    "section": "B.3 dvhar/dateparse",
    "text": "B.3 dvhar/dateparse\nGenerated harness:\n#include &lt;stddef.h&gt;\n#include &lt;stdint.h&gt;\n#include \"dateparse.h\"\n\n// LibFuzzer entry point\nint LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n    if (size == 0) return 0;\n\n    date_t parsed_date = 0;\n    int offset = 0;\n\n    // Pass the input data directly without copying\n    (void)dateparse((const char *)data, &parsed_date, &offset, (int)size);\n\n    return 0;\n}\nBody of function that the harness targets:\nint dateparse(const char* datestr, date_t* t, int *offset, int stringlen){\n    struct parser p;\n    *t = 0;\n    if (!stringlen)\n        stringlen = strlen(datestr);\n    if (parseTime(datestr, &p, stringlen))\n        return -1;\n    return parse(&p, t, offset);\n}\nCrash input that causes a heap buffer overflow in dateparse()&gt;parseTime()&gt;setMonth()&gt;strncpy().\n00000000: 6632 2066                                f2 f",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sample Generated Harnesses</span>"
    ]
  },
  {
    "objectID": "chapters/sample-harnesses.html#h2nonsemver.c",
    "href": "chapters/sample-harnesses.html#h2nonsemver.c",
    "title": "Appendix B — Sample Generated Harnesses",
    "section": "B.4 h2non/semver.c",
    "text": "B.4 h2non/semver.c\nGenerated harness:\n#include \"semver.h\"\n#include &lt;stdint.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;string.h&gt;\n\n// Enhanced fuzzer harness for semver_parse including calls to semver_numeric\n// and semver_render to help increase coverage and trigger more bugs faster.\n// Dynamically allocates input buffer with no size limit other than system\n// memory. Frees allocated semver_t string members after use.\nint\nLLVMFuzzerTestOneInput (const uint8_t *data, size_t size)\n{\n  // Allocate buffer dynamically to handle any input size.\n  char *input_str = (char *)malloc (size + 1);\n  if (input_str == NULL)\n    {\n      return 0;\n    }\n  memcpy (input_str, data, size);\n  input_str[size] = '\\0';\n\n  semver_t ver;\n  memset (&ver, 0, sizeof (ver));\n\n  // Parse the semver string.\n  (void)semver_parse (input_str, &ver);\n\n  // Call semver_numeric to exercise more code paths.\n  (void)semver_numeric (&ver);\n\n  // Call semver_render with buffer large enough for rendered output.\n  // Typical semantic version strings are short, but use a buffer of size\n  // size+20 to be safe.\n  char *render_buf = (char *)malloc (size + 20);\n  if (render_buf != NULL)\n    {\n      memset (render_buf, 0, size + 20);\n      semver_render (&ver, render_buf);\n      free (render_buf);\n    }\n\n  // Free dynamically allocated members inside semver_t.\n  if (ver.prerelease)\n    {\n      free (ver.prerelease);\n      ver.prerelease = NULL;\n    }\n  if (ver.metadata)\n    {\n      free (ver.metadata);\n      ver.metadata = NULL;\n    }\n\n  free (input_str);\n  return 0;\n}\nBodies of functions that the harness targets:\n/**\n * Parses a string as semver expression.\n *\n * Returns:\n *\n * `0` - Parsed successfully\n * `-1` - In case of error\n */\n\nint\nsemver_parse (const char *str, semver_t *ver)\n{\n  int valid, res;\n  size_t len;\n  char *buf;\n  valid = semver_is_valid (str);\n  if (!valid)\n    return -1;\n\n  len = strlen (str);\n  buf = (char *)calloc (len + 1, sizeof (*buf));\n  if (buf == NULL)\n    return -1;\n  strcpy (buf, str);\n\n  ver-&gt;metadata = parse_slice (buf, MT_DELIMITER[0]);\n  ver-&gt;prerelease = parse_slice (buf, PR_DELIMITER[0]);\n\n  res = semver_parse_version (buf, ver);\n  free (buf);\n#if DEBUG &gt; 0\n  printf (\"[debug] semver.c %s = %d.%d.%d, %s %s\\n\", str, ver-&gt;major,\n          ver-&gt;minor, ver-&gt;patch, ver-&gt;prerelease, ver-&gt;metadata);\n#endif\n  return res;\n}\n\n//...\n\n/**\n * Render a given semver as string\n */\n\nvoid\nsemver_render (semver_t *x, char *dest)\n{\n  concat_num (dest, x-&gt;major, NULL);\n  concat_num (dest, x-&gt;minor, DELIMITER);\n  concat_num (dest, x-&gt;patch, DELIMITER);\n  if (x-&gt;prerelease)\n    concat_char (dest, x-&gt;prerelease, PR_DELIMITER);\n  if (x-&gt;metadata)\n    concat_char (dest, x-&gt;metadata, MT_DELIMITER);\n}\nCrash input that causes a stack buffer overflow in semver_render()&gt;concat_char()&gt;sprintf():\n00000000: 392d 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b  9-++++++++++++++\n00000010: 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b  ++++++++++++++++\n00000020: 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b  ++++++++++++++++\n00000030: 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b 2b2b  ++++++++++++++++\n00000040: 2b2b 2b2b 2b2b 2b46 4c                   +++++++FL",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Sample Generated Harnesses</span>"
    ]
  },
  {
    "objectID": "chapters/prompts.html",
    "href": "chapters/prompts.html",
    "title": "Appendix C — DSPy Custom Signatures",
    "section": "",
    "text": "class GenerateHarness(dspy.Signature):\n    \"\"\"\n    You are an experienced C/C++ security testing engineer. You must write a\n    libFuzzer-compatible `int LLVMFuzzerTestOneInput(const uint8_t *data, size_t\n    size)` harness for a function of the given C project. Your goal is for the\n    harness to be ready for compilation and for it to find successfully a bug in\n    the function-under-test. Write verbose (within reason) and helpful comments\n    on each step/decision you take/make, especially if you use \"weird\" constants\n    or values that have something to do with the project.\n\n    You have access to a rag_tool, which contains a vector store of\n    function-level chunks of the project. Use it to write better harnesses. Keep\n    in mind that it can only reply with function chunks, do not ask it to\n    combine stuff.\n\n    The rag_tool does not store any information on which lines the functions\n    are. So do not ask questions based on lines.\n\n    Make sure that you only fuzz an existing function. You will know that a\n    functions exists when the rag_tool returns to you its signature and body.\n    \"\"\"\n\n    static: str = dspy.InputField(\n        desc=\"\"\" Output of static analysis tools for the project. If you find it\n        helpful, write your harness so that it leverages some of the potential\n        vulnerabilities described below.  \"\"\"\n    )\n    new_harness: str = dspy.OutputField(\n        desc=\"\"\" C code for a libFuzzer-compatible harness. Output only the C\n        code, **DO NOT format it in a markdown code cell with backticks**, so\n        that it will be ready for compilation.\n\n        &lt;important&gt;\n        \n        Add **all** the necessary includes, either project-specific or standard\n        libraries like &lt;string.h&gt;, &lt;stdint.h&gt; and &lt;stdlib.h&gt;. Also include any\n        header files that are part of the project and are probably useful. Most\n        projects have a header file with the same name as the project at the\n        root.\n\n        **The function to be fuzzed absolutely must be part of the source\n        code**, do not write a harness for your own functions or speculate about\n        existing ones. You must be sure that the function that is fuzzed exists\n        in the source code. Use your rag tool to query the source code.\n\n        Do not try to fuzz functions of the project that are static, since they\n        are only visible in the file that they were declared. Choose other\n        user-facing functions instead.\n\n        &lt;/important&gt;\n\n        **Do not truncate the input to a smaller size that the original**,\n        e.g. for avoiding large stack usage or to avoid excessive buffers. Opt\n        to using the heap when possible to increase the chance of exposing\n        memory errors of the library, e.g. mmap instead of declaring\n        buf[1024]. Any edge cases should be handled by the library itself, not\n        the harness. On the other hand, do not write code that will most\n        probably crash irregardless of the library under test. The point is for\n        a function of the library under test to crash, not the harness\n        itself. Use and take advantage of any custom structs that the library\n        declares.\n\n        Do not copy function declarations inside the harness. The harness will\n        be compiled in the root directory of the project.  \"\"\"\n    )\n\n\nclass FixHarness(dspy.Signature):\n    \"\"\"\n    You are an experienced C/C++ security testing engineer. Given a\n    libFuzzer-compatible harness that fails to compile and its compilation\n    errors, rewrite it so that it compiles successfully. Analyze the compilation\n    errors carefully and find the root causes. Add any missing #includes like\n    &lt;string.h&gt;, &lt;stdint.h&gt; and &lt;stdlib.h&gt; and #define required macros or\n    constants in the fuzz target. If needed, re-declare functions or struct\n    types. Add verbose comments to explain what you changed and why.\n    \"\"\"\n\n    old_harness: str = dspy.InputField(desc=\"The harness to be fixed.\")\n    error: str = dspy.InputField(desc=\"The compilaton error of the harness.\")\n    new_harness: str = dspy.OutputField(\n        desc=\"\"\"The newly created harness with the necessary modifications for\n        correct compilation.\"\"\"\n    )\n\n\nclass ImproveHarness(dspy.Signature):\n    f\"\"\"\n    You are an experienced C/C++ security testing engineer. Given a\n    libFuzzer-compatible harness that does not find any bug/does not crash (even\n    after running for {Config.EXECUTION_TIMEOUT} seconds) or has memory leaks\n    (generates leak files), you are called to rewrite it and improve it so that\n    a bug can be found more easily and/or memory is managed correctly. Determine\n    the information you need to write an effective fuzz target and understand\n    constraints and edge cases in the source code to do it more\n    effectively. Reply only with the source code --- without backticks.  Add\n    verbose comments to explain what you changed and why.\n    \"\"\"\n\n    old_harness: str = dspy.InputField(\n        desc=\"The harness to be improved so it can find a bug more quickly.\"\n    )\n    output: str = dspy.InputField(desc=\"The output of the harness' execution.\")\n    new_harness: str = dspy.OutputField(\n        desc=\"\"\"The newly created harness with the necessary modifications for\n        quicker bug-finding. If the provided harness has unnecessary input\n        limitations regarding size or format etc., remove them.\"\"\"\n    )",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>DSPy Custom Signatures</span>"
    ]
  }
]