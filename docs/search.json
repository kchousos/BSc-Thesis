[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LLM-Driven Fuzzing",
    "section": "",
    "text": "Preface\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\n\n\n\nAcknowledgments\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\n\n\n\n\nCitation\n\n\n\nBibTeX citation:\n\n@thesis{chousosLLMDrivenFuzzing2025,\n  title = {LLM-Driven Fuzzing: Automatic Harness Generation for Crypto Libraries},\n  shorttitle = {LLM-Driven Fuzzing},\n  author = {Chousos, Konstantinos},\n  date = {2025-07},\n  institution = {{National and Kapodistrian University of Athens}},\n  location = {Athens, Greece},\n  url = {https://kchousos.github.io/thesis/},\n  langid = {en, el}\n}\n\nFor attribution, please cite this work as:\n\n\nK. Chousos, “LLM-Driven Fuzzing: Automatic Harness Generation for Crypto Libraries,” Bachelor Thesis, National and Kapodistrian University of Athens, Athens, Greece, 2025. [Online]. Available: https://kchousos.github.io/thesis/\n\n\n\n\n\n\n\n\n[1] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[2] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[3] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[4] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining incremental steps of fuzzing research,” in 14th USENIX workshop on offensive technologies (WOOT 20), USENIX Association, Aug. 2020.\n\n\n[5] R. Anderson, “Why cryptosystems fail,” in Proceedings of the 1st ACM conference on Computer and communications security - CCS ’93, Fairfax, Virginia, United States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615. Available: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[6] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[7] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[8] Y. Cheng et al., “Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,” Mar. 02, 2025. doi: 10.48550/arXiv.2503.00795. Available: http://arxiv.org/abs/2503.00795\n\n\n[9] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[10] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[13] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[14] W. Gao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein, “Beyond the Coverage Plateau: A Comprehensive Study of Fuzz Blockers (Registered Report),” in Proceedings of the 2nd International Fuzzing Workshop, in FUZZING 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177. Available: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[15] Y. Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” Mar. 27, 2024. doi: 10.48550/arXiv.2312.10997. Available: http://arxiv.org/abs/2312.10997\n\n\n[16] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[17] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[18] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013. Available: https://ieeexplore.ieee.org/document/8089448/\n\n\n[19] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[20] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[21] “Heartbleed Bug.” Available: https://heartbleed.com/\n\n\n[22] A. Herrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking, “Seed selection for successful fuzzing,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Denmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795. Available: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[23] L. Huang, P. Zhao, H. Chen, and L. Ma, “Large language models based fuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[24] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[25] R. I. T. Jensen, V. Tawosi, and S. Alamir, “Software Vulnerability and Functionality Assessment using LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429. Available: http://arxiv.org/abs/2403.08429\n\n\n[26] S. Y. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May 07, 2024. doi: 10.48550/arXiv.2405.03927. Available: http://arxiv.org/abs/2405.03927\n\n\n[27] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[28] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[29] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[30] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[31] D. Lazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic software fail? A case study and open problems,” in Proceedings of 5th Asia-Pacific Workshop on Systems, in APSys ’14. New York, NY, USA: Association for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237. Available: https://doi.org/10.1145/2637166.2637237\n\n\n[32] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[33] C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic software repair,” Software Qual J, vol. 21, no. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0. Available: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[34] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[35] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[36] J. C. R. Licklider, “Man-Computer Symbiosis,” IRE Transactions on Human Factors in Electronics, vol. HFE–1, no. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259. Available: https://ieeexplore.ieee.org/document/4503259\n\n\n[37] J. Liu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate Regression Tests for Software Commits?” Jan. 19, 2025. doi: 10.48550/arXiv.2501.11086. Available: http://arxiv.org/abs/2501.11086\n\n\n[38] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[39] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[40] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[41] C. Meyer and J. Schwenk, “Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[42] N. Nethercote and J. Seward, “Valgrind: A framework for heavyweight dynamic binary instrumentation,” SIGPLAN Not., vol. 42, no. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746. Available: https://doi.org/10.1145/1273442.1250746\n\n\n[43] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[44] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[45] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[46] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[47] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[48] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[49] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[50] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[51] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[52] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[53] I. Tzachristas, “Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233. Available: http://arxiv.org/abs/2412.13233\n\n\n[54] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[55] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[56] A. Zebaze, B. Sagot, and R. Bawden, “Tree of Problems: Improving structured problem solving with compositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634. Available: http://arxiv.org/abs/2410.06634\n\n\n[57] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: http://arxiv.org/abs/2307.12469\n\n\n[58] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2024. New York, NY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K. Zhang et al., “Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312. Available: http://arxiv.org/abs/2501.04312\n\n\n[60] P. Y. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller, “A Guide to Large Language Model Abstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A. Zibaeirad and M. Vieira, “Reasoning with LLMs for Zero-Shot Vulnerability Detection,” Mar. 22, 2025. doi: 10.48550/arXiv.2503.17885. Available: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next Heartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier.” Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz Documentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/intro.html",
    "href": "chapters/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Fuzzing\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nWhat is fuzzing [1].\nWhy fuzz?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#fuzzing",
    "href": "chapters/intro.html#fuzzing",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Fuzzing examples\nHeartbleed [2], shellshock [3].\n\n\n1.1.2 Fuzzer engines\nC/C++: AFL [4] & AFL++ [4], ++. LibFuzzer [5].\nPython: Atheris [6].\nJava, Rust etc…\nAn example of a fuzz target/harness can be seen in Listing 1.1 [5].\n\n\n\nListing 1.1: A simple function that does something interesting if it receives the input “HI!”.\n\n\ncat &lt;&lt; EOF &gt; test_fuzzer.cc\n#include &lt;stdint.h&gt;\n#include &lt;stddef.h&gt;\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n  if (size &gt; 0 && data[0] == 'H')\n    if (size &gt; 1 && data[1] == 'I')\n       if (size &gt; 2 && data[2] == '!')\n       __builtin_trap();\n  return 0;\n}\nEOF\n# Build test_fuzzer.cc with asan and link against libFuzzer.\nclang++ -fsanitize=address,fuzzer test_fuzzer.cc\n# Run the fuzzer with no corpus.\n./a.out",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#large-language-models-llms",
    "href": "chapters/intro.html#large-language-models-llms",
    "title": "1  Introduction",
    "section": "1.2 Large Language Models (LLMs)",
    "text": "1.2 Large Language Models (LLMs)\nTransformers [7], 2017–2025. ChatGPT/OpenAI history & context. Claude, Llama (1–3) etc.\n\n1.2.1 Prompting\nPrompting techniques.\n\nZero-shot.\nChain of Thought [8].\nReACt [9].\nTree of Thoughts [10].\n\nComparison, strengths weaknesses etc. [11].",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/intro.html#neurosymbolic-ai",
    "href": "chapters/intro.html#neurosymbolic-ai",
    "title": "1  Introduction",
    "section": "1.3 Neurosymbolic AI",
    "text": "1.3 Neurosymbolic AI\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream TODO [12], [13], [14], [15], [16], [17]. ======= ==TODO== [12], [13], [14], [15], [16], [17]. &gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes\n\n\n\n\n[1] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[2] “Heartbleed Bug.” Available: https://heartbleed.com/\n\n\n[3] C. Meyer and J. Schwenk, “Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[4] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[5] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[6] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[7] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[8] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[9] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[10] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[11] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[12] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[13] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[14] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[15] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[16] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[17] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[18] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[19] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[20] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining incremental steps of fuzzing research,” in 14th USENIX workshop on offensive technologies (WOOT 20), USENIX Association, Aug. 2020.\n\n\n[21] R. Anderson, “Why cryptosystems fail,” in Proceedings of the 1st ACM conference on Computer and communications security - CCS ’93, Fairfax, Virginia, United States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615. Available: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[22] Y. Cheng et al., “Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,” Mar. 02, 2025. doi: 10.48550/arXiv.2503.00795. Available: http://arxiv.org/abs/2503.00795\n\n\n[23] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[24] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[25] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[26] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[27] W. Gao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein, “Beyond the Coverage Plateau: A Comprehensive Study of Fuzz Blockers (Registered Report),” in Proceedings of the 2nd International Fuzzing Workshop, in FUZZING 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177. Available: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[28] Y. Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” Mar. 27, 2024. doi: 10.48550/arXiv.2312.10997. Available: http://arxiv.org/abs/2312.10997\n\n\n[29] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013. Available: https://ieeexplore.ieee.org/document/8089448/\n\n\n[30] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[31] A. Herrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking, “Seed selection for successful fuzzing,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Denmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795. Available: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[32] L. Huang, P. Zhao, H. Chen, and L. Ma, “Large language models based fuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[33] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[34] R. I. T. Jensen, V. Tawosi, and S. Alamir, “Software Vulnerability and Functionality Assessment using LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429. Available: http://arxiv.org/abs/2403.08429\n\n\n[35] S. Y. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May 07, 2024. doi: 10.48550/arXiv.2405.03927. Available: http://arxiv.org/abs/2405.03927\n\n\n[36] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[37] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[38] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[39] D. Lazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic software fail? A case study and open problems,” in Proceedings of 5th Asia-Pacific Workshop on Systems, in APSys ’14. New York, NY, USA: Association for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237. Available: https://doi.org/10.1145/2637166.2637237\n\n\n[40] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[41] C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic software repair,” Software Qual J, vol. 21, no. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0. Available: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[42] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[43] J. C. R. Licklider, “Man-Computer Symbiosis,” IRE Transactions on Human Factors in Electronics, vol. HFE–1, no. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259. Available: https://ieeexplore.ieee.org/document/4503259\n\n\n[44] J. Liu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate Regression Tests for Software Commits?” Jan. 19, 2025. doi: 10.48550/arXiv.2501.11086. Available: http://arxiv.org/abs/2501.11086\n\n\n[45] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[46] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[47] N. Nethercote and J. Seward, “Valgrind: A framework for heavyweight dynamic binary instrumentation,” SIGPLAN Not., vol. 42, no. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746. Available: https://doi.org/10.1145/1273442.1250746\n\n\n[48] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[49] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[50] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[51] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[52] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[53] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[54] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[55] I. Tzachristas, “Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233. Available: http://arxiv.org/abs/2412.13233\n\n\n[56] A. Zebaze, B. Sagot, and R. Bawden, “Tree of Problems: Improving structured problem solving with compositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634. Available: http://arxiv.org/abs/2410.06634\n\n\n[57] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: http://arxiv.org/abs/2307.12469\n\n\n[58] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2024. New York, NY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K. Zhang et al., “Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312. Available: http://arxiv.org/abs/2501.04312\n\n\n[60] P. Y. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller, “A Guide to Large Language Model Abstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A. Zibaeirad and M. Vieira, “Reasoning with LLMs for Zero-Shot Vulnerability Detection,” Mar. 22, 2025. doi: 10.48550/arXiv.2503.17885. Available: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next Heartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier.” Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz Documentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/background.html",
    "href": "chapters/background.html",
    "title": "2  Background Work",
    "section": "",
    "text": "2.1 Automatic Harnesses\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nWhere we are right now. SOTA projects. Similar projects using LLMs in the fuzzing space [1], [2], [3].\nTODO.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background Work</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#automatic-harnesses",
    "href": "chapters/background.html#automatic-harnesses",
    "title": "2  Background Work",
    "section": "",
    "text": "2.1.1 Google\nFuzzGen, FUDGE, OSS-Fuzz-Gen [4], [5], [6], [7].\n\n2.1.1.1 OSS-Fuzz-Gen\nFeatures/caveats. from_scratch branch1.\n\n\ncommit 171aac2↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background Work</span>"
    ]
  },
  {
    "objectID": "chapters/background.html#llm-programming-libraries",
    "href": "chapters/background.html#llm-programming-libraries",
    "title": "2  Background Work",
    "section": "2.2 LLM Programming Libraries",
    "text": "2.2 LLM Programming Libraries\nLangchain & LangGraph, LlamaIndex [8], [9], [10]. DSPy [11].\nComparison, relevance to our usecase.\n\n\n\n\n[1] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[2] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[3] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[4] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[5] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[6] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[7] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[8] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[9] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[10] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[11] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[12] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[13] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[14] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[15] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining incremental steps of fuzzing research,” in 14th USENIX workshop on offensive technologies (WOOT 20), USENIX Association, Aug. 2020.\n\n\n[16] R. Anderson, “Why cryptosystems fail,” in Proceedings of the 1st ACM conference on Computer and communications security - CCS ’93, Fairfax, Virginia, United States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615. Available: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[17] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[18] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[19] Y. Cheng et al., “Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,” Mar. 02, 2025. doi: 10.48550/arXiv.2503.00795. Available: http://arxiv.org/abs/2503.00795\n\n\n[20] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[21] W. Gao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein, “Beyond the Coverage Plateau: A Comprehensive Study of Fuzz Blockers (Registered Report),” in Proceedings of the 2nd International Fuzzing Workshop, in FUZZING 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177. Available: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[22] Y. Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” Mar. 27, 2024. doi: 10.48550/arXiv.2312.10997. Available: http://arxiv.org/abs/2312.10997\n\n\n[23] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[24] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[25] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013. Available: https://ieeexplore.ieee.org/document/8089448/\n\n\n[26] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[27] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[28] “Heartbleed Bug.” Available: https://heartbleed.com/\n\n\n[29] A. Herrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking, “Seed selection for successful fuzzing,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Denmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795. Available: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[30] L. Huang, P. Zhao, H. Chen, and L. Ma, “Large language models based fuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[31] R. I. T. Jensen, V. Tawosi, and S. Alamir, “Software Vulnerability and Functionality Assessment using LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429. Available: http://arxiv.org/abs/2403.08429\n\n\n[32] S. Y. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May 07, 2024. doi: 10.48550/arXiv.2405.03927. Available: http://arxiv.org/abs/2405.03927\n\n\n[33] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[34] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[35] D. Lazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic software fail? A case study and open problems,” in Proceedings of 5th Asia-Pacific Workshop on Systems, in APSys ’14. New York, NY, USA: Association for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237. Available: https://doi.org/10.1145/2637166.2637237\n\n\n[36] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[37] C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic software repair,” Software Qual J, vol. 21, no. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0. Available: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[38] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[39] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[40] J. C. R. Licklider, “Man-Computer Symbiosis,” IRE Transactions on Human Factors in Electronics, vol. HFE–1, no. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259. Available: https://ieeexplore.ieee.org/document/4503259\n\n\n[41] J. Liu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate Regression Tests for Software Commits?” Jan. 19, 2025. doi: 10.48550/arXiv.2501.11086. Available: http://arxiv.org/abs/2501.11086\n\n\n[42] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[43] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[44] C. Meyer and J. Schwenk, “Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[45] N. Nethercote and J. Seward, “Valgrind: A framework for heavyweight dynamic binary instrumentation,” SIGPLAN Not., vol. 42, no. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746. Available: https://doi.org/10.1145/1273442.1250746\n\n\n[46] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[47] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[48] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[49] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[50] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[51] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[52] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[53] I. Tzachristas, “Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233. Available: http://arxiv.org/abs/2412.13233\n\n\n[54] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[55] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[56] A. Zebaze, B. Sagot, and R. Bawden, “Tree of Problems: Improving structured problem solving with compositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634. Available: http://arxiv.org/abs/2410.06634\n\n\n[57] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: http://arxiv.org/abs/2307.12469\n\n\n[58] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2024. New York, NY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K. Zhang et al., “Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312. Available: http://arxiv.org/abs/2501.04312\n\n\n[60] P. Y. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller, “A Guide to Large Language Model Abstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A. Zibaeirad and M. Vieira, “Reasoning with LLMs for Zero-Shot Vulnerability Detection,” Mar. 22, 2025. doi: 10.48550/arXiv.2503.17885. Available: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next Heartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier.” Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz Documentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background Work</span>"
    ]
  },
  {
    "objectID": "chapters/implementation.html",
    "href": "chapters/implementation.html",
    "title": "3  LLM-Harness",
    "section": "",
    "text": "3.1 Scope of Usage",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-Harness</span>"
    ]
  },
  {
    "objectID": "chapters/implementation.html#scope-of-usage",
    "href": "chapters/implementation.html#scope-of-usage",
    "title": "3  LLM-Harness",
    "section": "",
    "text": "In what contexts does it work?\nPrerequisites",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-Harness</span>"
    ]
  },
  {
    "objectID": "chapters/implementation.html#main-library-architecturestructure",
    "href": "chapters/implementation.html#main-library-architecturestructure",
    "title": "3  LLM-Harness",
    "section": "3.2 Main Library Architecture/Structure",
    "text": "3.2 Main Library Architecture/Structure",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-Harness</span>"
    ]
  },
  {
    "objectID": "chapters/implementation.html#github-workflowusage",
    "href": "chapters/implementation.html#github-workflowusage",
    "title": "3  LLM-Harness",
    "section": "3.3 GitHub Workflow/Usage",
    "text": "3.3 GitHub Workflow/Usage",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-Harness</span>"
    ]
  },
  {
    "objectID": "chapters/implementation.html#future-workextensions",
    "href": "chapters/implementation.html#future-workextensions",
    "title": "3  LLM-Harness",
    "section": "3.4 Future work/extensions",
    "text": "3.4 Future work/extensions\n\nBuild system\nGeneral localization problem\n\n\n\n\n\n[1] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[2] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[3] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[4] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining incremental steps of fuzzing research,” in 14th USENIX workshop on offensive technologies (WOOT 20), USENIX Association, Aug. 2020.\n\n\n[5] R. Anderson, “Why cryptosystems fail,” in Proceedings of the 1st ACM conference on Computer and communications security - CCS ’93, Fairfax, Virginia, United States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615. Available: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[6] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[7] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[8] Y. Cheng et al., “Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,” Mar. 02, 2025. doi: 10.48550/arXiv.2503.00795. Available: http://arxiv.org/abs/2503.00795\n\n\n[9] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[10] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[13] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[14] W. Gao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein, “Beyond the Coverage Plateau: A Comprehensive Study of Fuzz Blockers (Registered Report),” in Proceedings of the 2nd International Fuzzing Workshop, in FUZZING 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177. Available: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[15] Y. Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” Mar. 27, 2024. doi: 10.48550/arXiv.2312.10997. Available: http://arxiv.org/abs/2312.10997\n\n\n[16] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[17] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[18] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013. Available: https://ieeexplore.ieee.org/document/8089448/\n\n\n[19] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[20] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[21] “Heartbleed Bug.” Available: https://heartbleed.com/\n\n\n[22] A. Herrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking, “Seed selection for successful fuzzing,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Denmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795. Available: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[23] L. Huang, P. Zhao, H. Chen, and L. Ma, “Large language models based fuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[24] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[25] R. I. T. Jensen, V. Tawosi, and S. Alamir, “Software Vulnerability and Functionality Assessment using LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429. Available: http://arxiv.org/abs/2403.08429\n\n\n[26] S. Y. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May 07, 2024. doi: 10.48550/arXiv.2405.03927. Available: http://arxiv.org/abs/2405.03927\n\n\n[27] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[28] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[29] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[30] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[31] D. Lazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic software fail? A case study and open problems,” in Proceedings of 5th Asia-Pacific Workshop on Systems, in APSys ’14. New York, NY, USA: Association for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237. Available: https://doi.org/10.1145/2637166.2637237\n\n\n[32] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[33] C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic software repair,” Software Qual J, vol. 21, no. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0. Available: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[34] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[35] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[36] J. C. R. Licklider, “Man-Computer Symbiosis,” IRE Transactions on Human Factors in Electronics, vol. HFE–1, no. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259. Available: https://ieeexplore.ieee.org/document/4503259\n\n\n[37] J. Liu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate Regression Tests for Software Commits?” Jan. 19, 2025. doi: 10.48550/arXiv.2501.11086. Available: http://arxiv.org/abs/2501.11086\n\n\n[38] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[39] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[40] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[41] C. Meyer and J. Schwenk, “Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[42] N. Nethercote and J. Seward, “Valgrind: A framework for heavyweight dynamic binary instrumentation,” SIGPLAN Not., vol. 42, no. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746. Available: https://doi.org/10.1145/1273442.1250746\n\n\n[43] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[44] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[45] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[46] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[47] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[48] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[49] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[50] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[51] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[52] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[53] I. Tzachristas, “Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233. Available: http://arxiv.org/abs/2412.13233\n\n\n[54] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[55] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[56] A. Zebaze, B. Sagot, and R. Bawden, “Tree of Problems: Improving structured problem solving with compositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634. Available: http://arxiv.org/abs/2410.06634\n\n\n[57] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: http://arxiv.org/abs/2307.12469\n\n\n[58] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2024. New York, NY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K. Zhang et al., “Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312. Available: http://arxiv.org/abs/2501.04312\n\n\n[60] P. Y. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller, “A Guide to Large Language Model Abstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A. Zibaeirad and M. Vieira, “Reasoning with LLMs for Zero-Shot Vulnerability Detection,” Mar. 22, 2025. doi: 10.48550/arXiv.2503.17885. Available: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next Heartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier.” Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz Documentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-Harness</span>"
    ]
  },
  {
    "objectID": "chapters/conclusion.html",
    "href": "chapters/conclusion.html",
    "title": "4  Conclusion",
    "section": "",
    "text": "What works/what doesn’t\nMore general future steps for the field in general\nRecap\n\n\n\n\n\n[1] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[2] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[3] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[4] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining incremental steps of fuzzing research,” in 14th USENIX workshop on offensive technologies (WOOT 20), USENIX Association, Aug. 2020.\n\n\n[5] R. Anderson, “Why cryptosystems fail,” in Proceedings of the 1st ACM conference on Computer and communications security - CCS ’93, Fairfax, Virginia, United States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615. Available: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[6] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[7] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[8] Y. Cheng et al., “Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,” Mar. 02, 2025. doi: 10.48550/arXiv.2503.00795. Available: http://arxiv.org/abs/2503.00795\n\n\n[9] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[10] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[13] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[14] W. Gao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein, “Beyond the Coverage Plateau: A Comprehensive Study of Fuzz Blockers (Registered Report),” in Proceedings of the 2nd International Fuzzing Workshop, in FUZZING 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177. Available: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[15] Y. Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” Mar. 27, 2024. doi: 10.48550/arXiv.2312.10997. Available: http://arxiv.org/abs/2312.10997\n\n\n[16] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[17] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[18] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013. Available: https://ieeexplore.ieee.org/document/8089448/\n\n\n[19] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[20] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[21] “Heartbleed Bug.” Available: https://heartbleed.com/\n\n\n[22] A. Herrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking, “Seed selection for successful fuzzing,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Denmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795. Available: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[23] L. Huang, P. Zhao, H. Chen, and L. Ma, “Large language models based fuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[24] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[25] R. I. T. Jensen, V. Tawosi, and S. Alamir, “Software Vulnerability and Functionality Assessment using LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429. Available: http://arxiv.org/abs/2403.08429\n\n\n[26] S. Y. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May 07, 2024. doi: 10.48550/arXiv.2405.03927. Available: http://arxiv.org/abs/2405.03927\n\n\n[27] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[28] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[29] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[30] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[31] D. Lazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic software fail? A case study and open problems,” in Proceedings of 5th Asia-Pacific Workshop on Systems, in APSys ’14. New York, NY, USA: Association for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237. Available: https://doi.org/10.1145/2637166.2637237\n\n\n[32] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[33] C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic software repair,” Software Qual J, vol. 21, no. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0. Available: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[34] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[35] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[36] J. C. R. Licklider, “Man-Computer Symbiosis,” IRE Transactions on Human Factors in Electronics, vol. HFE–1, no. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259. Available: https://ieeexplore.ieee.org/document/4503259\n\n\n[37] J. Liu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate Regression Tests for Software Commits?” Jan. 19, 2025. doi: 10.48550/arXiv.2501.11086. Available: http://arxiv.org/abs/2501.11086\n\n\n[38] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[39] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[40] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[41] C. Meyer and J. Schwenk, “Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[42] N. Nethercote and J. Seward, “Valgrind: A framework for heavyweight dynamic binary instrumentation,” SIGPLAN Not., vol. 42, no. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746. Available: https://doi.org/10.1145/1273442.1250746\n\n\n[43] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[44] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[45] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[46] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[47] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[48] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[49] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[50] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[51] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[52] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[53] I. Tzachristas, “Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233. Available: http://arxiv.org/abs/2412.13233\n\n\n[54] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[55] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[56] A. Zebaze, B. Sagot, and R. Bawden, “Tree of Problems: Improving structured problem solving with compositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634. Available: http://arxiv.org/abs/2410.06634\n\n\n[57] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: http://arxiv.org/abs/2307.12469\n\n\n[58] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2024. New York, NY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K. Zhang et al., “Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312. Available: http://arxiv.org/abs/2501.04312\n\n\n[60] P. Y. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller, “A Guide to Large Language Model Abstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A. Zibaeirad and M. Vieira, “Reasoning with LLMs for Zero-Shot Vulnerability Detection,” Mar. 22, 2025. doi: 10.48550/arXiv.2503.17885. Available: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next Heartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier.” Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz Documentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/refs.html",
    "href": "chapters/refs.html",
    "title": "Bibliography",
    "section": "",
    "text": "[1] “Google/clusterfuzz.” Google, Apr.\n09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[2] “American fuzzy lop.” Available:\nhttps://lcamtuf.coredump.cx/afl/\n\n\n[3] M.\nHeuse, H. Eißfeldt, A. Fioraldi, and D. Maier,\n“AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[4] A.\nFioraldi, D. Maier, H. Eißfeldt, and M. Heuse,\n“AFL++: Combining incremental steps of\nfuzzing research,” in 14th USENIX workshop on\noffensive technologies (WOOT 20), USENIX Association,\nAug. 2020.\n\n\n[5] R.\nAnderson, “Why cryptosystems fail,” in Proceedings of\nthe 1st ACM conference on Computer and\ncommunications security - CCS ’93, Fairfax, Virginia,\nUnited States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615.\nAvailable: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[6] “Google/atheris.” Google, Apr. 09,\n2025. Available: https://github.com/google/atheris\n\n\n[7] J.\nWei et al., “Chain-of-Thought Prompting Elicits\nReasoning in Large Language Models,” Jan. 10,\n2023. doi: 10.48550/arXiv.2201.11903.\nAvailable: http://arxiv.org/abs/2201.11903\n\n\n[8] Y.\nCheng et al., “Towards Reliable LLM-Driven Fuzz\nTesting: Vision and Road Ahead,”\nMar. 02, 2025. doi: 10.48550/arXiv.2503.00795.\nAvailable: http://arxiv.org/abs/2503.00795\n\n\n[9] O.\nKhattab et al., “DSPy: Compiling\nDeclarative Language Model Calls into Self-Improving\nPipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714.\nAvailable: http://arxiv.org/abs/2310.03714\n\n\n[10] D.\nBabić et al., “FUDGE: Fuzz driver generation\nat scale,” in Proceedings of the 2019 27th ACM Joint\nMeeting on European Software Engineering Conference\nand Symposium on the Foundations of\nSoftware Engineering, Tallinn Estonia: ACM, Aug. 2019,\npp. 975–985. doi: 10.1145/3338906.3340456.\nAvailable: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[11] K.\nIspoglou, D. Austin, V. Mohan, and M. Payer,\n“FuzzGen: Automatic fuzzer\ngeneration,” in 29th USENIX Security Symposium\n(USENIX Security 20), 2020, pp. 2271–2287. Available:\nhttps://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] Y.\nDeng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,\n“Large Language Models are Edge-Case\nFuzzers: Testing Deep Learning Libraries via\nFuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014.\nAvailable: http://arxiv.org/abs/2304.02014\n\n\n[13] D.\nGanguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of\nThought : Neurosymbolic Program Synthesis\nallows Robust and Interpretable\nReasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270.\nAvailable: http://arxiv.org/abs/2409.17270\n\n\n[14] W.\nGao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein,\n“Beyond the Coverage Plateau: A Comprehensive\nStudy of Fuzz Blockers (Registered\nReport),” in Proceedings of the 2nd\nInternational Fuzzing Workshop, in\nFUZZING 2023. New York, NY, USA: Association for Computing\nMachinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177.\nAvailable: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[15] Y.\nGao et al., “Retrieval-Augmented Generation\nfor Large Language Models: A Survey,”\nMar. 27, 2024. doi: 10.48550/arXiv.2312.10997.\nAvailable: http://arxiv.org/abs/2312.10997\n\n\n[16] A.\nd’Avila Garcez and L. C. Lamb, “Neurosymbolic AI:\nThe 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876.\nAvailable: http://arxiv.org/abs/2012.05876\n\n\n[17] M.\nGaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI\nSystems: Consistency, Reliability,\nExplainability, and Safety,” Dec. 05,\n2023. doi: 10.48550/arXiv.2312.06798.\nAvailable: http://arxiv.org/abs/2312.06798\n\n\n[18] L.\nGazzola, D. Micucci, and L. Mariani, “Automatic Software\nRepair: A Survey,” IEEE Transactions on\nSoftware Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013.\nAvailable: https://ieeexplore.ieee.org/document/8089448/\n\n\n[19] D.\nGiannone, “Demystifying AI Agents: ReAct-Style\nAgents vs Agentic Workflows,” Feb. 09, 2025.\nAvailable: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[20] G.\nGrov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V.\nMavroeidis, “On the use of neurosymbolic AI for\ndefending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996.\nAvailable: http://arxiv.org/abs/2408.04996\n\n\n[21] “Heartbleed Bug.”\nAvailable: https://heartbleed.com/\n\n\n[22] A.\nHerrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking,\n“Seed selection for successful fuzzing,” in Proceedings\nof the 30th ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis, Virtual\nDenmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795.\nAvailable: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[23] L.\nHuang, P. Zhao, H. Chen, and L. Ma, “Large language models based\nfuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[24] Z.\nLi, S. Dutta, and M. Naik, “IRIS: LLM-Assisted\nStatic Analysis for Detecting Security\nVulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238.\nAvailable: http://arxiv.org/abs/2405.17238\n\n\n[25] R.\nI. T. Jensen, V. Tawosi, and S. Alamir, “Software\nVulnerability and Functionality Assessment\nusing LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429.\nAvailable: http://arxiv.org/abs/2403.08429\n\n\n[26] S.\nY. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May\n07, 2024. doi: 10.48550/arXiv.2405.03927.\nAvailable: http://arxiv.org/abs/2405.03927\n\n\n[27] C.\nCadar, D. Dunbar, and D. Engler, “KLEE:\nUnassisted and Automatic Generation of\nHigh-Coverage Tests for Complex Systems\nPrograms,” presented at the USENIX Symposium\non Operating Systems Design and\nImplementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[28] P.\nLaban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost\nIn Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120.\nAvailable: http://arxiv.org/abs/2505.06120\n\n\n[29] H.\nChase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[30] “Langchain-ai/langgraph.”\nLangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[31] D.\nLazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic\nsoftware fail? A case study and open problems,” in\nProceedings of 5th Asia-Pacific Workshop on\nSystems, in APSys ’14. New York, NY, USA:\nAssociation for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237.\nAvailable: https://doi.org/10.1145/2637166.2637237\n\n\n[32] H.-P. H. Lee et al., “The\nImpact of Generative AI on Critical\nThinking: Self-Reported Reductions in\nCognitive Effort and Confidence Effects From a\nSurvey of Knowledge Workers,” 2025,\nAvailable: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[33] C.\nLe Goues, S. Forrest, and W. Weimer, “Current challenges in\nautomatic software repair,” Software Qual J, vol. 21,\nno. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0.\nAvailable: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[34] H.\nLi, “Language models: Past, present, and future,”\nCommun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available:\nhttps://dl.acm.org/doi/10.1145/3490443\n\n\n[35] “libFuzzer –\na library for coverage-guided fuzz testing. — LLVM\n21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[36] J.\nC. R. Licklider, “Man-Computer Symbiosis,”\nIRE Transactions on Human Factors in Electronics, vol. HFE–1,\nno. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259.\nAvailable: https://ieeexplore.ieee.org/document/4503259\n\n\n[37] J.\nLiu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate\nRegression Tests for Software Commits?” Jan.\n19, 2025. doi: 10.48550/arXiv.2501.11086.\nAvailable: http://arxiv.org/abs/2501.11086\n\n\n[38] J.\nLiu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234.\nAvailable: https://github.com/jerryjliu/llama_index\n\n\n[39] Y.\nLyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing\nfor Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677.\nAvailable: http://arxiv.org/abs/2312.17677\n\n\n[40] V.\nJ. M. Manes et al., “The Art,\nScience, and Engineering of\nFuzzing: A Survey,” Apr. 07, 2019. doi:\n10.48550/arXiv.1812.00140.\nAvailable: http://arxiv.org/abs/1812.00140\n\n\n[41] C.\nMeyer and J. Schwenk, “Lessons Learned From Previous\nSSL/TLS Attacks - A Brief Chronology Of\nAttacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[42] N.\nNethercote and J. Seward, “Valgrind: A framework for heavyweight\ndynamic binary instrumentation,” SIGPLAN Not., vol. 42,\nno. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746.\nAvailable: https://doi.org/10.1145/1273442.1250746\n\n\n[43] A.\nArya, O. Chang, J. Metzman, K. Serebryany, and D. Liu,\n“OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[44] D.\nLiu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target\ngeneration.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[45] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed\nprojects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[46] N.\nPerry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users\nWrite More Insecure Code with AI Assistants?”\nDec. 18, 2023. doi: 10.48550/arXiv.2211.03622.\nAvailable: http://arxiv.org/abs/2211.03622\n\n\n[47] D.\nWang, G. Zhou, L. Chen, D. Li, and Y. Miao,\n“ProphetFuzz: Fully Automated Prediction\nand Fuzzing of High-Risk Option Combinations\nwith Only Documentation via Large Language\nModel,” Sep. 01, 2024. doi: 10.1145/3658644.3690231.\nAvailable: http://arxiv.org/abs/2409.00922\n\n\n[48] S.\nYao et al., “ReAct: Synergizing\nReasoning and Acting in Language\nModels,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629.\nAvailable: http://arxiv.org/abs/2210.03629\n\n\n[49] A.\nSheth, K. Roy, and M. Gaur, “Neurosymbolic AI –\nWhy, What, and How,” May\n01, 2023. doi: 10.48550/arXiv.2305.00813.\nAvailable: http://arxiv.org/abs/2305.00813\n\n\n[50] T.\nSimonite, “This Bot Hunts Software Bugs for the\nPentagon,” Wired, Jun. 01, 2020. Available:\nhttps://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[51] D.\nTilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic\nAI approach to Attribution in Large\nLanguage Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726.\nAvailable: http://arxiv.org/abs/2410.03726\n\n\n[52] Y.\nDeng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large\nLanguage Models Are Zero-Shot Fuzzers: Fuzzing\nDeep-Learning Libraries via Large Language\nModels,” in Proceedings of the 32nd ACM SIGSOFT\nInternational Symposium on Software Testing and\nAnalysis, in ISSTA 2023. New York, NY,\nUSA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi:\n10.1145/3597926.3598067.\nAvailable: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[53] I.\nTzachristas, “Creating an LLM-based\nAI-agent: A high-level methodology towards enhancing\nLLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233.\nAvailable: http://arxiv.org/abs/2412.13233\n\n\n[54] A.\nVaswani et al., “Attention Is All You\nNeed,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762.\nAvailable: http://arxiv.org/abs/1706.03762\n\n\n[55] S.\nYao et al., “Tree of Thoughts:\nDeliberate Problem Solving with Large Language\nModels,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601.\nAvailable: http://arxiv.org/abs/2305.10601\n\n\n[56] A.\nZebaze, B. Sagot, and R. Bawden, “Tree of Problems:\nImproving structured problem solving with\ncompositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634.\nAvailable: http://arxiv.org/abs/2410.06634\n\n\n[57] C.\nZhang et al., “How Effective Are They?\nExploring Large Language Model Based Fuzz Driver\nGeneration,” in Proceedings of the 33rd ACM\nSIGSOFT International Symposium on Software Testing\nand Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355.\nAvailable: http://arxiv.org/abs/2307.12469\n\n\n[58] C.\nZhang et al., “How Effective Are They?\nExploring Large Language Model Based Fuzz Driver\nGeneration,” in Proceedings of the 33rd ACM\nSIGSOFT International Symposium on Software Testing\nand Analysis, in ISSTA 2024. New York,\nNY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235.\ndoi: 10.1145/3650212.3680355.\nAvailable: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K.\nZhang et al., “Your Fix Is My Exploit:\nEnabling Comprehensive DL Library API Fuzzing with\nLarge Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312.\nAvailable: http://arxiv.org/abs/2501.04312\n\n\n[60] P.\nY. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller,\n“A Guide to Large Language Model\nAbstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A.\nZibaeirad and M. Vieira, “Reasoning with LLMs for\nZero-Shot Vulnerability Detection,” Mar. 22, 2025.\ndoi: 10.48550/arXiv.2503.17885.\nAvailable: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next\nHeartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing:\nBreaking the Bug Hunting Barrier.”\nAvailable: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz\nDocumentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "chapters/appendix.html",
    "href": "chapters/appendix.html",
    "title": "Appendix A — Failed Techniques",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\nDuis ornare ex ac iaculis pretium. Maecenas sagittis odio id erat pharetra, sit amet consectetur quam sollicitudin. Vivamus pharetra quam purus, nec sagittis risus pretium at. Nullam feugiat, turpis ac accumsan interdum, sem tellus blandit neque, id vulputate diam quam semper nisl. Donec sit amet enim at neque porttitor aliquet. Phasellus facilisis nulla eget placerat eleifend. Vestibulum non egestas eros, eget lobortis ipsum. Nulla rutrum massa eget enim aliquam, id porttitor erat luctus. Nunc sagittis quis eros eu sagittis. Pellentesque dictum, erat at pellentesque sollicitudin, justo augue pulvinar metus, quis rutrum est mi nec felis. Vestibulum efficitur mi lorem, at elementum purus tincidunt a. Aliquam finibus enim magna, vitae pellentesque erat faucibus at. Nulla mauris tellus, imperdiet id lobortis et, dignissim condimentum ipsum. Morbi nulla orci, varius at aliquet sed, facilisis id tortor. Donec ut urna nisi.\nAenean placerat luctus tortor vitae molestie. Nulla at aliquet nulla. Sed efficitur tellus orci, sed fringilla lectus laoreet eget. Vivamus maximus quam sit amet arcu dignissim, sed accumsan massa ullamcorper. Sed iaculis tincidunt feugiat. Nulla in est at nunc ultricies dictum ut vitae nunc. Aenean convallis vel diam at malesuada. Suspendisse arcu libero, vehicula tempus ultrices a, placerat sit amet tortor. Sed dictum id nulla commodo mattis. Aliquam mollis, nunc eu tristique faucibus, purus lacus tincidunt nulla, ac pretium lorem nunc ut enim. Curabitur eget mattis nisl, vitae sodales augue. Nam felis massa, bibendum sit amet nulla vel, vulputate rutrum lacus. Aenean convallis odio pharetra nulla mattis consequat.\nUt ut condimentum augue, nec eleifend nisl. Sed facilisis egestas odio ac pretium. Pellentesque consequat magna sed venenatis sagittis. Vivamus feugiat lobortis magna vitae accumsan. Pellentesque euismod malesuada hendrerit. Ut non mauris non arcu condimentum sodales vitae vitae dolor. Nullam dapibus, velit eget lacinia rutrum, ipsum justo malesuada odio, et lobortis sapien magna vel lacus. Nulla purus neque, hendrerit non malesuada eget, mattis vel erat. Suspendisse potenti.\nNullam dapibus cursus dolor sit amet consequat. Nulla facilisi. Curabitur vel nulla non magna lacinia tincidunt. Duis porttitor quam leo, et blandit velit efficitur ut. Etiam auctor tincidunt porttitor. Phasellus sed accumsan mi. Fusce ut erat dui. Suspendisse eu augue eget turpis condimentum finibus eu non lorem. Donec finibus eros eu ante condimentum, sed pharetra sapien sagittis. Phasellus non dolor ac ante mollis auctor nec et sapien. Pellentesque vulputate at nisi eu tincidunt. Vestibulum at dolor aliquam, hendrerit purus eu, eleifend massa. Morbi consectetur eros id tincidunt gravida. Fusce ut enim quis orci hendrerit lacinia sed vitae enim.\n\n\n\n\n[1] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[2] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[3] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[4] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “AFL++: Combining incremental steps of fuzzing research,” in 14th USENIX workshop on offensive technologies (WOOT 20), USENIX Association, Aug. 2020.\n\n\n[5] R. Anderson, “Why cryptosystems fail,” in Proceedings of the 1st ACM conference on Computer and communications security - CCS ’93, Fairfax, Virginia, United States: ACM Press, 1993, pp. 215–227. doi: 10.1145/168588.168615. Available: http://portal.acm.org/citation.cfm?doid=168588.168615\n\n\n[6] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[7] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[8] Y. Cheng et al., “Towards Reliable LLM-Driven Fuzz Testing: Vision and Road Ahead,” Mar. 02, 2025. doi: 10.48550/arXiv.2503.00795. Available: http://arxiv.org/abs/2503.00795\n\n\n[9] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[10] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[13] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[14] W. Gao, V.-T. Pham, D. Liu, O. Chang, T. Murray, and B. I. P. Rubinstein, “Beyond the Coverage Plateau: A Comprehensive Study of Fuzz Blockers (Registered Report),” in Proceedings of the 2nd International Fuzzing Workshop, in FUZZING 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 47–55. doi: 10.1145/3605157.3605177. Available: https://dl.acm.org/doi/10.1145/3605157.3605177\n\n\n[15] Y. Gao et al., “Retrieval-Augmented Generation for Large Language Models: A Survey,” Mar. 27, 2024. doi: 10.48550/arXiv.2312.10997. Available: http://arxiv.org/abs/2312.10997\n\n\n[16] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[17] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[18] L. Gazzola, D. Micucci, and L. Mariani, “Automatic Software Repair: A Survey,” IEEE Transactions on Software Engineering, vol. 45, no. 1, pp. 34–67, Jan. 2019, doi: 10.1109/TSE.2017.2755013. Available: https://ieeexplore.ieee.org/document/8089448/\n\n\n[19] D. Giannone, “Demystifying AI Agents: ReAct-Style Agents vs Agentic Workflows,” Feb. 09, 2025. Available: https://medium.com/@DanGiannone/demystifying-ai-agents-react-style-agents-vs-agentic-workflows-cedca7e26471\n\n\n[20] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[21] “Heartbleed Bug.” Available: https://heartbleed.com/\n\n\n[22] A. Herrera, H. Gunadi, S. Magrath, M. Norrish, M. Payer, and A. L. Hosking, “Seed selection for successful fuzzing,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, Virtual Denmark: ACM, Jul. 2021, pp. 230–243. doi: 10.1145/3460319.3464795. Available: https://dl.acm.org/doi/10.1145/3460319.3464795\n\n\n[23] L. Huang, P. Zhao, H. Chen, and L. Ma, “Large language models based fuzzing techniques: A survey,” 2024. Available: https://arxiv.org/abs/2402.00350\n\n\n[24] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[25] R. I. T. Jensen, V. Tawosi, and S. Alamir, “Software Vulnerability and Functionality Assessment using LLMs,” Mar. 13, 2024. doi: 10.48550/arXiv.2403.08429. Available: http://arxiv.org/abs/2403.08429\n\n\n[26] S. Y. Kim, Z. Fan, Y. Noller, and A. Roychoudhury, “Codexity: Secure AI-assisted Code Generation,” May 07, 2024. doi: 10.48550/arXiv.2405.03927. Available: http://arxiv.org/abs/2405.03927\n\n\n[27] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[28] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[29] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[30] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[31] D. Lazar, H. Chen, X. Wang, and N. Zeldovich, “Why does cryptographic software fail? A case study and open problems,” in Proceedings of 5th Asia-Pacific Workshop on Systems, in APSys ’14. New York, NY, USA: Association for Computing Machinery, Jun. 2014, pp. 1–7. doi: 10.1145/2637166.2637237. Available: https://doi.org/10.1145/2637166.2637237\n\n\n[32] H.-P. H. Lee et al., “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” 2025, Available: https://hankhplee.com/papers/genai_critical_thinking.pdf\n\n\n[33] C. Le Goues, S. Forrest, and W. Weimer, “Current challenges in automatic software repair,” Software Qual J, vol. 21, no. 3, pp. 421–443, Sep. 2013, doi: 10.1007/s11219-013-9208-0. Available: http://link.springer.com/10.1007/s11219-013-9208-0\n\n\n[34] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[35] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation.” Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[36] J. C. R. Licklider, “Man-Computer Symbiosis,” IRE Transactions on Human Factors in Electronics, vol. HFE–1, no. 1, pp. 4–11, Mar. 1960, doi: 10.1109/THFE2.1960.4503259. Available: https://ieeexplore.ieee.org/document/4503259\n\n\n[37] J. Liu, S. Lee, E. Losiouk, and M. Böhme, “Can LLM Generate Regression Tests for Software Commits?” Jan. 19, 2025. doi: 10.48550/arXiv.2501.11086. Available: http://arxiv.org/abs/2501.11086\n\n\n[38] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[39] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677\n\n\n[40] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[41] C. Meyer and J. Schwenk, “Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses,” 2013. Available: https://eprint.iacr.org/2013/049\n\n\n[42] N. Nethercote and J. Seward, “Valgrind: A framework for heavyweight dynamic binary instrumentation,” SIGPLAN Not., vol. 42, no. 6, pp. 89–100, Jun. 2007, doi: 10.1145/1273442.1250746. Available: https://doi.org/10.1145/1273442.1250746\n\n\n[43] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[44] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[45] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[46] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[47] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[48] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[49] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[50] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[51] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726\n\n\n[52] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[53] I. Tzachristas, “Creating an LLM-based AI-agent: A high-level methodology towards enhancing LLMs with APIs,” Dec. 21, 2024. doi: 10.48550/arXiv.2412.13233. Available: http://arxiv.org/abs/2412.13233\n\n\n[54] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[55] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[56] A. Zebaze, B. Sagot, and R. Bawden, “Tree of Problems: Improving structured problem solving with compositionality,” Oct. 09, 2024. doi: 10.48550/arXiv.2410.06634. Available: http://arxiv.org/abs/2410.06634\n\n\n[57] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: http://arxiv.org/abs/2307.12469\n\n\n[58] C. Zhang et al., “How Effective Are They? Exploring Large Language Model Based Fuzz Driver Generation,” in Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2024. New York, NY, USA: Association for Computing Machinery, Sep. 2024, pp. 1223–1235. doi: 10.1145/3650212.3680355. Available: https://dl.acm.org/doi/10.1145/3650212.3680355\n\n\n[59] K. Zhang et al., “Your Fix Is My Exploit: Enabling Comprehensive DL Library API Fuzzing with Large Language Models,” Jan. 08, 2025. doi: 10.48550/arXiv.2501.04312. Available: http://arxiv.org/abs/2501.04312\n\n\n[60] P. Y. Zhong, H. He, O. Khattab, C. Potts, M. Zaharia, and H. Miller, “A Guide to Large Language Model Abstractions,” Jan. 16, 2024. Available: https://www.twosigma.com/articles/a-guide-to-large-language-model-abstractions/\n\n\n[61] A. Zibaeirad and M. Vieira, “Reasoning with LLMs for Zero-Shot Vulnerability Detection,” Mar. 22, 2025. doi: 10.48550/arXiv.2503.17885. Available: http://arxiv.org/abs/2503.17885\n\n\n[62] “How to Prevent the next Heartbleed.” Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[63] “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier.” Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[64] “OSS-Fuzz Documentation.” Available: https://google.github.io/oss-fuzz/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Failed Techniques</span>"
    ]
  }
]