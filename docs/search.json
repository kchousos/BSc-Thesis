[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OverHAuL",
    "section": "",
    "text": "Preface\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\nEtiam maximus accumsan gravida. Maecenas at nunc dignissim, euismod enim ac, bibendum ipsum. Maecenas vehicula velit in nisl aliquet ultricies. Nam eget massa interdum, maximus arcu vel, pretium erat. Maecenas sit amet tempor purus, vitae aliquet nunc. Vivamus cursus urna velit, eleifend dictum magna laoreet ut. Duis eu erat mollis, blandit magna id, tincidunt ipsum. Integer massa nibh, commodo eu ex vel, venenatis efficitur ligula. Integer convallis lacus elit, maximus eleifend lacus ornare ac. Vestibulum scelerisque viverra urna id lacinia. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Aenean eget enim at diam bibendum tincidunt eu non purus. Nullam id magna ultrices, sodales metus viverra, tempus turpis.\n\n\n\nAcknowledgments\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\n\n\n\n\n\n\nCitation\n\n\n\nBibTeX citation:\n\n@thesis{chousos2025,\n  title = {LLM-Driven Fuzzing: Automatic Harness Generation for Crypto Libraries},\n  shorttitle = {LLM-Driven Fuzzing},\n  author = {Chousos, Konstantinos},\n  date = {2025-07},\n  institution = {{National and Kapodistrian University of Athens}},\n  location = {Athens, Greece},\n  url = {https://kchousos.github.io/BSc-Thesis/},\n  langid = {en, el}\n}\n\nFor attribution, please cite this work as:\n\n\nK. Chousos, “LLM-Driven Fuzzing: Automatic Harness Generation for Crypto Libraries,” Bachelor Thesis, National and Kapodistrian University of Athens, Athens, Greece, 2025. [Online]. Available: https://kchousos.github.io/BSc-Thesis/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/01-intro.html",
    "href": "chapters/01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Motivation\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.\nNunc ac dignissim magna. Vestibulum vitae egestas elit. Proin feugiat leo quis ante condimentum, eu ornare mauris feugiat. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Mauris cursus laoreet ex, dignissim bibendum est posuere iaculis. Suspendisse et maximus elit. In fringilla gravida ornare. Aenean id lectus pulvinar, sagittis felis nec, rutrum risus. Nam vel neque eu arcu blandit fringilla et in quam. Aliquam luctus est sit amet vestibulum eleifend. Phasellus elementum sagittis molestie. Proin tempor lorem arcu, at condimentum purus volutpat eu. Fusce et pellentesque ligula. Pellentesque id tellus at erat luctus fringilla. Suspendisse potenti.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-intro.html#preview-of-following-sections-rename",
    "href": "chapters/01-intro.html#preview-of-following-sections-rename",
    "title": "1  Introduction",
    "section": "1.2 Preview of following sections (rename)",
    "text": "1.2 Preview of following sections (rename)\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Duis sagittis posuere ligula sit amet lacinia. Duis dignissim pellentesque magna, rhoncus congue sapien finibus mollis. Ut eu sem laoreet, vehicula ipsum in, convallis erat. Vestibulum magna sem, blandit pulvinar augue sit amet, auctor malesuada sapien. Nullam faucibus leo eget eros hendrerit, non laoreet ipsum lacinia. Curabitur cursus diam elit, non tempus ante volutpat a. Quisque hendrerit blandit purus non fringilla. Integer sit amet elit viverra ante dapibus semper. Vestibulum viverra rutrum enim, at luctus enim posuere eu. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html",
    "href": "chapters/02-background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Fuzz Testing\nFuzzing is an automated software-testing technique in which a Program Under Test (PUT) is executed with (pseudo-)random inputs in the hope of exposing undefined behavior. When such behavior manifests as a crash, hang, or memory-safety violation, the corresponding input constitutes a test-case that reveals a bug and often a vulnerability [1]. In essence, fuzzing is a form of adversarial, penetration-style testing carried out by the defender before the adversary has an opportunity to do so. Interest in the technique surged after the publication of three practitioner-oriented books in 2007–2008 [2], [3], [4].\nHistorically, the term was coined by Miller et al. in 1990, who used “fuzz” to describe a program that “generates a stream of random characters to be consumed by a target program” [5]. More rigorously—borrowing Manes et al.’s definitions [1]:\nAn individual execution (or bounded set of executions) that meets this definition is a fuzzing run. When such runs are performed systematically and at scale to test whether the PUT violates a specified security policy, we speak of fuzz testing (or simply fuzzing in common parlance):\nA fuzzer engine orchestrates a fuzz campaign—one concrete instantiation of a fuzzer running against a single PUT under a particular policy:\nDuring each execution the bug oracle decides whether the observed behaviour constitutes a policy violation:\nIn practice, many oracles are based on runtime instrumentation such as fatal POSIX signals (e.g., SIGSEGV) or sanitizers like AddressSanitizer (ASan) [6], as used by LibFuzzer [7].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html#fuzz-testing",
    "href": "chapters/02-background.html#fuzz-testing",
    "title": "2  Background",
    "section": "",
    "text": "Definition 2.1 (Fuzzing) Fuzzing is the execution of a Program Under Test (PUT) using input(s) sampled from an input space (the fuzz input space) that protrudes the expected input space of the PUT.\n\n\n\nDefinition 2.2 (Fuzz Testing) Fuzz testing is the use of fuzzing to test whether a PUT violates a security policy.\n\n\n\nDefinition 2.3 (Fuzzer, Fuzzer Engine) A fuzzer is a program that performs fuzz testing on a PUT.\n\n\nDefinition 2.4 (Fuzz Campaign) A fuzz campaign is a specific execution of a fuzzer on a PUT with a specific security policy.\n\n\n\nDefinition 2.5 (Bug Oracle) A bug oracle is a component (often inside the fuzzer) that determines whether a given execution of the PUT violates a specific security policy.\n\n\n\n2.1.1 Seeds, Mutation, and the Seed-Selection Problem\nA campaign usually begins with one or more seeds—well-formed inputs that belong to the PUT’s expected input space. The fuzzer mutates these seeds to explore the larger fuzz input space:\n\nDefinition 2.6 (Seed) An input given to the PUT that is mutated by the fuzzer to produce new test cases. During a fuzz campaign (Definition 2.4) all seeds are stored in a seed pool or corpus.\n\nSelecting an initial corpus that leads to fast, wide code-coverage is non-trivial and has been studied as the seed-selection problem [8].\n\n\n2.1.2 Taxonomies of Fuzzing\nFuzzers are traditionally classified along two orthogonal dimensions. Namely, in regards to the knowledge of the PUT they have access to and the strategy used for the input generation.\n\n2.1.2.1 Knowledge of the PUT\n\nDefinition 2.7 (Black-box fuzzer) Operates solely on program binaries, with no knowledge of internal structure; input generation is guided only by external observations.\n\n\nDefinition 2.8 (Grey-box fuzzer) Gains limited insight—typically lightweight coverage metrics—via instrumentation, allowing more informed mutations while retaining scalability.\n\n\nDefinition 2.9 (White-box fuzzer) Has full source-level visibility and employs heavy program analysis (symbolic execution, constraint solving, etc.) to systematically enumerate paths.\n\n\n\n2.1.2.2 Test-case Generation Strategy\n\nDefinition 2.10 (Generational fuzzing) Produces inputs from a structural model (e.g., a BNF grammar [9]) or protocol description, ensuring that test-cases are syntactically valid yet semantically diverse.\n\n\nDefinition 2.11 (Mutational fuzzing) Starts from existing seeds and applies stochastic mutations (bit-flips, block insertions, splice operations). Coverage-guided mutational fuzzers such as AFL have proved especially effective.\n\nThese axes can be combined: e.g., AFL [10] is a grey-box, mutational fuzzer; Honggfuzz [11] with a grammar description becomes grey-box generational.\n\n\n\n2.1.3 Why Fuzz?\n\nThe purpose of fuzzing relies on the assumption that there are bugs within every program, which are waiting to be discovered. Therefore, a systematic approach should find them sooner or later.\n— OWASP Foundation [12]\n\nFuzz testing offers several tangible benefits:\n\nEarly vulnerability discovery. Detecting defects during development is cheaper and safer than addressing exploits in production.\nAdversary-parity. Performing the same randomised stress that attackers employ allows defenders to pre-empt zero-days.\nRobustness and correctness. Beyond security, fuzzing exposes logic errors and stability issues in complex, high-throughput APIs (e.g., decompressors) even when inputs are expected to be well-formed.\nRegression prevention. Re-running a corpus of crashing inputs as part of continuous integration ensures that fixed bugs remain fixed.\n\n\n\n2.1.4 Success Stories\nHeartbleed (CVE-2014-0160) [13], [14] arose from a buffer over-read in OpenSSL [15] introduced on 1 February 2012 and unnoticed until 1 April 2014. Post-mortem analyses showed that a simple fuzz campaign exercising the TLS heartbeat extension would have revealed the defect almost immediately [16].\nLikewise, the Shellshock (or Bashdoor) family of bugs in GNU Bash [17] enabled arbitrary command execution on many UNIX systems. While the initial flaw was fixed promptly, subsequent bug variants were discovered by Google’s Michał Zalewski using fuzzing in late 2014 [18].\nOn the defensive tooling side, the security tool named Mayhem—developed by the company of the same name—has since been adopted by the US Air Force, the Pentagon, Cloudflare, and numerous open-source communities. It has found and facilitated the remediation of thousands of previously unknown vulnerabilities [19].\nThese cases underscore the central thesis of fuzz testing: exhaustive manual review is infeasible, but scalable stochastic exploration reliably surfaces the critical few defects that matter most.\nqqqqqq\n\n\n2.1.5 How to Fuzz?\n\n\n2.1.6 Fuzzer engines\nC/C++: AFL [10] & AFL++ [10], ++. LibFuzzer [7]. [11].\nPython: Atheris [20].\n\nLibFuzzer is an in-process, coverage-guided, evolutionary fuzzing engine. LibFuzzer is linked with the library under test, and feeds fuzzed inputs to the library via a specific fuzzing entrypoint (fuzz target). - In-process, coverage-guided, mutation-based fuzzer.\n\nUsed to fuzz library functions. The programmer writes a fuzz target to test their implementation.\n\nDefinition 2.12 (Fuzz target) A function that accepts an array of bytes and does something interesting with these bytes using the API under test [7].\nAKA fuzz driver, fuzzer entry point, harness.\n\nFuzz target structure\n\nEntry point called repeatedly with mutated inputs.\nFeedback-driven: uses coverage to guide mutations.\nBest for libraries, not full programs.\n\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *Data, size_t Size) {\n  DoSomethingWithData(Data, Size);\n  return 0;\n}\nAn example of a fuzz target/harness can be seen in Listing 2.1 [7].\n\n\n\nListing 2.1: A simple function that does something interesting if it receives the input “HI!”.\n\n\ncat &lt;&lt; EOF &gt; test_fuzzer.cc\n#include &lt;stdint.h&gt;\n#include &lt;stddef.h&gt;\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t *data, size_t size) {\n  if (size &gt; 0 && data[0] == 'H')\n    if (size &gt; 1 && data[1] == 'I')\n       if (size &gt; 2 && data[2] == '!')\n       __builtin_trap();\n  return 0;\n}\nEOF\n# Build test_fuzzer.cc with asan and link against libFuzzer.\nclang++ -fsanitize=address,fuzzer test_fuzzer.cc\n# Run the fuzzer with no corpus.\n./a.out\n\n\n\n\nAmerican Fuzzy Lop (AFL) [10].\n\nInstrumented binaries for edge coverage.\nAdds more fuzzing strategies, better speed, and QEMU/Unicorn support.\nSuperseded by AFL++ [21].\n\n\nAFL fuzzes programs/binaries. The inputs are taken from the seeds_dir and their mutations.\n$ ./afl-fuzz -i seeds_dir -o output_dir -- /path/to/tested/program\n\nWorks on black-box or instrumented binaries.\nUses fork-server model for speed.\nSupports persistent mode, QEMU, and Unicorn modes.\n\nΜπορεί επίσης να χρησιμοποιηθεί για fuzzing βιβλιοθηκών κτλ., απλά αντί για LLVMFuzzerTestOneInput έχουμε την main.\nΜπορεί να χρησιμοποιήσει και LLVMFuzzerTestOneInput harnesses.\nOSS-Fuzz: 2016, after heartbleed.\n\n\n2.1.7 Difficulties\n\nRelatively unknown practice, especially compared to TDD\nUpfront cost of writing harnesses\nCost of examining and evaluating crashes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html#large-language-models-llms",
    "href": "chapters/02-background.html#large-language-models-llms",
    "title": "2  Background",
    "section": "2.2 Large Language Models (LLMs)",
    "text": "2.2 Large Language Models (LLMs)\nTransformers [22], 2017–2025. ChatGPT/OpenAI history & context. Claude, Llama (1–3) etc.\n\n2.2.1 What are they?\n[23] Transformers [22]\n\n\n2.2.2 Biggest GPTs\n\nClosed-source ChatGPT, Claude, Gemini [24], [25], [26]\nOpen-source (i.e. open-weights)\n\nLlama{1,..,3}, Deepseek [27], [28]\n\n\n2.2.3 Prompting\n\nZero-shot\nFew-shot [29]\nRAG [30]\nchain of thought [31]\ntree of thought [32]\nreAct [33]\n\nComparison, strengths weaknesses etc. [34].\n\n\n2.2.4 For coding\nLLM-assisted IDEs [35], [36], [37] Vibecoding [38]\n\n\n2.2.5 For fuzzing\nThey don't work\nThe above problems can be solved with NS AI\n[39]\n\n\n2.2.6 LLM Programming Libraries (?)\nLangchain & LangGraph, LlamaIndex [40], [41], [42]. DSPy [43].\nComparison, relevance to our usecase.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/02-background.html#neurosymbolic-ai",
    "href": "chapters/02-background.html#neurosymbolic-ai",
    "title": "2  Background",
    "section": "2.3 Neurosymbolic AI",
    "text": "2.3 Neurosymbolic AI\nTODO [44], [45], [46], [47], [48], [49].\n\n2.3.1 What is it?\n[46], [48] Neurosymbolic AI for attribution in LLMs [49]\n\n\n2.3.2 What does it solve?\n\n\n2.3.3 Its state\nrestatement of overarching goal, segue to section 2\n\n\n\n\n[1] V. J. M. Manes et al., “The Art, Science, and Engineering of Fuzzing: A Survey,” Apr. 07, 2019. doi: 10.48550/arXiv.1812.00140. Available: http://arxiv.org/abs/1812.00140\n\n\n[2] A. Takanen, J. DeMott, C. Miller, and A. Kettunen, Fuzzing for software security testing and quality assurance, Second edition. in Information security and privacy library. Boston London Norwood, MA: Artech House, 2018.\n\n\n[3] M. Sutton, A. Greene, and P. Amini, Fuzzing: Brute force vulnerabilty discovery. Upper Saddle River, NJ: Addison-Wesley, 2007.\n\n\n[4] N. Rathaus and G. Evron, Open source fuzzing tools. Burlington, MA: Syngress Pub, 2007.\n\n\n[5] B. P. Miller, L. Fredriksen, and B. So, “An empirical study of the reliability of UNIX utilities,” Commun. ACM, vol. 33, no. 12, pp. 32–44, Dec. 1990, doi: 10.1145/96267.96279. Available: https://dl.acm.org/doi/10.1145/96267.96279\n\n\n[6] K. Serebryany, D. Bruening, A. Potapenko, and D. Vyukov, “AddressSanitizer: A fast address sanity checker,” in 2012 USENIX annual technical conference (USENIX ATC 12), 2012, pp. 309–318. Available: https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany\n\n\n[7] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[8] A. Rebert et al., “Optimizing seed selection for fuzzing,” in Proceedings of the 23rd USENIX conference on Security Symposium, in SEC’14. USA: USENIX Association, Aug. 2014, pp. 861–875.\n\n\n[9] J. W. Backus, “The syntax and the semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference,” in ICIP Proceedings, 1959, pp. 125–132. Available: https://cir.nii.ac.jp/crid/1572824501224489728\n\n\n[10] “American fuzzy lop.” Available: https://lcamtuf.coredump.cx/afl/\n\n\n[11] “Google/honggfuzz.” Google, Jul. 10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[12] OWASP Foundation, “Fuzzing.” Available: https://owasp.org/www-community/Fuzzing\n\n\n[13] “Heartbleed Bug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[14] CVE Program, “CVE - CVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[15] The OpenSSL Project, “Openssl/openssl.” OpenSSL, Jul. 15, 2025. Available: https://github.com/openssl/openssl\n\n\n[16] D. Wheeler, “How to Prevent the next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[17] GNU Project, “Bash - GNU Project - Free Software Foundation.” Available: https://www.gnu.org/software/bash/\n\n\n[18] J. Saarinen, “Further flaws render Shellshock patch ineffective,” Sep. 29, 2014. Available: https://www.itnews.com.au/news/further-flaws-render-shellshock-patch-ineffective-396256\n\n\n[19] T. Simonite, “This Bot Hunts Software Bugs for the Pentagon,” Wired, Jun. 01, 2020. Available: https://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[20] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[21] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[22] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[23] H. Li, “Language models: Past, present, and future,” Commun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available: https://dl.acm.org/doi/10.1145/3490443\n\n\n[24] OpenAI, “ChatGPT,” 2025. Available: https://chatgpt.com\n\n\n[25] Anthropic, “Claude,” 2025. Available: https://claude.ai/new\n\n\n[26] Google, “‎Google Gemini,” 2025. Available: https://gemini.google.com\n\n\n[27] A. Grattafiori et al., “The Llama 3 Herd of Models,” Nov. 23, 2024. doi: 10.48550/arXiv.2407.21783. Available: http://arxiv.org/abs/2407.21783\n\n\n[28] DeepSeek-AI et al., “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,” Jan. 22, 2025. doi: 10.48550/arXiv.2501.12948. Available: http://arxiv.org/abs/2501.12948\n\n\n[29] T. B. Brown et al., “Language Models are Few-Shot Learners,” Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165. Available: http://arxiv.org/abs/2005.14165\n\n\n[30] P. Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” Apr. 12, 2021. doi: 10.48550/arXiv.2005.11401. Available: http://arxiv.org/abs/2005.11401\n\n\n[31] J. Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,” Jan. 10, 2023. doi: 10.48550/arXiv.2201.11903. Available: http://arxiv.org/abs/2201.11903\n\n\n[32] S. Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601. Available: http://arxiv.org/abs/2305.10601\n\n\n[33] S. Yao et al., “ReAct: Synergizing Reasoning and Acting in Language Models,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629. Available: http://arxiv.org/abs/2210.03629\n\n\n[34] P. Laban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost In Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120. Available: http://arxiv.org/abs/2505.06120\n\n\n[35] Anysphere, “Cursor - The AI Code Editor,” 2025. Available: https://cursor.com/\n\n\n[36] Microsoft, “GitHub Copilot · Your AI pair programmer,” 2025. Available: https://github.com/features/copilot\n\n\n[37] M. Chen et al., “Evaluating Large Language Models Trained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374. Available: http://arxiv.org/abs/2107.03374\n\n\n[38] A. Sarkar and I. Drosos, “Vibe coding: Programming through conversation with artificial intelligence,” Jun. 29, 2025. doi: 10.48550/arXiv.2506.23253. Available: http://arxiv.org/abs/2506.23253\n\n\n[39] N. Perry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users Write More Insecure Code with AI Assistants?” Dec. 18, 2023. doi: 10.48550/arXiv.2211.03622. Available: http://arxiv.org/abs/2211.03622\n\n\n[40] H. Chase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[41] “Langchain-ai/langgraph.” LangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[42] J. Liu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234. Available: https://github.com/jerryjliu/llama_index\n\n\n[43] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[44] D. Ganguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of Thought : Neurosymbolic Program Synthesis allows Robust and Interpretable Reasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270. Available: http://arxiv.org/abs/2409.17270\n\n\n[45] A. d’Avila Garcez and L. C. Lamb, “Neurosymbolic AI: The 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876. Available: http://arxiv.org/abs/2012.05876\n\n\n[46] M. Gaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI Systems: Consistency, Reliability, Explainability, and Safety,” Dec. 05, 2023. doi: 10.48550/arXiv.2312.06798. Available: http://arxiv.org/abs/2312.06798\n\n\n[47] G. Grov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V. Mavroeidis, “On the use of neurosymbolic AI for defending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996. Available: http://arxiv.org/abs/2408.04996\n\n\n[48] A. Sheth, K. Roy, and M. Gaur, “Neurosymbolic AI – Why, What, and How,” May 01, 2023. doi: 10.48550/arXiv.2305.00813. Available: http://arxiv.org/abs/2305.00813\n\n\n[49] D. Tilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic AI approach to Attribution in Large Language Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726. Available: http://arxiv.org/abs/2410.03726",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/03-related.html",
    "href": "chapters/03-related.html",
    "title": "3  Related work",
    "section": "",
    "text": "3.1 Previous Projects\nAutomated testing, automated fuzzing and automated harness creation have a long research history. Still, a lot of ground remains to be covered until true automation of these tasks is achieved. Until the introduction of transformers [1] and the 2020’s boom of commercial GPTs [2], automation regarding testing and fuzzing was mainly attempted through static and dynamic program analysis methods. These approaches are still utilized, but the fuzzing community has shifted almost entirely to researching the incorporation and employment of LLMs in the last half decade, in the name of automation [3], [4], [5], [6], [7], [8], [9], [10], [11], [12].",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/03-related.html#previous-projects",
    "href": "chapters/03-related.html#previous-projects",
    "title": "3  Related work",
    "section": "",
    "text": "3.1.1 KLEE\nKLEE [13] is a seminal and widely cited symbolic execution engine introduced in 2008 by Cadar et al. It was designed to automatically generate high-coverage test cases for programs written in C, using symbolic execution to systematically explore the control flow of a program. KLEE operates on the LLVM [14] bytecode representation of programs, allowing it to be applied to a wide range of C programs compiled to the LLVM intermediate representation.\nInstead of executing a program on concrete inputs, KLEE performs symbolic execution—that is, it runs the program on symbolic inputs, which represent all possible values simultaneously. At each conditional branch, KLEE explores both paths by forking the execution and accumulating path constraints (i.e., logical conditions on input variables) along each path. This enables it to traverse many feasible execution paths in the program, including corner cases that may be difficult to reach through random testing or manual test creation.\nWhen an execution path reaches a terminal state (e.g., a program exit, an assertion failure, or a segmentation fault), KLEE uses a constraint solver to compute concrete input values that satisfy the accumulated constraints for that path. These values form a test case that will deterministically drive the program down that specific path when executed concretely.\n\n\n3.1.2 IRIS\nIRIS [3] is a 2025 open-source neurosymbolic system for static vulnerability analysis. Given a codebase and a list of user-specified Common Weakness Enumerations (CWEs), it analyzes source code to identify paths that may correspond to known vulnerability classes. IRIS combines symbolic analysis—such as control- and data-flow reasoning—with neural models trained to generalize over code patterns. It outputs candidate vulnerable paths along with explanations and CWE references. The system operates on full repositories and supports extensible CWE definitions.\n\n\n3.1.3 FUDGE\nFUDGE [12] is a closed-source tool, made by Google, for automatic harness generation of C and C++ projects based on existing client code. It was used in conjunction with and in the improvement of Google’s OSS-Fuzz [15]. Being deployed inside Google’s infrastructure, FUDGE continuously examines Google’s internal code repository, searching for code that uses external libraries in a meaningful and “fuzzable” way (i.e. predominantly for parsing). If found, such code is sliced [16], per FUDGE, based on its Abstract Syntax Tree (AST) using LLVM’s Clang tool [14]. The above process results in a set of abstracted mostly-self-contained code snippets that make use of a library’s calls and/or API. These snippets are later synthesized into the body of a fuzz driver, with variables being replaced and the fuzz input being utilized. Each is then injected in an LLVMFuzzerTestOneInput function and finalized as a fuzzing harness. A building and evaluation phase follows for each harness, where they are executed and examined. Every passing harness along with its evaluation results is stored in FUDGE’s database, reachable to the user through a custom web-based UI.\n\n\n3.1.4 UTopia\nUTopia [8] (stylized UTopia) is another open-source automatic harness generation framework. Aside from the library code, It operates solely on user-provided unit tests since, according to [8], they are a resource of complete and correct API usage examples containing working library set-ups and tear-downs. Additionally, each of them are already close to a fuzz target, in the sense that they already examine a single and self-contained API usage pattern. Each generated harness follows the same data flow of the originating unit test. Static analysis is employed to figure out what fuzz input placement would yield the most results. It is also utilized in abstracting the tests away from the syntactical differences between testing frameworks, along with slicing and AST traversing using Clang.\n\n\n3.1.5 FuzzGen\nAnother project of Google is FuzzGen [11], this time open-source. Like FUDGE, it leverages existing client code of the target library to create fuzz targets for it. FuzzGen uses whole-system analysis, through which it creates an Abstract API Dependence Graph (A2DG). It uses the latter to automatically generate LibFuzzer-compatible harnesses. For FuzzGen to work, the user needs to provide both client code and/or tests for the API and the API library’s source code as well. FuzzGen uses the client code to infer the correct usage of the API and not its general structure, in contrast to FUDGE. FuzzGen’s workflow can be divided into three phases: 1. API usage inference. By consuming and analyzing client code and tests that concern the library under test, FuzzGen recognizes which functions belong to the library and learns its correct API usage patterns. This process is done with the help of Clang. To test if a function is actually a part of the library, a sample program is created that uses it. If the program compiles successfully, then the function is indeed a valid API call. 2. A2DG construction mechanism. For all the existing API calls, FuzzGen builds an A2DG to record the API usages and infers its intended structure. After completion, this directed graph contains all the valid API call sequences found in the client code corpus. It is built in a two-step process: First, many smaller A2DGs are created, one for each root function per client code snippet. Once such graphs have been created for all the available client code instances, they are combined to formulate the master A2DG. This graph can be seen as a template for correct usage of the library. 3. Fuzzer generator. Through the A2DG, a fuzzing harness is created. Contrary to FUDGE, FuzzGen does not create multiple “simple” harnesses but a single complex one with the goal of covering the whole of the A2DG. In other words, while FUDGE fuzzes a single API call at a time, FuzzGen’s result is a single harness that tries to fuzz the given library all at once through complex API usage.\n\n\n3.1.6 OSS-Fuzz\nOSS-Fuzz [15], [17] is a continuous, scalable and distributed cloud fuzzing solution for critical and prominent open-source projects. Developers of such software can submit their projects to OSS-Fuzz’s platform, where its harnesses are built and constantly executed. This results in multiple bug findings that are later disclosed to the primary developers and are later patched.\nOSS-Fuzz started operating in 2016, an initiative in response to the Heartbleed vulnerability [18], [19], [20]. Its hope is that through more extensive fuzzing such errors could be caught and corrected before having the chance to be exploited and thus disrupt the public digital infrastructure. So far, it has helped uncover over 10,000 security vulnerabilities and 36,000 bugs across more than 1,000 projects, significantly enhancing the quality and security of major software like Chrome, OpenSSL, and systemd.\nA project that’s part of OSS-Fuzz must have been configured as a ClusterFuzz [21] project. ClusterFuzz is the fuzzing infrastructure that OSS-Fuzz uses under the hood and depends on Google Cloud Platform services, although it can be hosted locally. Such an integration requires setting up a build pipeline, fuzzing jobs and expects a Google Developer account. Results are accessible through a web interface. ClusterFuzz, and by extension OSS-Fuzz, supports fuzzing through LibFuzzer, AFL++, Honggfuzz and FuzzTest—successor to Centipede— with the last two being Google projects [22], [23], [24], [25]. C, C++, Rust, Go, Python and Java/JVM projects are supported.\n\n\n3.1.7 OSS-Fuzz-Gen\nOSS-Fuzz-Gen (OFG) [6], [26] is Google’s current State-Of-The-Art (SOTA) project regarding automatic harness generation through LLMs. It’s purpose is to improve the fuzzing infrastructure of open-source projects that are already integrated into OSS-Fuzz. Given such a project, OSS-Fuzz-Gen uses its preexisting fuzzing harnesses and modifies them to produce new ones. Its architecture can be described as follows: 1. With an OSS-Fuzz project’s GitHub repository link, OSS-Fuzz-Gen iterates through a set of predefined build templates and generates potential build scripts for the project’s harnesses. 2. If any of them succeed they are once again compiled, this time through fuzz-introspector [27]. The latter constitutes a static analysis tool, with fuzzer developers specifically in mind. 3. Build results, old harness and fuzz-introspector report are included in a template-generated prompt, through which an LLM is called to generate a new harness. 4. The newly generated fuzz target is compiled and if it is done so successfully it begins execution inside OSS-Fuzz’s infrastructure.\nThis method proved meaningful, with code coverage in fuzz campaigns increasing thanks to the new generated fuzz drivers. In the case of [28], line coverage went from 38% to 69% without any manual interventions [26].\nIn 2024, OSS-Fuzz-Gen introduced an experimental feature for generating harnesses in previously unfuzzed projects [29]. The code for this feature resides in the experimental/from_scratch directory of the project’s GitHub repository [6], with the latest known working commit being 171aac2 and the latest overall commit being four months ago.\n\n\n3.1.8 AutoGen\nAutoGen [4] is a closed-source tool that generates new fuzzing harnesses, given only the library code and documentation. It works as following: The user specifies the function for which a harness is to be generated. AutoGen gathers information for this function—such as the function body, used header files, function calling examples—from the source code and documentation. Through specific prompt templates containing the above information, an LLM is tasked with generating a new fuzz driver, while another is tasked with generating a compilation command for said driver. If the compilation fails, both LLMs are called again to fix the problem, whether it was on the driver’s or command’s side. This loop iterates until a predefined maximum value or until a fuzz driver is successfully generated and compiled. If the latter is the case, it is then executed. If execution errors exist, the LLM responsible for the driver generation is used to correct them. If not, the pipeline has terminated and a new fuzz driver has been successfully generated.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/03-related.html#differences",
    "href": "chapters/03-related.html#differences",
    "title": "3  Related work",
    "section": "3.2 Differences",
    "text": "3.2 Differences\nOverHAuL differs, in some way, with each of the aforementioned works. Firstly, although KLEE and IRIS [3], [13] tackle the problem of automated testing and both IRIS and OverHAuL can be considered neurosymbolic AI tools, the similarities end there. None of them utilize LLMs the same way we do—with KLEE not utilizing them by default, as it precedes them chronologically—and neither are automating any part of the fuzzing process.\nWhen it comes to FUDGE, FuzzGen and UTopia [8], [11], [12], all three depend on and demand existing client code and/or unit tests. On the other hand, OverHAuL requires only the bare minimum: the library code itself. Another point of difference is that in contrast with OverHAuL, these tools operate in a linear fashion. No feedback is produced or used in any step and any point failure results in the termination of the entire run.\nOverHAuL challenges a common principle of these tools, stated explicitly in FUDGE’s paper [12]: “Choosing a suitable fuzz target (still) requires a human”. OverHAuL chooses to let the LLM, instead of the user, explore the available functions and choose one to target in its fuzz driver.\nOSS-Fuzz-Gen [6] can be considered a close counterpart of OverHAuL, and in some ways it is. A lot of inspiration was gathered from it, like for example the inclusion of static analysis and its usage in informing the LLM. Yet, OSS-Fuzz-Gen has a number of disadvantages that make it in some cases an inferior option. For one, OFG is tightly coupled with the OSS-Fuzz platform [15], which even on its own creates a plethora of issues for the common developer. To integrate their project into OSS-Fuzz, they would need to: Transform it into a ClusterFuzz project [21] and take time to write harnesses for it. Even if these prerequisites are carried out, it probably would not be enough. Per OSS-Fuzz’s documentation [17]: “To be accepted to OSS-Fuzz, an open-source project must have a significant user base and/or be critical to the global IT infrastructure”. This means that OSS-Fuzz is a viable option only for a small minority of open-source developers and maintainers. One countermeasure of the above shortcoming would be for a developer to run OSS-Fuzz-Gen locally. This unfortunately proves to be an arduous task. As it is not meant to be used standalone, OFG is not packaged in the form of a self-contained application. This makes it hard to setup and difficult to use interactively. Like in the case of FUDGE, OFG’s actions are performed linearly. No feedback is utilized nor is there graceful error handling in the case of a step’s failure. Even in the case of the experimental feature for bootstrapping unfuzzed projects, OFG’s performance varies heavily. During experimentation, a lot of generated harnesses were still wrapped either in Markdown backticks or &lt;code&gt; tags, or were accompanied with explanations inside the generated .c source file. Even if code was formatted correctly, in many cases it missed necessary headers for compilation or used undeclared functions.\nLastly, the closest counterpart to OverHAuL is AutoGen [4]. Their similarity stands in the implementation of a feedback loop between LLM and generated harness. However, most other implementation decisions remain distinct. One difference regards the fuzzed function. While AutoGen requires a target function to be specified by the user in which it narrows during its whole run, OverHAuL delegates this to the LLM, letting it explore the codebase and decide by itself the best candidate. Another difference lies in the need—and the lack of—of documentation. While AutoGen requires it to gather information for the given function, OverHAuL leans into the role of a developer by reading the related code and comments and thus avoiding any mismatches between documentation and code. Finally, the LLMs’ input is built based on predefined prompt templates, a technique also present in OSS-Fuzz-Gen. OverHAuL operates one abstraction level higher, leveraging DSPy [30] for programming instead of prompting the LLMs used.\nIn conclusion, OverHAuL constitutes an open-source tool that offers new functionality by offering a straightforward installation process, packaged as a self-contained Python package with minimal external dependencies. It also introduces novel approaches compared to previous work by\n\nImplementing a feedback mechanism between harness generation, compilation, and evaluation phases,\nUsing autonomous ReAct agents capable of codebase exploration,\nLeveraging a vector store for code consumption and retrieval.\n\nTODO να συμπεριλάβω και τα:\n\n3.2.1 IntelliGen [[20250711141156]]\nSAMPLE\nIntelliGen: Automatic Fuzz Driver Synthesis Based on Vulnerability Heuristics Zhang et al. (2021) present IntelliGen, a system for automatically synthesizing fuzz drivers by statically identifying potentially vulnerable entry-point functions within C projects. Implemented using LLVM, IntelliGen focuses on improving fuzzing efficiency by targeting code more likely to contain memory safety issues, rather than exhaustively fuzzing all available functions.\nThe system comprises two main components: the Entry Function Locator and the Fuzz Driver Synthesizer. The Entry Function Locator analyzes the project’s abstract syntax tree (AST) and classifies functions based on heuristics that indicate vulnerability. These include pointer dereferencing, calls to memory-related functions (e.g., memcpy, memset), and invocation of other internal functions. Functions that score highly on these metrics are prioritized for fuzz driver generation. The guiding insight is that entry points with fewer argument checks and more direct memory operations expose more useful program logic for fuzz testing.\nThe Fuzz Driver Synthesizer then generates harnesses for these entry points. For each target function, it synthesizes a LLVMFuzzerTestOneInput function that invokes the target with arguments derived from the fuzzer input. This process involves inferring argument types from the source code and ensuring that runtime behavior does not violate memory safety—thus avoiding invalid inputs that would cause crashes unrelated to genuine bugs.\nIntelliGen stands out by integrating static vulnerability estimation into the driver generation pipeline. Compared to prior tools like FuzzGen and FUDGE, it uses a more targeted, heuristic-based selection of functions, increasing the likelihood that fuzzing will exercise meaningful and vulnerable code paths.\n\n\n3.2.2 CKGFuzzer [[20250711203054]]\nSAMPLE\nCKGFuzzer is a fuzzing framework designed to automate the generation of effective fuzz drivers for C/C++ libraries by leveraging static analysis and large language models. Its workflow begins by parsing the target project along with any associated library APIs to construct a code knowledge graph. This involves two primary steps: first, parsing the abstract syntax tree (AST), and second, performing interprocedural program analysis. Through this process, CKGFuzzer extracts essential program elements such as data structures, function signatures, function implementations, and call relationships.\nUsing the knowledge graph, CKGFuzzer then identifies and queries meaningful API combinations, focusing on those that are either frequently invoked together or exhibit functional similarity. It generates candidate fuzz drivers for these combinations and attempts to compile them. Any compilation errors encountered during this phase are automatically repaired using heuristics and domain knowledge. A dynamically updated knowledge base, constructed from prior library usage patterns, guides both the generation and repair processes.\nOnce the drivers are successfully compiled, CKGFuzzer executes them while monitoring code coverage at the file level. It uses coverage feedback to iteratively mutate underperforming API combinations, refining them until new execution paths are discovered or a preset mutation budget is exhausted.\nFinally, any crashes triggered during fuzzing are subjected to a reasoning process based on chain-of-thought prompting. To help determine their severity and root cause, CKGFuzzer consults an LLM-generated knowledge base containing real-world examples of vulnerabilities mapped to known Common Weakness Enumeration (CWE) entries.\n\n\n3.2.3 PromptFuzz [[20250713225436]]\nSAMPLE\nLyu et al. (2024) introduce PromptFuzz [31], a system for automatically generating fuzz drivers using LLMs, with a novel focus on prompt mutation to improve coverage. The system is implemented in Rust and targets C libraries, aiming to explore more of the API surface with each iteration.\nThe workflow begins with the random selection of API functions, extracted from header file declarations. These functions are used to construct initial prompts that instruct the LLM to generate a simple program utilizing the API. Each generated program is compiled, executed, and monitored for code coverage. Programs that fail to compile or violate runtime checks (e.g., sanitizers) are discarded.\nA key innovation in PromptFuzz is coverage-guided prompt mutation. Instead of mutating generated code directly, PromptFuzz mutates the LLM prompts—selecting new combinations of API functions to target unexplored code paths. This process is guided by a power scheduling strategy that prioritizes underused or promising API functions based on feedback from previous runs.\nOnce an effective program is produced, it is transformed into a fuzz driver by replacing constants and arguments with variables derived from the fuzzer input. Multiple such drivers are embedded into a single harness, where the input determines which program variant to execute, typically via a case-switch construct.\nOverall, PromptFuzz demonstrates that prompt-level mutation enables more effective exploration of complex APIs and achieves better coverage than direct code mutations, offering a compelling direction for LLM-based automated fuzzing systems.\n\n\n\n\n[1] A. Vaswani et al., “Attention Is All You Need,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762. Available: http://arxiv.org/abs/1706.03762\n\n\n[2] OpenAI, “ChatGPT,” 2025. Available: https://chatgpt.com\n\n\n[3] Z. Li, S. Dutta, and M. Naik, “IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238. Available: http://arxiv.org/abs/2405.17238\n\n\n[4] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272\n\n\n[5] D. Wang, G. Zhou, L. Chen, D. Li, and Y. Miao, “ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,” Sep. 01, 2024. doi: 10.1145/3658644.3690231. Available: http://arxiv.org/abs/2409.00922\n\n\n[6] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[7] H. Green and T. Avgerinos, “GraphFuzz: Library API fuzzing with lifetime-aware dataflow graphs,” in Proceedings of the 44th International Conference on Software Engineering, Pittsburgh Pennsylvania: ACM, May 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228. Available: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[8] B. Jeong et al., “UTopia: Automatic Generation of Fuzz Driver using Unit Tests,” in 2023 IEEE Symposium on Security and Privacy (SP), May 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394. Available: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[9] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang, “Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014. Available: http://arxiv.org/abs/2304.02014\n\n\n[10] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, in ISSTA 2023. New York, NY, USA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi: 10.1145/3597926.3598067. Available: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[11] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[12] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[13] C. Cadar, D. Dunbar, and D. Engler, “KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs,” presented at the USENIX Symposium on Operating Systems Design and Implementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[14] “The LLVM Compiler Infrastructure Project,” 2025. Available: https://llvm.org/\n\n\n[15] A. Arya, O. Chang, J. Metzman, K. Serebryany, and D. Liu, “OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[16] N. Sasirekha, A. Edwin Robert, and M. Hemalatha, “Program Slicing Techniques and its Applications,” IJSEA, vol. 2, no. 3, pp. 50–64, Jul. 2011, doi: 10.5121/ijsea.2011.2304. Available: http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf\n\n\n[17] “OSS-Fuzz Documentation,” 2025. Available: https://google.github.io/oss-fuzz/\n\n\n[18] CVE Program, “CVE - CVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[19] D. Wheeler, “How to Prevent the next Heartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[20] “Heartbleed Bug,” Mar. 07, 2025. Available: https://heartbleed.com/\n\n\n[21] “Google/clusterfuzz.” Google, Apr. 09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[22] “libFuzzer – a library for coverage-guided fuzz testing. — LLVM 21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[23] “Google/fuzztest.” Google, Jul. 10, 2025. Available: https://github.com/google/fuzztest\n\n\n[24] “Google/honggfuzz.” Google, Jul. 10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[25] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[26] D. Liu, J. Metzman, O. Chang, and G. O. S. S. Team, “AI-Powered Fuzzing: Breaking the Bug Hunting Barrier,” Aug. 16, 2023. Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[27] “Ossf/fuzz-introspector.” Open Source Security Foundation (OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[28] L. Thomason, “Leethomason/Tinyxml2.” Jul. 10, 2025. Available: https://github.com/leethomason/tinyxml2\n\n\n[29] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed projects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[30] O. Khattab et al., “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714. Available: http://arxiv.org/abs/2310.03714\n\n\n[31] Y. Lyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing for Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677. Available: http://arxiv.org/abs/2312.17677",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Related work</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html",
    "href": "chapters/04-overhaul.html",
    "title": "4  OverHAuL",
    "section": "",
    "text": "4.1 Architecture",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/04-overhaul.html#architecture",
    "href": "chapters/04-overhaul.html#architecture",
    "title": "4  OverHAuL",
    "section": "",
    "text": "System diagram\nMain Library Architecture/Structure\nLLM usage\n\nPrompting techniques used (callback to Section 2.2.3).\n\nStatic analysis\nCode localization(?)\nFuzzers\nGitHub Workflow/Usage\n“Ieration budget”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>OverHAuL</span>"
    ]
  },
  {
    "objectID": "chapters/05-results.html",
    "href": "chapters/05-results.html",
    "title": "5  Evaluation",
    "section": "",
    "text": "5.1 Benchmarks\nResults from integration with 10/100 open-source C/C++ projects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-results.html#performance",
    "href": "chapters/05-results.html#performance",
    "title": "5  Evaluation",
    "section": "5.2 Performance",
    "text": "5.2 Performance",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-results.html#issues",
    "href": "chapters/05-results.html#issues",
    "title": "5  Evaluation",
    "section": "5.3 Issues",
    "text": "5.3 Issues",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/05-results.html#future-work",
    "href": "chapters/05-results.html#future-work",
    "title": "5  Evaluation",
    "section": "5.4 Future Work",
    "text": "5.4 Future Work\n\n5.4.1 Technical Future Work\n\n\n5.4.2 Architectural Future Work/Extensions\n\nBuild system\nMore (static) analysis tolls integrations\nGeneral localization problem",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "chapters/06-future.html",
    "href": "chapters/06-future.html",
    "title": "6  Future Work",
    "section": "",
    "text": "6.1 Enhancements to Core Features\nThe prototype implementation of OverHAuL offers a compelling demonstration of its potential to automate the fuzzing process for open-source libraries, providing tangible benefits to developers and maintainers alike. This initial version successfully validates the core design principles underpinning OverHAuL, showcasing its ability to streamline and enhance the software testing workflow through automated generation of fuzz drivers using large language models. Nevertheless, while these foundational capabilities lay a solid groundwork, numerous avenues exist for further expansion, refinement, and rigorous evaluation to fully realize the tool’s potential and adapt to evolving challenges in software quality assurance.\nEnhancing OverHAuL’s core functionality represents a primary direction for future development. First, expanding support to encompass a wider array of build systems commonly employed in C and C++ projects—such as GNU Make, CMake, Meson, and Ninja [1], [2], [3], [4]—would significantly broaden the scope of libraries amenable to automated fuzzing using OverHAuL. This advancement would enable OverHAuL to scale effectively and be applied to larger, more complex codebases, thereby increasing its practical utility and impact.\nSecond, integrating additional fuzzing engines beyond LibFuzzer stands out as a strategic enhancement. Incorporation of widely adopted fuzzers like AFL++ [5] could diversify the fuzzing strategies available to OverHAuL, while exploring more experimental tools such as GraphFuzz [6] may pioneer specialized approaches for certain code patterns or architectures. Multi-engine support would also facilitate extending language coverage, for instance by incorporating fuzzers tailored to other programming ecosystems—for example, Google’s Atheris for Python projects [7]. Such versatility would position OverHAuL as a more universal fuzzing automation platform.\nThird, the evaluation component of OverHAuL presents an opportunity for refinement through more sophisticated analysis techniques. Beyond the current criteria, incorporating dynamic metrics such as differential code coverage tracking between generated fuzz harnesses would yield deeper insights into test quality and coverage completeness. This quantitative evaluation could guide iterative improvements in fuzz driver generation and overall testing effectiveness.\nFinally, OverHAuL’s methodology could be extended to leverage existing client codebases and unit tests in addition to the library source code itself, resources that for now OverHAuL leaves untapped. Inspired by approaches like those found in FUDGE and FuzzGen [8], [9], this enhancement would enable the tool to exploit programmer-written usage scenarios as seeds or contexts, potentially generating more meaningful and targeted fuzz inputs. Incorporating these richer information sources would likely improve the efficacy of fuzzing campaigns and uncover subtler bugs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/06-future.html#experimentation-with-large-language-models-and-data-representation",
    "href": "chapters/06-future.html#experimentation-with-large-language-models-and-data-representation",
    "title": "6  Future Work",
    "section": "6.2 Experimentation with Large Language Models and Data Representation",
    "text": "6.2 Experimentation with Large Language Models and Data Representation\nOverHAuL’s reliance on large language models (LLMs) invites comprehensive experimentation with different providers and architectures to assess their comparative strengths and limitations. Conducting empirical evaluations across leading models—such as OpenAI’s o1 and o3 families and Anthropic’s Claude Opus 4—will provide valuable insights into their capabilities, cost-efficiency, and suitability for fuzz driver synthesis. Additionally, specialized code-focused LLMs, including generative and fill-in models like Codex-1 and CodeGen [10], [11], [12], merit exploration due to their targeted optimization for source code generation and understanding.\nAnother dimension worthy of investigation concerns the granularity of code chunking employed during the given project’s code processing stage. Whereas the current approach partitions code at the function level, experimenting with more nuanced segmentation strategies—such as splitting per step inside a function, as a finer-grained technique—could improve the semantic coherence of stored representations and enhance retrieval relevance during fuzz driver generation. This line of inquiry has the potential to optimize model input preparation and ultimately improve output quality.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/06-future.html#comprehensive-evaluation-and-benchmarking",
    "href": "chapters/06-future.html#comprehensive-evaluation-and-benchmarking",
    "title": "6  Future Work",
    "section": "6.3 Comprehensive Evaluation and Benchmarking",
    "text": "6.3 Comprehensive Evaluation and Benchmarking\nTo thoroughly establish OverHAuL’s effectiveness, extensive large-scale evaluation beyond the initial 10-project corpus is imperative. Applying the tool to repositories indexed in the clib package manager [13], which encompasses hundreds of C libraries, would test scalability and robustness across diverse real-world settings. Such a broad benchmark would also enable systematic comparisons against state-of-the-art automated fuzzing frameworks like OSS-Fuzz-Gen and AutoGen, elucidating OverHAuL’s relative strengths and identifying areas for improvement [14], [15].\nComplementing broad benchmarking, detailed ablation studies dissecting the contributions of individual pipeline components and algorithmic choices will yield critical insights into what drives OverHAuL’s performance. Understanding the impact of each module will guide targeted optimizations and support evidence-based design decisions.\nFurthermore, an economic analysis exploring resource consumption—such as API token usage and associated monetary costs—relative to fuzzing effectiveness would be valuable for assessing the practical viability of integrating LLM-based fuzz driver generation into continuous integration processes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/06-future.html#practical-deployment-and-community-engagement",
    "href": "chapters/06-future.html#practical-deployment-and-community-engagement",
    "title": "6  Future Work",
    "section": "6.4 Practical Deployment and Community Engagement",
    "text": "6.4 Practical Deployment and Community Engagement\nFrom a usability perspective, embedding OverHAuL within a GitHub Actions workflow represents a practical and impactful enhancement, enabling seamless integration with developers’ existing toolchains and continuous integration pipelines. This would promote wider adoption by reducing barriers to entry and fostering real-time feedback during code development cycles.\nAdditionally, establishing a mechanism to generate and submit automated pull requests (PRs) to the maintainers of fuzzed libraries—highlighting detected bugs and proposing patches—would not only validate OverHAuL’s findings but also contribute tangible improvements to open-source software quality. This collaborative feedback loop epitomizes the symbiosis between automated testing tools and the open-source community. As an initial step, developing targeted PRs for the projects where bugs were discovered during OverHAuL’s development would help facilitate practical follow-up and improvements.\n\n\n\n\n[1] A. Cedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf, “CMake - Upgrade Your Software Build System.” 2000. Available: https://cmake.org/\n\n\n[2] S. I. Feldman, “Make — a program for maintaining computer programs,” Software: Practice and Experience, vol. 9, no. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402. Available: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[3] E. Martin, “Ninja-build/ninja.” ninja-build, Jul. 14, 2025. Available: https://github.com/ninja-build/ninja\n\n\n[4] J. Pakkanen, “Mesonbuild/meson.” The Meson Build System, Jul. 14, 2025. Available: https://github.com/mesonbuild/meson\n\n\n[5] M. Heuse, H. Eißfeldt, A. Fioraldi, and D. Maier, “AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[6] H. Green and T. Avgerinos, “GraphFuzz: Library API fuzzing with lifetime-aware dataflow graphs,” in Proceedings of the 44th International Conference on Software Engineering, Pittsburgh Pennsylvania: ACM, May 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228. Available: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[7] “Google/atheris.” Google, Apr. 09, 2025. Available: https://github.com/google/atheris\n\n\n[8] K. Ispoglou, D. Austin, V. Mohan, and M. Payer, “FuzzGen: Automatic fuzzer generation,” in 29th USENIX Security Symposium (USENIX Security 20), 2020, pp. 2271–2287. Available: https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[9] D. Babić et al., “FUDGE: Fuzz driver generation at scale,” in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Tallinn Estonia: ACM, Aug. 2019, pp. 975–985. doi: 10.1145/3338906.3340456. Available: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[10] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou, “CodeGen2: Lessons for training llms on programming and natural languages,” ICLR, 2023.\n\n\n[11] E. Nijkamp et al., “CodeGen: An open large language model for code with multi-turn program synthesis,” ICLR, 2023.\n\n\n[12] OpenAI, “Introducing Codex,” May 16, 2025. Available: https://openai.com/index/introducing-codex/\n\n\n[13] “Clib Packages,” 2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[14] D. Liu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target generation.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[15] Y. Sun, “Automated Generation and Compilation of Fuzz Driver Based on Large Language Models,” in Proceedings of the 2024 9th International Conference on Cyber Security and Information Engineering, in ICCSIE ’24. New York, NY, USA: Association for Computing Machinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272. Available: https://doi.org/10.1145/3689236.3689272",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Future Work</span>"
    ]
  },
  {
    "objectID": "chapters/07-discussion.html",
    "href": "chapters/07-discussion.html",
    "title": "7  Discussion",
    "section": "",
    "text": "more powerful llms -&gt; better results\nopen source libraries might have been in the training data results for closed source libraries could be worse this could be mitigated with llm fine-tuning",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Discussion</span>"
    ]
  },
  {
    "objectID": "chapters/08-conclusion.html",
    "href": "chapters/08-conclusion.html",
    "title": "8  Conclusion",
    "section": "",
    "text": "Recap",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/refs.html",
    "href": "chapters/refs.html",
    "title": "Bibliography",
    "section": "",
    "text": "[1] “American fuzzy lop.” Available:\nhttps://lcamtuf.coredump.cx/afl/\n\n\n[2] M.\nHeuse, H. Eißfeldt, A. Fioraldi, and D. Maier,\n“AFL++.” Jan. 2022. Available: https://github.com/AFLplusplus/AFLplusplus\n\n\n[3] “Google/atheris.” Google, Apr. 09,\n2025. Available: https://github.com/google/atheris\n\n\n[4] J.\nW. Backus, “The syntax and the semantics of the proposed\ninternational algebraic language of the Zurich ACM-GAMM\nConference,” in ICIP Proceedings,\n1959, pp. 125–132. Available: https://cir.nii.ac.jp/crid/1572824501224489728\n\n\n[5] GNU\nProject, “Bash - GNU Project - Free Software\nFoundation.” Available: https://www.gnu.org/software/bash/\n\n\n[6] T.\nB. Brown et al., “Language Models are\nFew-Shot Learners,” Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165.\nAvailable: http://arxiv.org/abs/2005.14165\n\n\n[7] A.\nCedilnik, B. Hoffman, B. King, K. Martin, and A. Neundorf,\n“CMake - Upgrade Your Software Build\nSystem.” 2000. Available: https://cmake.org/\n\n\n[8] J.\nWei et al., “Chain-of-Thought Prompting Elicits\nReasoning in Large Language Models,” Jan. 10,\n2023. doi: 10.48550/arXiv.2201.11903.\nAvailable: http://arxiv.org/abs/2201.11903\n\n\n[9] OpenAI, “ChatGPT,”\n2025. Available: https://chatgpt.com\n\n\n[10] M.\nChen et al., “Evaluating Large Language Models\nTrained on Code,” Jul. 14, 2021. doi: 10.48550/arXiv.2107.03374.\nAvailable: http://arxiv.org/abs/2107.03374\n\n\n[11] Anthropic, “Claude,” 2025.\nAvailable: https://claude.ai/new\n\n\n[12] “Clib Packages,”\n2025. Available: https://github.com/clibs/clib/wiki/Packages\n\n\n[13] “Google/clusterfuzz.” Google, Apr.\n09, 2025. Available: https://github.com/google/clusterfuzz\n\n\n[14] Anysphere, “Cursor - The AI Code\nEditor,” 2025. Available: https://cursor.com/\n\n\n[15] DeepSeek-AI et al.,\n“DeepSeek-R1: Incentivizing Reasoning\nCapability in LLMs via Reinforcement\nLearning,” Jan. 22, 2025. doi: 10.48550/arXiv.2501.12948.\nAvailable: http://arxiv.org/abs/2501.12948\n\n\n[16] O.\nKhattab et al., “DSPy: Compiling\nDeclarative Language Model Calls into Self-Improving\nPipelines,” Oct. 05, 2023. doi: 10.48550/arXiv.2310.03714.\nAvailable: http://arxiv.org/abs/2310.03714\n\n\n[17] S.\nI. Feldman, “Make — a program for maintaining computer\nprograms,” Software: Practice and Experience, vol. 9,\nno. 4, pp. 255–265, 1979, doi: 10.1002/spe.4380090402.\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.4380090402\n\n\n[18] D.\nBabić et al., “FUDGE: Fuzz driver generation\nat scale,” in Proceedings of the 2019 27th ACM Joint\nMeeting on European Software Engineering Conference\nand Symposium on the Foundations of\nSoftware Engineering, Tallinn Estonia: ACM, Aug. 2019,\npp. 975–985. doi: 10.1145/3338906.3340456.\nAvailable: https://dl.acm.org/doi/10.1145/3338906.3340456\n\n\n[19] “Ossf/fuzz-introspector.” Open\nSource Security Foundation (OpenSSF), Jun. 30, 2025. Available: https://github.com/ossf/fuzz-introspector\n\n\n[20] K.\nIspoglou, D. Austin, V. Mohan, and M. Payer,\n“FuzzGen: Automatic fuzzer\ngeneration,” in 29th USENIX Security Symposium\n(USENIX Security 20), 2020, pp. 2271–2287. Available:\nhttps://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou\n\n\n[21] Y.\nDeng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,\n“Large Language Models are Edge-Case\nFuzzers: Testing Deep Learning Libraries via\nFuzzGPT,” Apr. 04, 2023. doi: 10.48550/arXiv.2304.02014.\nAvailable: http://arxiv.org/abs/2304.02014\n\n\n[22] “Google/fuzztest.” Google, Jul.\n10, 2025. Available: https://github.com/google/fuzztest\n\n\n[23] D.\nGanguly, S. Iyengar, V. Chaudhary, and S. Kalyanaraman, “Proof of\nThought : Neurosymbolic Program Synthesis\nallows Robust and Interpretable\nReasoning,” Sep. 25, 2024. doi: 10.48550/arXiv.2409.17270.\nAvailable: http://arxiv.org/abs/2409.17270\n\n\n[24] A.\nd’Avila Garcez and L. C. Lamb, “Neurosymbolic AI:\nThe 3rd Wave,” Dec. 16, 2020. doi: 10.48550/arXiv.2012.05876.\nAvailable: http://arxiv.org/abs/2012.05876\n\n\n[25] M.\nGaur and A. Sheth, “Building Trustworthy NeuroSymbolic AI\nSystems: Consistency, Reliability,\nExplainability, and Safety,” Dec. 05,\n2023. doi: 10.48550/arXiv.2312.06798.\nAvailable: http://arxiv.org/abs/2312.06798\n\n\n[26] Google, “‎Google\nGemini,” 2025. Available: https://gemini.google.com\n\n\n[27] Microsoft, “GitHub Copilot ·\nYour AI pair programmer,” 2025. Available: https://github.com/features/copilot\n\n\n[28] A.\nGrattafiori et al., “The Llama 3\nHerd of Models,” Nov. 23, 2024. doi: 10.48550/arXiv.2407.21783.\nAvailable: http://arxiv.org/abs/2407.21783\n\n\n[29] H.\nGreen and T. Avgerinos, “GraphFuzz: Library\nAPI fuzzing with lifetime-aware dataflow graphs,” in\nProceedings of the 44th International Conference on\nSoftware Engineering, Pittsburgh Pennsylvania: ACM,\nMay 2022, pp. 1070–1081. doi: 10.1145/3510003.3510228.\nAvailable: https://dl.acm.org/doi/10.1145/3510003.3510228\n\n\n[30] G.\nGrov, J. Halvorsen, M. W. Eckhoff, B. J. Hansen, M. Eian, and V.\nMavroeidis, “On the use of neurosymbolic AI for\ndefending against cyber attacks,” Aug. 09, 2024. doi: 10.48550/arXiv.2408.04996.\nAvailable: http://arxiv.org/abs/2408.04996\n\n\n[31] “Heartbleed Bug,”\nMar. 07, 2025. Available: https://heartbleed.com/\n\n\n[32] CVE Program, “CVE -\nCVE-2014-0160,” 2014. Available: https://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2014-0160\n\n\n[33] “Google/honggfuzz.” Google, Jul.\n10, 2025. Available: https://github.com/google/honggfuzz\n\n\n[34] Z.\nLi, S. Dutta, and M. Naik, “IRIS: LLM-Assisted\nStatic Analysis for Detecting Security\nVulnerabilities,” Apr. 06, 2025. doi: 10.48550/arXiv.2405.17238.\nAvailable: http://arxiv.org/abs/2405.17238\n\n\n[35] C.\nCadar, D. Dunbar, and D. Engler, “KLEE:\nUnassisted and Automatic Generation of\nHigh-Coverage Tests for Complex Systems\nPrograms,” presented at the USENIX Symposium\non Operating Systems Design and\nImplementation, Dec. 2008. Available: https://www.semanticscholar.org/paper/KLEE%3A-Unassisted-and-Automatic-Generation-of-Tests-Cadar-Dunbar/0b93657965e506dfbd56fbc1c1d4b9666b1d01c8\n\n\n[36] P.\nLaban, H. Hayashi, Y. Zhou, and J. Neville, “LLMs Get Lost\nIn Multi-Turn Conversation,” May 09, 2025. doi: 10.48550/arXiv.2505.06120.\nAvailable: http://arxiv.org/abs/2505.06120\n\n\n[37] H.\nChase, “LangChain.” Oct. 2022. Available: https://github.com/langchain-ai/langchain\n\n\n[38] “Langchain-ai/langgraph.”\nLangChain, May 21, 2025. Available: https://github.com/langchain-ai/langgraph\n\n\n[39] P.\nLewis et al., “Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP\nTasks,” Apr. 12, 2021. doi: 10.48550/arXiv.2005.11401.\nAvailable: http://arxiv.org/abs/2005.11401\n\n\n[40] H.\nLi, “Language models: Past, present, and future,”\nCommun. ACM, vol. 65, no. 7, pp. 56–63, Jun. 2022, doi: 10.1145/3490443. Available:\nhttps://dl.acm.org/doi/10.1145/3490443\n\n\n[41] “libFuzzer –\na library for coverage-guided fuzz testing. — LLVM\n21.0.0git documentation,” 2025. Available: https://llvm.org/docs/LibFuzzer.html\n\n\n[42] D.\nLiu, J. Metzman, O. Chang, and G. O. S. S. Team, “AI-Powered\nFuzzing: Breaking the Bug Hunting\nBarrier,” Aug. 16, 2023. Available: https://security.googleblog.com/2023/08/ai-powered-fuzzing-breaking-bug-hunting.html\n\n\n[43] J.\nLiu, “LlamaIndex.” Nov. 2022. doi: 10.5281/zenodo.1234.\nAvailable: https://github.com/jerryjliu/llama_index\n\n\n[44] “The LLVM Compiler Infrastructure\nProject,” 2025. Available: https://llvm.org/\n\n\n[45] Y.\nLyu, Y. Xie, P. Chen, and H. Chen, “Prompt Fuzzing\nfor Fuzz Driver Generation,” May 29, 2024. doi: 10.48550/arXiv.2312.17677.\nAvailable: http://arxiv.org/abs/2312.17677\n\n\n[46] V.\nJ. M. Manes et al., “The Art,\nScience, and Engineering of\nFuzzing: A Survey,” Apr. 07, 2019. doi:\n10.48550/arXiv.1812.00140.\nAvailable: http://arxiv.org/abs/1812.00140\n\n\n[47] E.\nMartin, “Ninja-build/ninja.” ninja-build, Jul. 14, 2025.\nAvailable: https://github.com/ninja-build/ninja\n\n\n[48] B.\nP. Miller, L. Fredriksen, and B. So, “An empirical study of the\nreliability of UNIX utilities,” Commun.\nACM, vol. 33, no. 12, pp. 32–44, Dec. 1990, doi: 10.1145/96267.96279.\nAvailable: https://dl.acm.org/doi/10.1145/96267.96279\n\n\n[49] E.\nNijkamp et al., “CodeGen: An\nopen large language model for code with multi-turn program\nsynthesis,” ICLR, 2023.\n\n\n[50] E.\nNijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y. Zhou,\n“CodeGen2: Lessons for training llms on\nprogramming and natural languages,” ICLR, 2023.\n\n\n[51] OpenAI, “Introducing\nCodex,” May 16, 2025. Available: https://openai.com/index/introducing-codex/\n\n\n[52] A.\nArya, O. Chang, J. Metzman, K. Serebryany, and D. Liu,\n“OSS-Fuzz.” Apr. 08, 2025. Available: https://github.com/google/oss-fuzz\n\n\n[53] D.\nLiu, O. Chang, J. metzman, M. Sablotny, and M. Maruseac, “OSS-fuzz-gen: Automated fuzz target\ngeneration.” May 2024. Available: https://github.com/google/oss-fuzz-gen\n\n\n[54] OSS-Fuzz Maintainers, “Introducing LLM-based harness synthesis for unfuzzed\nprojects,” May 27, 2024. Available: https://blog.oss-fuzz.com/posts/introducing-llm-based-harness-synthesis-for-unfuzzed-projects/\n\n\n[55] “OSS-Fuzz\nDocumentation,” 2025. Available: https://google.github.io/oss-fuzz/\n\n\n[56] OWASP Foundation, “Fuzzing.”\nAvailable: https://owasp.org/www-community/Fuzzing\n\n\n[57] J.\nPakkanen, “Mesonbuild/meson.” The Meson Build System, Jul.\n14, 2025. Available: https://github.com/mesonbuild/meson\n\n\n[58] N.\nPerry, M. Srivastava, D. Kumar, and D. Boneh, “Do Users\nWrite More Insecure Code with AI Assistants?”\nDec. 18, 2023. doi: 10.48550/arXiv.2211.03622.\nAvailable: http://arxiv.org/abs/2211.03622\n\n\n[59] D.\nWang, G. Zhou, L. Chen, D. Li, and Y. Miao,\n“ProphetFuzz: Fully Automated Prediction\nand Fuzzing of High-Risk Option Combinations\nwith Only Documentation via Large Language\nModel,” Sep. 01, 2024. doi: 10.1145/3658644.3690231.\nAvailable: http://arxiv.org/abs/2409.00922\n\n\n[60] N.\nRathaus and G. Evron, Open source fuzzing tools. Burlington,\nMA: Syngress Pub, 2007.\n\n\n[61] S.\nYao et al., “ReAct: Synergizing\nReasoning and Acting in Language\nModels,” Mar. 10, 2023. doi: 10.48550/arXiv.2210.03629.\nAvailable: http://arxiv.org/abs/2210.03629\n\n\n[62] A.\nRebert et al., “Optimizing seed selection for\nfuzzing,” in Proceedings of the 23rd USENIX\nconference on Security Symposium, in\nSEC’14. USA: USENIX Association, Aug. 2014, pp.\n861–875.\n\n\n[63] J.\nSaarinen, “Further flaws render Shellshock patch\nineffective,” Sep. 29, 2014. Available: https://www.itnews.com.au/news/further-flaws-render-shellshock-patch-ineffective-396256\n\n\n[64] A.\nSarkar and I. Drosos, “Vibe coding: Programming through\nconversation with artificial intelligence,” Jun. 29, 2025. doi:\n10.48550/arXiv.2506.23253.\nAvailable: http://arxiv.org/abs/2506.23253\n\n\n[65] N.\nSasirekha, A. Edwin Robert, and M. Hemalatha, “Program\nSlicing Techniques and its\nApplications,” IJSEA, vol. 2, no. 3, pp.\n50–64, Jul. 2011, doi: 10.5121/ijsea.2011.2304.\nAvailable: http://www.airccse.org/journal/ijsea/papers/0711ijsea04.pdf\n\n\n[66] K.\nSerebryany, D. Bruening, A. Potapenko, and D. Vyukov,\n“AddressSanitizer: A fast address sanity\nchecker,” in 2012 USENIX annual technical\nconference (USENIX ATC 12), 2012, pp. 309–318.\nAvailable: https://www.usenix.org/conference/atc12/technical-sessions/presentation/serebryany\n\n\n[67] A.\nSheth, K. Roy, and M. Gaur, “Neurosymbolic AI –\nWhy, What, and How,” May\n01, 2023. doi: 10.48550/arXiv.2305.00813.\nAvailable: http://arxiv.org/abs/2305.00813\n\n\n[68] T.\nSimonite, “This Bot Hunts Software Bugs for the\nPentagon,” Wired, Jun. 01, 2020. Available:\nhttps://www.wired.com/story/bot-hunts-software-bugs-pentagon/\n\n\n[69] Y.\nSun, “Automated Generation and\nCompilation of Fuzz Driver Based on\nLarge Language Models,” in Proceedings of the\n2024 9th International Conference on Cyber\nSecurity and Information Engineering, in\nICCSIE ’24. New York, NY, USA: Association for Computing\nMachinery, Dec. 2024, pp. 461–468. doi: 10.1145/3689236.3689272.\nAvailable: https://doi.org/10.1145/3689236.3689272\n\n\n[70] M.\nSutton, A. Greene, and P. Amini, Fuzzing: Brute force vulnerabilty\ndiscovery. Upper Saddle River, NJ: Addison-Wesley, 2007.\n\n\n[71] A.\nTakanen, J. DeMott, C. Miller, and A. Kettunen, Fuzzing for software\nsecurity testing and quality assurance, Second edition. in\nInformation security and privacy library. Boston London Norwood, MA:\nArtech House, 2018.\n\n\n[72] The OpenSSL Project,\n“Openssl/openssl.” OpenSSL, Jul. 15, 2025. Available: https://github.com/openssl/openssl\n\n\n[73] L.\nThomason, “Leethomason/Tinyxml2.” Jul. 10, 2025. Available:\nhttps://github.com/leethomason/tinyxml2\n\n\n[74] D.\nTilwani, R. Venkataramanan, and A. P. Sheth, “Neurosymbolic\nAI approach to Attribution in Large\nLanguage Models,” Sep. 30, 2024. doi: 10.48550/arXiv.2410.03726.\nAvailable: http://arxiv.org/abs/2410.03726\n\n\n[75] Y.\nDeng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Large\nLanguage Models Are Zero-Shot Fuzzers: Fuzzing\nDeep-Learning Libraries via Large Language\nModels,” in Proceedings of the 32nd ACM SIGSOFT\nInternational Symposium on Software Testing and\nAnalysis, in ISSTA 2023. New York, NY,\nUSA: Association for Computing Machinery, Jul. 2023, pp. 423–435. doi:\n10.1145/3597926.3598067.\nAvailable: https://dl.acm.org/doi/10.1145/3597926.3598067\n\n\n[76] B.\nJeong et al., “UTopia: Automatic\nGeneration of Fuzz Driver using Unit\nTests,” in 2023 IEEE Symposium on\nSecurity and Privacy (SP),\nMay 2023, pp. 2676–2692. doi: 10.1109/SP46215.2023.10179394.\nAvailable: https://ieeexplore.ieee.org/abstract/document/10179394\n\n\n[77] A.\nVaswani et al., “Attention Is All You\nNeed,” Aug. 01, 2023. doi: 10.48550/arXiv.1706.03762.\nAvailable: http://arxiv.org/abs/1706.03762\n\n\n[78] D.\nWheeler, “How to Prevent the next\nHeartbleed,” 2014. Available: https://dwheeler.com/essays/heartbleed.html\n\n\n[79] S.\nYao et al., “Tree of Thoughts:\nDeliberate Problem Solving with Large Language\nModels,” Dec. 03, 2023. doi: 10.48550/arXiv.2305.10601.\nAvailable: http://arxiv.org/abs/2305.10601",
    "crumbs": [
      "Bibliography"
    ]
  }
]